#!/usr/bin/env python

import os
import subprocess
import json
import sys

def executable(fpath):
    """Returns true if the file exists and can be executed. """
    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)


# From http://stackoverflow.com/a/600612/496605
# when migrating to Python 3, os.makedirs(.., exists_ok = True)
# should be used instead.
def mkdir_p(path):
    """Makes a directory and all non-existing directories needed to contain this directory. 
    If the directory already exists, no error message is given and the function returns.
    
    keyword arguments:
    path -- full path of the directory to be made
    """
    try:
        os.makedirs(path)
    except OSError as exc: # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else: raise


class Implementation:
    """An implementation is the source code of a solution to a general problem.
    The hierarchy is: problem -> implementation -> benchmark.
    """
    def __init__(self, hipermark_lib_dir, benchmark_lib_dir, name, directory):
        """The constructor of the class "Implementation".
        Sets the appropriate variables for an implementation.
        Also checks whether an instantiation is correctly configured with folders,
        executables etc.
        Does not compile the source code of the implementation.
        
        keyword arguments:
        hipermark_lib_dir -- /lib
        benchmark_lib_dir -- /benchmarks/<benchmark>/lib
        name -- Name of implementation's folder.
        directory -- /benchmarks/<benchmark>/implementations/<implementation>
        """
        if os.path.isdir(directory): #returns true if path is an existing directory.
            self.directory = directory # directory of implementation
        else:
            # If the path is not a directory, one of these two errors are raised.
            if os.path.isfile(directory):
                raise Exception("File %s exists, but implementation should be contained in its own directory." % directory)
            else:
                raise Exception("Implementation directory %s does not exist " % directory)

        self.name = name
        self.hipermark_lib_dir = hipermark_lib_dir
        self.benchmark_lib_dir = benchmark_lib_dir

        # Joins the directory path with "instantiate" to get the full path of the instantiate file. Sets the object var instantiatefile if this is an executable.
        instantiate_file = os.path.join(self.directory, "instantiate")
        instantiate_data_file = os.path.join(self.directory, "instantiate_data")
        if not executable(instantiate_file):
            raise Exception("%s does not exist or it is not an executable file" % instantiate_file)
        if not executable(instantiate_data_file):
            raise Exception("%s does not exist or it is not an executable file" % instantiate_data_file)
        self.instantiate_file = instantiate_file
        self.instantiate_data_file = instantiate_data_file


    # instantiate_data should also take the name of the dataset as argument.
    # This name should be the name of the folder in which the dataset is placed.
    def instantiate_data(self, target_dir, input_data):
        """Run the instantiate_data script which creates a run file to run the
        implementation.

        keyword arguments:
        target_dir -- /instantiations/"<benchmark>-<implementation>"/datasets/<dataset>
        input_data -- path of input file. /benchmarks/<benchmark>/datasets/<dataset>
        """
        if not os.path.isdir(target_dir):
            mkdir_p(target_dir)
        
        # HIPERMARK_LIB_DIR: path of main library. /lib/
        # HIPERMARK_INPUT: path of input file. /benchmarks/<benchmark>/dataset/input.json
        instantiate_env = os.environ.copy()
        instantiate_env["HIPERMARK_LIB_DIR"] = self.hipermark_lib_dir
        instantiate_env["HIPERMARK_INPUT"] = input_data

        proc = subprocess.Popen(self.instantiate_data_file,
                                env=instantiate_env,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                close_fds=True,
                                cwd=target_dir)

        # Read data from stdout and stderr, until end-of-file is reached. Also waits for process to terminate.
        (stdoutdata, stderrdata) = proc.communicate(input=None)
        return (proc.returncode, stdoutdata, stderrdata)


    
    # This function could receive ALL possible input data and run script could then receive an argument to determine which input data should be run.
    # the instantiate file copies all the necessary source files to its own folder.
    def instantiate(self, target_dir, platform_file):
        """Runs the instantiate_file which creates a run file to run the
        implementation and also runs the local Makefile.
        The instantiate file copies all the necessary source files in its own
        folder in /instantiations/.
        The compilation of the implementation source code takes place here.

        keyword arguments:
        target_dir -- /instantiations/"<benchmark>-<implementation>-<dataset>"
        platform_file -- /config/platform_example.json
        """
        if not os.path.isdir(target_dir):
            mkdir_p(target_dir)

        # Sets some environment variables. These are used in the subprocess.Popen function call, and they are handled in the "instantiate" files.
        # We use the current environment as a template, and then extend it with some new variables.
        # HIPERMARK_LIB_DIR: /lib/
        # HIPERMARK_BENCHMARK_LIB_DIR: /benchmarks/<benchmark>/lib
        # HIPERMARK_IMPLEMENTATION: /benchmarks/<benchmark>/implementations/<implementation>
        # HIPERMARK_PLATFORM: path of platform file. /config/platform_example.json
        instantiate_env = os.environ.copy()
        instantiate_env["HIPERMARK_LIB_DIR"] = self.hipermark_lib_dir
        instantiate_env["HIPERMARK_BENCHMARK_LIB_DIR"] = self.benchmark_lib_dir
        instantiate_env["HIPERMARK_IMPLEMENTATION"] = self.directory
        instantiate_env["HIPERMARK_PLATFORM"] = platform_file

        # cwd = target_dir sets the child process' folder to:
        # /instantiations/"<benchmark>-<implementation>/"
        proc = subprocess.Popen(self.instantiate_file,
                                env=instantiate_env,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                close_fds=True,
                                cwd=target_dir)

        # Read data from stdout and stderr, until end-of-file is reached. Also waits for process to terminate.
        (stdoutdata, stderrdata) = proc.communicate(input=None)
        return (proc.returncode, stdoutdata, stderrdata)

class Benchmark:
    # This class describes a benchmark which is an abstract problem 
    # but not its specific implementation. A benchmark can in general have 
    # many implementations and this object contains all the implementations
    # in its self.implementations variable.
    def __init__(self, hipermarkdir, name, directory):
        """This initialization stores all the implementations and all the datasets of
        this benchmark. It thus collects info about this benchmark's implementations.
        These source code of which should be located in folders in:
        /benchmarks/<benchmark>/implementations/
        It also does some checks on the content of the implementations and the datasets 
        such as checking that an "implementations" and a "datasets" folder exists in
        the correct folder.
        
        keyword arguments:
        hipermark_dir -- The directory from which this process was invoked.
        name -- name of benchmark as defined in the folder for its source code.
        directory -- directory of the benchmark. E.g. /benchmarks/OptionPricing
        """
        self.hipermarkdir = hipermarkdir
        self.name = name

        if os.path.isdir(directory):
            self.directory = directory
        else:
            raise Exception("Benchmark directory %s does not exist" % directory)
        dircontents = os.listdir(self.directory) # list of content in benchmark root directory

        # The lib directories do not have to exist.
        # The library functions for a benchmark are located in benchmarks/<benchmark>/lib/include
        self.benchmark_lib_dir = os.path.join(self.directory, "lib")
        self.hipermark_lib_dir = os.path.join(self.hipermarkdir, "lib")

        if "implementations" in dircontents:
            self.__gather_implementations() #creates list of implementations to be run.
        else:
            raise Exception("No \"implementation\" directory in %s" % directory)

        if "datasets" in dircontents:
            self.datasets = os.path.join(self.directory, "datasets")
            self.__gather_datasets()
        else:
            raise Exception("No datasets directory in %s" % directory)


    def __gather_implementations(self):
        """ Creates an implementation object for each non-disabled folder in 
        /benchmarks/<benchmark>/implementations/. The objects are placed in
        a list self.implementations.
        """
        self.implementations = [] #list of implementation objects.
        # This for-loop runs over all folders (and files) found in the "implementations" folder.
        implrootpath = os.path.join(self.directory, "implementations")
        for impl in os.listdir(implrootpath):
            implpath = os.path.join(self.directory, "implementations", impl)
            if os.path.isdir(implpath):
                disable_file = os.path.join(implpath, "disable")
                if os.path.isfile(disable_file):
                    continue
                self.implementations.append(Implementation(self.hipermark_lib_dir,
                                                           self.benchmark_lib_dir,
                                                           impl,
                                                           os.path.join(self.directory, "implementations", impl)))
            else:
                raise Exception("%s may only contain folders, not files." %implrootpath )

    def __gather_datasets(self):
        """Checks that each dataset in /benchmarks/<benchmark>/datasets/
        has the correct structure. Sets the self.datasets variable to an
        associative array of tuples containing the file name of the input
        file and a Python object representing the expected output.
        """
        self.datasets = {}
        for dataset in os.listdir(os.path.join(self.directory, "datasets")):
            datasetpath = os.path.join(self.directory, "datasets", dataset)
            if os.path.isdir(datasetpath):
                inputfile = os.path.join(self.directory, "datasets", dataset, "input.json")
                outputfile = os.path.join(self.directory, "datasets", dataset, "output.json")
                # Checks that input and output files are present.
                if not os.path.isfile(inputfile):
                    raise Exception("File %s missing or not a file." % inputfile)
                if not os.path.isfile(outputfile):
                    raise Exception("File %s missing or not a file." % outputfile)

                self.datasets[dataset] = { 'input_file' : inputfile,
                                           'expected_output' : read_json_file(outputfile)
                                         }

def desc_triple(benchmark, implementation, dataset):
    return "Benchmark %s, implementation %s, dataset %s" % (benchmark, implementation, dataset)

def note_result(results, benchmark, implementation, dataset, runtime):
    results[(benchmark,implementation,dataset)] = runtime

def run_some_benchmark_implementations(hipermark_dir,
                                       workdir,
                                       benchmarkdir,
                                       platform,
                                       run_implementation_fn,
                                       use_dataset_fn):
    """Declares all the objects associated with a specific benchmark:
    both benchmark and implementation objects declaration are called here
    (implementation decl. is called through benchmark decl.). 
    
    keyword arguments:
    hipermark_dir -- The directory from which this process was invoked.
    workdir -- hipermark_dir/instantiations. Where the binaries are placed.
    benchmarkdir -- benchmarks/<benchmark>. Path of benchmark to run.
    platform -- path of platform file.
    run_implementation_fn -- pointer to function run_implementation_fn
    use_dataset_fn --pointer to function use_dataset_fn
    """

    # Instantiates the benchmark objects which contain the implementation objects. 
    # Does not compile.
    b = Benchmark(hipermark_dir, os.path.basename(os.path.normpath(benchmarkdir)), benchmarkdir)
    data_insts = []  # all succesfull data compilations.
    failed_data_insts = [] # all failed data compilations
    insts = []  # all succesfull data compilations.
    failed_insts = [] # all failed data compilations
    results = {}

    # Running all "instantiate_data" files
    for impl in b.implementations:
        if not run_implementation_fn(impl):
            continue
        for dataset in b.datasets:
            if not use_dataset_fn(dataset):
                continue
            #inst_data_dir =<hipermark_dir>/instantiations/<benchmark>-<implementation>/datasets/<dataset>
            inst_data_dir = os.path.join(workdir,
                                   "%s-%s/datasets/%s" % (b.name, impl.name, dataset)) 
            print("Instantiating data in %s" % inst_data_dir)
            (retcode, stdout, stderr) = impl.instantiate_data(inst_data_dir, b.datasets[dataset]['input_file'], "[dataset.name]") #compilation of dataset.
            if retcode != 0:
                failed_data_insts.append({ 'benchmark' : b.name,
                                      'implementation' : impl.name,
                                      'dataset' : dataset,
                                      'instantiation_directory' : inst_data_dir,
                                      'retcode' : retcode,
                                      'stdout' : stdout,
                                      'stderr' : stderr})
                print("Spoiler: failed")
            else:
                data_insts.append({ 'benchmark' : b.name,
                               'implementation' : impl.name,
                               'dataset' : dataset,
                               'instantiation_directory' : inst_data_dir,
                               'expected_output' : b.datasets[dataset]['expected_output']
                           })

    # Running all "instantiate" files
    for impl in b.implementations:
        if not run_implementation_fn(impl):
            continue
        inst_dir = os.path.join(workdir, "%s-%s" % (b.name, impl.name))
        print("Instantiating source code in %s" % inst_dir)
        (retcode, stdout, stderr) = impl.instantiate(inst_dir, platform)
        if retcode != 0:
            failed_insts.append({ 'benchmark' : b.name,
                                      'implementation' : impl.name,
                                      #'dataset' : dataset,
                                      'instantiation_directory' : inst_dir,
                                      'retcode' : retcode,
                                      'stdout' : stdout,
                                      'stderr' : stderr})
            print("Spoiler: failed")
        else:
            insts.append({ 'benchmark' : b.name,
                        'implementation' : impl.name,
                        #'dataset' : dataset,
                        'instantiation_directory' : inst_dir,
                        'expected_output' : b.datasets[dataset]['expected_output']})

    # Here, all data instantiations are looped over and the run file
    # is executed for each dataset. Error messages are also collected.
    # This execution should allow run to take an argument.
    for inst in data_insts:
        #inst_dir = os.path.join(workdir, "%s-%s" % (b.name, impl.name))
        # we need to be able to access parent folder.
        # instdir = /instantiations/<benchmark>-<implementation>/datasets/<dataset>
        instdir = inst['instantiation_directory']
        print("\nRunning %s" % instdir)
        stdoutpath = os.path.join(instdir, "stdout.log")
        stderrpath = os.path.join(instdir, "stderr.log")
        with open(stdoutpath, "w") as stdout:
            with open(stderrpath, "w") as stderr:
                print("Output can be found in %s and %s" % (stdoutpath, stderrpath))
                try:
                    # Here, the execution of the code takes place. 
                    # cat input.data | ./executable is called here.
                    # This sub process is run in the target directory
                    # For now, the dataset is hardcoded. Obviously, this must be changed.
                    # run will need to be told where ./VolCalib (binary) is located.
                    # This should be done through environment variables.
                    print("Before subprocess")
                    proc = subprocess.Popen([os.path.join(instdir,"../..", "run"),
                                             os.path.join(instdir, "input.data")],
                                            stdout=stdout,
                                            stderr=stderr,
                                            stdin=subprocess.PIPE,
                                            close_fds=True,
                                            cwd=instdir)
                    proc.stdin.close() # Make sure stdin EOFs.
                    proc.wait()
                    print("After subprocess")
                except:
                    note_result(results,
                                inst['benchmark'],
                                inst['implementation'],
                                inst['dataset'],
                                "could not run")
                    continue

        if proc.returncode != 0:
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "non-zero exit code")
            continue

        try:
            runtime = read_runtime(instdir)
        except IOError:
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "no runtime measurement")
            continue
        except ValueError:
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "non-integer runtime measurement")
            continue

        try:
            result = read_json_file(os.path.join(instdir, "result.json"))
            expected = inst['expected_output']
            if compare_json(result, expected):
                note_result(results,
                            inst['benchmark'],
                            inst['implementation'],
                            inst['dataset'],
                            runtime)
            else:
                note_result(results,
                            inst['benchmark'],
                            inst['implementation'],
                            inst['dataset'],
                            "invalid result")
        except (IOError, ValueError):
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "cannot read result")
    return (results, failed_data_insts)


def read_runtime(instdir):
    """Reads the runtime.txt holding the run time, and created by the executable. 
    """
    with open(os.path.join(instdir, "runtime.txt"), "r") as file:
        return int(file.read())


def read_json_file(filename):
    """ returns an array containing the content of the JSON file.
    """
    with open(filename, "r") as file:
        return json.loads(file.read())


# Should a relative value be used instead of epsilon?
epsilon = 0.001


def compare_json(json1, json2):
    """Generic comparison function using epsilon for comparing floats.
    """
    if type(json1) != type(json2):
        return False

    if type(json1) is float:
        return abs(json1-json2) < epsilon

    if type(json1) is list:
        if len(json1) != len(json2):
            return False
        for x,y in zip(json1, json2):
            if not compare_json(x,y):
                return False
        return True

    if type(json1) is dict:
        keys1 = json1.keys()
        keys2 = json2.keys()
        keys1.sort()
        keys2.sort()
        if keys1 == keys2:
            for key in keys1:
                if not compare_json(json1[key], json2[key]):
                    return False
            return True
        else:
            return False

    return json1 == json2

def what_to_run():
    if len(sys.argv) == 1:
        return ("benchmarks/CalibVolDiff", None, None)
    elif len(sys.argv) == 2:
        return (sys.argv[1], None, None)
    elif len(sys.argv) == 3:
        return (sys.argv[1], sys.argv[2].split(','), None)
    elif len(sys.argv) == 4:
        return (sys.argv[1], sys.argv[2].split(','), sys.argv[3].split(','))
    else:
        exit("Usage: %s [benchmark] [implementation1,implementation2,...] [dataset1,dataset2,...]" %
             sys.argv[0])

if __name__ == '__main__':
    """This main function first creates lists of benchmarks, implementations, and datasets
    which it will run. It then runs these benchmarks and prints the results from stdout 
    and from stderr.
    """
    (benchmark, implementations, datasets) = what_to_run()
    instantiations_dir = "instantiations"
    platform_file = "config/platform_example.json"

    print("Assuming I am being run from main hipermark directory.")
    print("Will run benchmark %s." % benchmark)

    if implementations is None:
        print("Will run all implementations.")
    else:
        print("Will run these implementations: %s" % implementations)

    if datasets is None:
        print("Will use all datasets.")
    else:
        print("Will use these datasets: %s" % datasets)

    print("Will instantiate in %s." % instantiations_dir)
    print("Will use platform configuration %s." % platform_file)
    hipermark_dir = os.getcwd()

    #Pointers to these functions are passed as arguments below.
    def run_implementation_fn(impl):
        if implementations is None:
            return True
        else:
            return impl.name in implementations

    def use_dataset_fn(dataset):
        if datasets is None:
            return True
        else:
            return dataset in datasets

    (results, failed_to_instantiate) = (
        run_some_benchmark_implementations(hipermark_dir,
                                           os.path.join(hipermark_dir, instantiations_dir),
                                           os.path.join(hipermark_dir, benchmark),
                                           os.path.join(hipermark_dir, platform_file),
                                           run_implementation_fn,
                                           use_dataset_fn)
    )

    for failed in failed_to_instantiate:
        print("Failed to instantiate implementation %s with dataset %s; exit code %d." %
              (failed['implementation'], failed['dataset'], failed['retcode']))
        print("Stdout\n")
        print(failed['stdout'])
        print("Stderr\n")
        print(failed['stderr'])
        print("")

    successful = {}
    failed = {}

    for result in results:
        if type(results[result]) is int:
            successful[result] = results[result]
        else:
            failed[result] = results[result]

    print "\nFailed:"
    for f in failed:
        (benchmark, implementation, dataset) = f
        print("%s: %s" %
              (desc_triple(benchmark, implementation, dataset),
               failed[f]))

    print "\nSucceeded:"
    for s in successful:
        (benchmark, implementation, dataset) = s
        print("%s: %s" %
              (desc_triple(benchmark, implementation, dataset),
               successful[s]))
