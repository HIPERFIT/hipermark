#!/usr/bin/env python

import os
import subprocess
import json
import sys
import errno # needed to prevent error in exc.errno == errno.EEXIST

def executable(fpath):
    """Returns true if the file exists and can be executed. """
    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)


# From http://stackoverflow.com/a/600612/496605
# when migrating to Python 3, os.makedirs(.., exists_ok = True)
# should be used instead.
def mkdir_p(path):
    """Makes a directory and all non-existing directories needed to contain this directory 
    If the directory already exists, no error message is given and the function returns.
    
    keyword arguments:
    path -- full path of the directory to be made
    """
    try:
        os.makedirs(path)
    except OSError as exc: # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path): # this fails.
            pass
        else: raise


class Implementation:
    """An implementation is the source code of a solution to a general problem.
    The hierarchy is: problem -> implementation -> benchmark.
    """
    def __init__(self, hipermark_lib_dir, benchmark_lib_dir, name, directory):
        """The constructor of the class "Implementation".
        Sets the appropriate variables for an implementation.
        Also checks whether an instantiation is correctly configured with folders,
        executables etc.
        Does not compile the source code of the implementation.
        
        keyword arguments:
        hipermark_lib_dir -- /lib
        benchmark_lib_dir -- /benchmarks/<benchmark>/lib
        name -- Name of implementation's folder.
        directory -- /benchmarks/<benchmark>/implementations/<implementation>
        """
        if os.path.isdir(directory): #returns true if path is an existing directory.
            self.directory = directory # directory of implementation
        else:
            # If the path is not a directory, one of these two errors are raised.
            if os.path.isfile(directory):
                raise Exception("File %s exists, but implementation should be contained in its own directory." % directory)
            else:
                raise Exception("Implementation directory %s does not exist " % directory)

        self.name = name
        self.hipermark_lib_dir = hipermark_lib_dir
        self.benchmark_lib_dir = benchmark_lib_dir

        # Joins the directory path with "instantiate" to get the full path of the instantiate file. Sets the object var instantiatefile if this is an executable.
        instantiate_file = os.path.join(self.directory, "instantiate")
        instantiate_data_file = os.path.join(self.directory, "instantiate_data")
        if not executable(instantiate_file):
            raise Exception("%s does not exist or it is not an executable file" % instantiate_file)
        if not executable(instantiate_data_file):
            raise Exception("%s does not exist or it is not an executable file" % instantiate_data_file)
        self.instantiate_file = instantiate_file
        self.instantiate_data_file = instantiate_data_file


    def instantiate_data(self, inst_dir, input_data, dataset_name):
        """Run the instantiate_data script which prepares the input data of the case.

        keyword arguments:
        inst_dir -- /instantiations/"<benchmark>-<implementation>" (target directory)
        input_data -- path of input file. /benchmarks/<benchmark>/datasets/<dataset>
        dataset_name -- name of dataset, specified by dir name of dataset in source files.
        """
        if not os.path.isdir(inst_dir):
            mkdir_p(inst_dir)

        # HIPERMARK_LIB_DIR: path of main library. /lib/
        # HIPERMARK_INPUT: path of input file. /benchmarks/<benchmark>/datasets/<dataset>/input.json
        # HIPERMARK_INPUT_NAME: Name of the folder in which the dataset was contained. This
        # prevents the overwriting of other data files, so instantiate_data must use this 
        # variable.
        instantiate_env = os.environ.copy()
        instantiate_env["HIPERMARK_LIB_DIR"] = self.hipermark_lib_dir
        instantiate_env["HIPERMARK_INPUT"] = input_data
        instantiate_env["HIPERMARK_INPUT_NAME"] = dataset_name

        proc = subprocess.Popen(self.instantiate_data_file,
                                env=instantiate_env,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                close_fds=True,
                                cwd=inst_dir)

        # Read data from stdout and stderr, until end-of-file is reached. Also waits for process to terminate.
        (stdoutdata, stderrdata) = proc.communicate(input=None)
        return (proc.returncode, stdoutdata, stderrdata)

 
    def instantiate(self, inst_dir, platform_file):
        """Runs the instantiate_file which creates a run file to run the
        implementation and also runs the local Makefile.
        The instantiate file copies all the necessary source files in its own
        folder in /instantiations/.
        The compilation of the implementation source code takes place here.

        keyword arguments:
        inst_dir -- /instantiations/"<benchmark>-<implementation>" (target directory)
        platform_file -- /config/platform_example.json
        """
        if not os.path.isdir(inst_dir):
            mkdir_p(inst_dir)

        # Sets some environment variables. These are used in the subprocess.Popen function call, and they are handled in the "instantiate" files.
        # HIPERMARK_LIB_DIR: /lib/
        # HIPERMARK_BENCHMARK_LIB_DIR: /benchmarks/<benchmark>/lib
        # HIPERMARK_IMPLEMENTATION: /benchmarks/<benchmark>/implementations/<implementation>
        # HIPERMARK_PLATFORM: path of platform file. /config/platform_example.json
        instantiate_env = os.environ.copy()
        instantiate_env["HIPERMARK_LIB_DIR"] = self.hipermark_lib_dir
        instantiate_env["HIPERMARK_BENCHMARK_LIB_DIR"] = self.benchmark_lib_dir
        instantiate_env["HIPERMARK_IMPLEMENTATION"] = self.directory
        instantiate_env["HIPERMARK_PLATFORM"] = platform_file

        # cwd = inst_dir sets the child process' folder to:
        # /instantiations/"<benchmark>-<implementation>/"
        proc = subprocess.Popen(self.instantiate_file,
                                env=instantiate_env,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                close_fds=True,
                                cwd=inst_dir)

        # Read data from stdout and stderr, until end-of-file is reached. Also waits for process to terminate.
        (stdoutdata, stderrdata) = proc.communicate(input=None)
        return (proc.returncode, stdoutdata, stderrdata)

class Benchmark:
    def __init__(self, hipermarkdir, name, directory):
        """This initialization stores all the implementations and all the datasets of
        this benchmark. It thus collects info about this benchmark's implementations.
        These source code of which should be located in folders in:
        /benchmarks/<benchmark>/implementations/
        It also does some checks on the content of the implementations and the datasets 
        such as checking that an "implementations" and a "datasets" folder exists in
        the correct folder.
        
        keyword arguments:
        hipermark_dir -- The directory from which this program was invoked.
        name -- name of benchmark as defined in the folder for its source code.
        directory -- directory of the benchmark. E.g. /benchmarks/OptionPricing
        """
        self.hipermarkdir = hipermarkdir
        self.name = name

        if os.path.isdir(directory):
            self.directory = directory
        else:
            raise Exception("Benchmark directory %s does not exist" % directory)
        dircontents = os.listdir(self.directory) # list of content in benchmark root directory

        # The lib directories do not have to exist.
        # The library functions for a benchmark are located in benchmarks/<benchmark>/lib/include
        self.benchmark_lib_dir = os.path.join(self.directory, "lib")
        self.hipermark_lib_dir = os.path.join(self.hipermarkdir, "lib")

        if "implementations" in dircontents:
            self.__gather_implementations() #creates list of implementations to be run.
        else:
            raise Exception("No \"implementation\" directory in %s" % directory)

        if "datasets" in dircontents:
            self.datasets = os.path.join(self.directory, "datasets")
            self.__gather_datasets()
        else:
            raise Exception("No datasets directory in %s" % directory)


    def __gather_implementations(self):
        """ Creates an implementation object for each non-disabled folder in 
        /benchmarks/<benchmark>/implementations/. The objects are placed in
        a list self.implementations.
        """
        self.implementations = [] #list of implementation objects.
        # This for-loop runs over all folders (and files) found in the "implementations" folder.
        implrootpath = os.path.join(self.directory, "implementations")
        for impl in os.listdir(implrootpath):
            implpath = os.path.join(self.directory, "implementations", impl)
            if os.path.isdir(implpath):
                disable_file = os.path.join(implpath, "disable")
                if os.path.isfile(disable_file):
                    continue
                self.implementations.append(Implementation(self.hipermark_lib_dir,
                                                           self.benchmark_lib_dir,
                                                           impl,
                                                           os.path.join(self.directory, "implementations", impl)))
            else:
                raise Exception("%s may only contain folders, not files." %implrootpath )

            
    def __gather_datasets(self):
        """Checks that each dataset in /benchmarks/<benchmark>/datasets/
        has the correct structure. Sets the self.datasets variable to an
        associative array of tuples containing the file name of the input
        file and a Python object representing the expected output.
        """
        self.datasets = {}
        for dataset in os.listdir(os.path.join(self.directory, "datasets")):
            datasetpath = os.path.join(self.directory, "datasets", dataset)
            if os.path.isdir(datasetpath):
                inputfile = os.path.join(self.directory, "datasets", dataset, "input.json")
                outputfile = os.path.join(self.directory, "datasets", dataset, "output.json")
                # Checks that input and output files are present.
                if not os.path.isfile(inputfile):
                    raise Exception("File %s missing or not a file." % inputfile)
                if not os.path.isfile(outputfile):
                    raise Exception("File %s missing or not a file." % outputfile)

                self.datasets[dataset] = { 'input_file' : inputfile,
                                           'expected_output' : read_json_file(outputfile)
                                         }

def desc_quadruple(benchmark, implementation, dataset, run):
    return "Benchmark %s, implementation %s, dataset %s, run %d" % (benchmark, implementation, dataset, run)

def note_result(results, benchmark, implementation, dataset, run, runtime):
    """Stores the runtime of the execution of an implementation in a dictionary.
    This dictionary has triple's (benchmark, implementation, dataset) as key.

    keyword arguments:
    results -- name of dictionary in which runtime is saved.
    benchmark -- name of benchmark
    implementation -- name of implementation
    dataset -- name of dataset.
    run -- run counter
    runtime -- runtime as reported by ???
    """
    # this means that this function adds an entry to a dictionary
    results[(benchmark,implementation,dataset, run)] = runtime

    # This function can be expanded to take iterations as argument.
def run_some_benchmark_implementations(hipermark_dir,
                                       workdir,
                                       benchmarkdir,
                                       number_of_runs,
                                       platform,
                                       run_implementation_fn,
                                       use_dataset_fn):
    """Declares all the objects associated with a specific benchmark:
    both benchmark and implementation objects declaration are called here
    (implementation decl. is called through benchmark decl.). 
    
    keyword arguments:
    hipermark_dir -- The directory from which this process was invoked.
    workdir -- hipermark_dir/instantiations. Where the binaries are placed.
    benchmarkdir -- benchmarks/<benchmark>. Path of benchmark to run.
    number_of_runs -- number of times that each case (dataset) will be run
    platform -- path of platform file.
    run_implementation_fn -- pointer to function run_implementation_fn
    use_dataset_fn --pointer to function use_dataset_fn
    """

    # Instantiates the benchmark objects which contain the implementation objects. 
    # Does not compile.
    b = Benchmark(hipermark_dir, os.path.basename(os.path.normpath(benchmarkdir)), benchmarkdir)
    data_insts = []  # all succesfull data compilations.
    failed_data_insts = [] # all failed data compilations
    insts = []  # all succesfull implementation compilations.
    failed_insts = [] # all failed implementation compilations
    results = {} # dict in Python. Like an associative array. keys: bench, impl, datas, run_n

    # Running all "instantiate_data" files
    for impl in b.implementations:
        if not run_implementation_fn(impl):
            continue
        inst_dir = os.path.join(workdir, "%s-%s" % (b.name, impl.name))
        for dataset in b.datasets:
            if not use_dataset_fn(dataset):
                continue
            print("Instantiating dataset \"%s\" in %s" % (dataset, inst_dir))
            (retcode, stdout, stderr) = impl.instantiate_data(inst_dir, b.datasets[dataset]['input_file'], dataset) #compilation of dataset.
            if retcode != 0:
                failed_data_insts.append({ 'benchmark' : b.name,
                                      'implementation' : impl.name,
                                      'dataset' : dataset,
                                      'instantiation_directory' : inst_dir,
                                      'retcode' : retcode,
                                      'stdout' : stdout,
                                      'stderr' : stderr})
                print("Spoiler: failed")
            else:
                data_insts.append({ 'benchmark' : b.name,
                               'implementation' : impl.name,
                               'dataset' : dataset,
                               'instantiation_directory' : inst_dir,
                               'expected_output' : b.datasets[dataset]['expected_output']
                           })

    # Running all "instantiate" files
    for impl in b.implementations:
        if not run_implementation_fn(impl):
            continue
        inst_dir = os.path.join(workdir, "%s-%s" % (b.name, impl.name))
        print("Instantiating source code in %s" % inst_dir)
        (retcode, stdout, stderr) = impl.instantiate(inst_dir, platform)
        if retcode != 0:
            failed_insts.append({ 'benchmark' : b.name,
                                      'implementation' : impl.name,
                                      'instantiation_directory' : inst_dir,
                                      'retcode' : retcode,
                                      'stdout' : stdout,
                                      'stderr' : stderr})
            print("Spoiler: failed")
        else:
            insts.append({ 'benchmark' : b.name,
                        'implementation' : impl.name,
                        'instantiation_directory' : inst_dir,
                        'expected_output' : b.datasets[dataset]['expected_output']})

    # Here, all data instantiations are looped over and the run file
    # is executed for each dataset. 
    # This execution should allow run to take an argument in the form of
    # HIPERMARK_INPUT_NAME.
    for inst in data_insts:
        #inst_dir = os.path.join(workdir, "%s-%s" % (b.name, impl.name))
        inst_dir = inst['instantiation_directory'] #should not be changed by counter.
        for run in range(1, number_of_runs+1):
            run_output = os.path.join(inst_dir, "run_output/", inst['dataset'], "run"+str(run))
            mkdir_p(run_output)
            print("\nRunning %s" % run_output)
            stdoutpath = os.path.join(run_output, "stdout.log")
            stderrpath = os.path.join(run_output, "stderr.log")
            resultpath = os.path.join(run_output, "result.json")
            runtimepath = os.path.join(run_output, "runtime.txt")
            with open(stdoutpath, "w") as stdout:
                with open(stderrpath, "w") as stderr:
                    print("Output can be found in %s and %s" % (stdoutpath, stderrpath))
                    try:
                        instantiate_env = os.environ.copy()
                        #HIPERMARK_RESULT/RUNTIME: path for impl. to store result/runtime
                        instantiate_env["HIPERMARK_INPUT_NAME"] = inst['dataset']
                        instantiate_env["HIPERMARK_RESULT"] = resultpath
                        instantiate_env["HIPERMARK_RUNTIME"] = runtimepath
                        proc = subprocess.Popen(os.path.join(inst_dir, "run"),
                                                env=instantiate_env,
                                                stdout=stdout,
                                                stderr=stderr,
                                                stdin=subprocess.PIPE,
                                                close_fds=True,
                                                cwd=inst_dir)
                        proc.stdin.close() # Make sure stdin EOFs.
                        proc.wait()
                    except:
                        note_result(results,
                                    inst['benchmark'],
                                    inst['implementation'],
                                    inst['dataset'],
                                    run,
                                    "could not run")
                        # a call to note_result function adds an entry in the result function
                        print("Spoiler: failed")
                        continue
                    if proc.returncode != 0:
                        note_result(results,
                                    inst['benchmark'],
                                    inst['implementation'],
                                    inst['dataset'],
                                    run,
                                    "non-zero exit code")
                        print("Spoiler: failed")
                        continue
                    
                    try:
                        runtime = read_runtime(run_output)
                    except IOError:
                        note_result(results,
                                    inst['benchmark'],
                                    inst['implementation'],
                                    inst['dataset'],
                                    run,
                                    "no runtime measurement")
                        continue
                    except ValueError:
                        note_result(results,
                                    inst['benchmark'],
                                    inst['implementation'],
                                    inst['dataset'],
                                    run,
                                    "non-integer runtime measurement")
                        continue
                    
                    try:
                        result = read_json_file(os.path.join(run_output, "result.json"))
                        expected = inst['expected_output']
                        if compare_json(result, expected):
                            note_result(results,
                                        inst['benchmark'],
                                        inst['implementation'],
                                        inst['dataset'],
                                        run,
                                        runtime)
                        else:
                            note_result(results,
                                        inst['benchmark'],
                                        inst['implementation'],
                                        inst['dataset'],
                                        run,
                                        "result did not match expected result")
                    except (IOError, ValueError):
                        note_result(results,
                                    inst['benchmark'],
                                    inst['implementation'],
                                    inst['dataset'],
                                    run,
                                    "cannot read result")
    return (results, failed_data_insts) #returns the dictionary and more


def read_runtime(output_dir):
    """Reads the runtime.txt holding the run time, and created by the executable.

    keyword arguments:
    output_dir -- absolute path of dir in which runtime results are stored
    """
    with open(os.path.join(output_dir, "runtime.txt"), "r") as file:
        return int(file.read())


def read_json_file(filename):
    """ returns an array containing the content of the JSON file.

    keyword arguments:
    filename -- name of json file to interpret
    """
    with open(filename, "r") as file:
        return json.loads(file.read())


# Should a relative value be used instead of epsilon?
epsilon = 0.001


def compare_json(json1, json2):
    """Generic comparison function using epsilon for comparing floats.
    The order of the arguments is arbitrary.
    
    keyword aguments:
    json1 -- calculated result (arbitrary order)
    json2 -- expected result (arbitrary order)
    """
    if type(json1) != type(json2):
        return False

    if type(json1) is float:
        return abs(json1-json2) < epsilon

    if type(json1) is list:
        if len(json1) != len(json2):
            return False
        for x,y in zip(json1, json2):
            if not compare_json(x,y):
                return False
        return True

    if type(json1) is dict:
        keys1 = json1.keys()
        keys2 = json2.keys()
        keys1.sort()
        keys2.sort()
        if keys1 == keys2:
            for key in keys1:
                if not compare_json(json1[key], json2[key]):
                    return False
            return True
        else:
            return False

    return json1 == json2

def what_to_run():
    if len(sys.argv) == 1:
        return ("benchmarks/CalibVolDiff", None, None)
    elif len(sys.argv) == 2:
        return (sys.argv[1], None, None)
    elif len(sys.argv) == 3:
        return (sys.argv[1], sys.argv[2].split(','), None)
    elif len(sys.argv) == 4:
        return (sys.argv[1], sys.argv[2].split(','), sys.argv[3].split(','))
    else:
        exit("Usage: %s [benchmark] [implementation1,implementation2,...] [dataset1,dataset2,...]" %
             sys.argv[0])

if __name__ == '__main__':
    """This main function first creates lists of benchmarks, implementations, and datasets
    which it will run. It then runs these benchmarks and prints the results from stdout 
    and from stderr.
    """
    (benchmark, implementations, datasets) = what_to_run()
    instantiations_dir = "instantiations"
    platform_file = "config/platform_example.json"
    number_of_runs = 2

    print("Assuming I am being run from main hipermark directory.")
    print("Will run benchmark %s." % benchmark)

    if implementations is None:
        print("Will run all implementations.")
    else:
        print("Will run these implementations: %s" % implementations)

    if datasets is None:
        print("Will use all datasets.")
    else:
        print("Will use these datasets: %s" % datasets)

    print("Each dataset will be run %d times" % number_of_runs)

    print("Will instantiate in %s." % instantiations_dir)
    print("Will use platform configuration %s." % platform_file)
    hipermark_dir = os.getcwd()

    #Pointers to these functions are passed as arguments below.
    def run_implementation_fn(impl):
        if implementations is None:
            return True
        else:
            return impl.name in implementations

    def use_dataset_fn(dataset):
        if datasets is None:
            return True
        else:
            return dataset in datasets

    (results, failed_to_instantiate) = (
        run_some_benchmark_implementations(hipermark_dir,
                                           os.path.join(hipermark_dir, instantiations_dir),
                                           os.path.join(hipermark_dir, benchmark),
                                           number_of_runs,
                                           os.path.join(hipermark_dir, platform_file),
                                           run_implementation_fn,
                                           use_dataset_fn)
    )

    for failed in failed_to_instantiate:
        print("Failed to instantiate implementation %s with dataset %s; exit code %d." %
              (failed['implementation'], failed['dataset'], failed['retcode']))
        print("Stdout\n")
        print(failed['stdout'])
        print("Stderr\n")
        print(failed['stderr'])
        print("")

    successful = {}
    failed = {}

    for result in results:
        if type(results[result]) is int:
            successful[result] = results[result]
        else:
            failed[result] = results[result]

    print "\nFailed:"
    for f in failed:
        (benchmark, implementation, dataset, run) = f
        print("%s: %s" %
              (desc_quadruple(benchmark, implementation, dataset, run),
               failed[f]))

    print "\nSucceeded:"
    for s in successful:
        (benchmark, implementation, dataset, run) = s
        print("%s: %s" %
              (desc_quadruple(benchmark, implementation, dataset, run),
               successful[s]))


    # # could loop over all implementations, datasets
    # for impl in implementations:
    #     for ds in implementations.datasets:
            
    
    # # calculate mean:
    # # build array of run times for matching cases

    # if (number_of_runs > 1):
    #     for s in successful:

    # # Calculate average and std dev. of results. Somehow we need to iterate over those successful results that have the same benchmark, implementation, and dataset.

    # def same_case (bench1, impl1, datas1, run1, benchmark2, implementation2, dataset2, run2):
    #     return (bech1 == bech2) and (impl1 == impl2) and (datas1 == datas2)
