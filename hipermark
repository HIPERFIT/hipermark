#!/usr/bin/env python

import os
import subprocess
import json
import sys

def executable(fpath):
    """Returns true if the file exists and can be executed. """
    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)


# From http://stackoverflow.com/a/600612/496605
# when migrating to Python 3, os.makedirs(.., exists_ok = True)
# should be used instead.
def mkdir_p(path):
    """Makes a directory and all non-existing directories needed to contain this directory. 
    If the directory already exists, no error message is given and the function returns.
    """
    try:
        os.makedirs(path)
    except OSError as exc: # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else: raise


class Implementation:
    """An implementation is the source code of a solution to a general problem.
    The hierarchy is: problem -> implementation -> benchmark.
    """
    def __init__(self, hipermark_lib_dir, benchmark_lib_dir, name, directory):
        """The constructor of the class "Implementation".
        Sets the appropriate variables for an implementation.
        Also checks whether an instantiation is correctly configured with folders,
        executables etc.
        Does not compile the source code of the implementation.
        
        keyword arguments:
        hipermark_lib_dir -- /lib
        benchmark_lib_dir -- /benchmarks/<benchmark>/lib
        name -- Name of implementation's folder.
        directory -- /benchmarks/<benchmark>/implementations/<implementation>
        """
        if os.path.isdir(directory): #returns true if path is an existing directory.
            self.directory = directory # directory of implementation
        else:
            # If the path is not a directory, one of these two errors are raised.
            if os.path.isfile(directory):
                raise Exception("File %s exists, but implementation should be contained in its own directory." % directory)
            else:
                raise Exception("Implementation directory %s does not exist " % directory)

        self.name = name
        self.hipermark_lib_dir = hipermark_lib_dir
        self.benchmark_lib_dir = benchmark_lib_dir

        # Joins the directory path with "instantiate" to get the full path of the instantiate file. Sets the object var instantiatefile if this is an executable.
        instantiatefile = os.path.join(self.directory, "instantiate")
        if not executable(instantiatefile):
            raise Exception("%s does not exist or it is not an executable file" % instantiatefile)
        self.instantiatefile = instantiatefile

    
    # This function returns the spawned process' retcode, its stdout, and its stderr.
    # This function ALSO handles the input data which is the 2nd argument of the function call. This function creates the run file which contains a reference to input data.
    # This function could receive ALL possible input data and run script could then receive an argument to determine which input data should be run.
    def instantiate(self, target_dir, input_data, platform_file):
        """Runs the instantiatefile which creates a run file to run the
        implementation and also runs the local Makefile.
        The compilation of the implementation source code takes place here.

        keyword arguments:
        target_dir -- /instantiations/<benchmark>/<implementation>/<dataset>
        input_data -- path of input file
        platform_file -- /config/platform_example.json
        """
        if not os.path.isdir(target_dir):
            mkdir_p(target_dir)

        # Sets some environment variables. These are used in the subprocess.Popen function call, and they are handled in the "instantiate" files.
        # We use the current environment as a template, and the extend it with some new variables.
        # HIPERMARK_LIB_DIR: /lib/
        # HIPERMARK_BENCHMARK_LIB_DIR: /benchmarks/<benchmark>/lib
        # HIPERMARK_IMPLEMENTATION: /benchmarks/<benchmark>/implementations/<implementation>
        # HIPERMARK_INPUT: path of input file. /benchmarks/<benchmark>/dataset/input.json
        # HIPERMARK_PLATFORM: path of platform file. /config/platform_example.json
        instantiate_env = os.environ.copy()
        instantiate_env["HIPERMARK_LIB_DIR"] = self.hipermark_lib_dir
        instantiate_env["HIPERMARK_BENCHMARK_LIB_DIR"] = self.benchmark_lib_dir
        instantiate_env["HIPERMARK_IMPLEMENTATION"] = self.directory
        instantiate_env["HIPERMARK_INPUT"] = input_data
        instantiate_env["HIPERMARK_PLATFORM"] = platform_file

        # A new process which runs the instantiatefile is created. The stderr + stdout can be read from this process. 
        # close_fds=true means that the open file descriptors in this process are not inherited by the new process.
        # The env=instantiate_env gives the instantiate file some values for its variables.
        # cwd = target_dir sets the child process' folder to:
        # /instantiations/"<benchmark>-<implementation>-<dataset>"
        proc = subprocess.Popen(self.instantiatefile,
                                env=instantiate_env,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                close_fds=True,
                                cwd=target_dir)

        # Read data from stdout and stderr, until end-of-file is reached. Also waits for process to terminate.
        (stdoutdata, stderrdata) = proc.communicate(input=None)
        return (proc.returncode, stdoutdata, stderrdata)

class Benchmark:
    # This class describes a benchmark which is an abstract problem 
    # but not its specific implementation. A benchmark can in general have 
    # many implementations and this object contains all the implementations
    # in its self.implementations variable.
    def __init__(self, hipermarkdir, name, directory):
        """
        This initialization stores all the implementations and all the datasets of
        this benchmark. 
        These should be located in folders under specific benchmark folder.
        It also does some checks on the content of the implementations and the datasets 
        such as checking that an "implementations" and a "datasets" folder exists in
        the correct folder.
        
        keyword arguments:
        hipermark_dir -- The directory from which this process was invoked.
        name -- name of benchmark as defined in the folder for its source code.
        directory -- directory of the benchmark. E.g. finpar/benchmarks/OptionPricing
        """
        self.hipermarkdir = hipermarkdir
        self.name = name

        if os.path.isdir(directory):
            self.directory = directory
        else:
            raise Exception("Benchmark directory %s does not exist" % directory)
        dircontents = os.listdir(self.directory) # list of content in benchmark root directory

        # The lib directories do not have to exist.
        # The library functions for a benchmark is located in benchmarks/<benchmark>/lib/include
        self.benchmark_lib_dir = os.path.join(self.directory, "lib")
        self.hipermark_lib_dir = os.path.join(self.hipermarkdir, "lib")

        # The implementations folder contains the different implementations of a benchmark.
        if "implementations" in dircontents:
            self.__gather_implementations() #creates list of implementations to be run.
        else:
            raise Exception("No \"implementation\" directory in %s" % directory)

        if "datasets" in dircontents:
            self.datasets = os.path.join(self.directory, "datasets")
            self.__gather_datasets()
        else:
            raise Exception("No datasets directory in %s" % directory)


    def __gather_implementations(self):
        """ Creates an implementation object for each non-disabled folder in 
        /benchmarks/<benchmark>/implementations/. The objects are placed in
        a list self.implementations.
        """
        self.implementations = [] #list of implementation objects.
        # This for-loop runs over all folders (and files) found in the "implementations" folder.
        for impl in os.listdir(os.path.join(self.directory, "implementations")):
            # implpath contains the absolute path of the folder containing the specific implementation.
            implpath = os.path.join(self.directory, "implementations", impl)
            if os.path.isdir(implpath):
                # Placing a file called "disable" inside an implementation folder will disable its execution and implementation. 
                disable_file = os.path.join(implpath, "disable")
                if os.path.isfile(disable_file):
                    continue
                self.implementations.append(Implementation(self.hipermark_lib_dir,
                                                           self.benchmark_lib_dir,
                                                           impl,
                                                           os.path.join(self.directory, "implementations", impl)))

    def __gather_datasets(self):
        """Checks that each dataset in /benchmarks/<benchmark>/datasets/
        has the correct structure. Sets the self.datasets variable to an
        associative array of tuples containing the file name of the input
        file and a Python object representing the expected output.
        """
        self.datasets = {}
        for dataset in os.listdir(os.path.join(self.directory, "datasets")):
            datasetpath = os.path.join(self.directory, "datasets", dataset)
            if os.path.isdir(datasetpath):
                inputfile = os.path.join(self.directory, "datasets", dataset, "input.json")
                outputfile = os.path.join(self.directory, "datasets", dataset, "output.json")
                # Checks that input and output files are present.
                if not os.path.isfile(inputfile):
                    raise Exception("File %s missing or not a file." % inputfile)
                if not os.path.isfile(outputfile):
                    raise Exception("File %s missing or not a file." % outputfile)

                self.datasets[dataset] = { 'input_file' : inputfile,
                                           'expected_output' : read_json_file(outputfile)
                                         }

def desc_triple(benchmark, implementation, dataset):
    return "Benchmark %s, implementation %s, dataset %s" % (benchmark, implementation, dataset)

def note_result(results, benchmark, implementation, dataset, runtime):
    results[(benchmark,implementation,dataset)] = runtime

def run_some_benchmark_implementations(hipermark_dir,
                                       workdir,
                                       benchmarkdir,
                                       platform,
                                       run_implementation_fn,
                                       use_dataset_fn):
    """Declares all the objects associated with a specific benchmark:
    both benchmark and implementation objects declaration are called here
    (implementation decl. is called through benchmark decl.)
    
    keyword arguments:
    hipermark_dir -- The directory from which this process was invoked.
    workdir -- hipermark_dir/instantiations. Where the binaries are placed.
    benchmarkdir -- benchmarks/<benchmark>. Path of benchmark to run.
    platform -- path of platform file.
    run_implementation_fn -- pointer to function run_implementation_fn
    use_dataset_fn --pointer to function use_dataset_fn
    """

    # Instantiates the benchmark objects which contain the implementation objects. 
    # Does not compile.
    b = Benchmark(hipermark_dir, os.path.basename(os.path.normpath(benchmarkdir)), benchmarkdir)
    insts = []  # all succesfull compilations.
    failed_insts = [] # all failed compilations
    results = {}

    for impl in b.implementations:
        if not run_implementation_fn(impl):
            continue
        for dataset in b.datasets:
            if not use_dataset_fn(dataset):
                continue
            #instdir = <hipermark_dir>/instantiations/<benchmark>/<implementation>/<dataset>
            instdir = os.path.join(workdir,
                                   "%s-%s-%s" % (b.name, impl.name, dataset))
            print("Instantiating in %s" % instdir)
            # Here, the compilation takes place. It happens one time for each 
            # dataset in every instantiation.
            (retcode, stdout, stderr) = impl.instantiate(instdir, b.datasets[dataset]['input_file'], platform) #compilation
            if retcode != 0:
                failed_insts.append({ 'benchmark' : b.name,
                                      'implementation' : impl.name,
                                      'dataset' : dataset,
                                      'instantiation_directory' : instdir,
                                      'retcode' : retcode,
                                      'stdout' : stdout,
                                      'stderr' : stderr})
                print("Spoiler: failed")
            else:
                insts.append({ 'benchmark' : b.name,
                               'implementation' : impl.name,
                               'dataset' : dataset,
                               'instantiation_directory' : instdir,
                               'expected_output' : b.datasets[dataset]['expected_output']
                           })

    for inst in insts:
        instdir = inst['instantiation_directory']
        print("\nRunning %s" % instdir)
        stdoutpath = os.path.join(instdir, "stdout.log")
        stderrpath = os.path.join(instdir, "stderr.log")
        with open(stdoutpath, "w") as stdout:
            with open(stderrpath, "w") as stderr:
                print("Output can be found in %s and %s" % (stdoutpath, stderrpath))
                try:
                    proc = subprocess.Popen(os.path.join(instdir, "run"),
                                            stdout=stdout,
                                            stderr=stderr,
                                            stdin=subprocess.PIPE,
                                            close_fds=True,
                                            cwd=instdir)
                    proc.stdin.close() # Make sure stdin EOFs.
                    proc.wait()
                except:
                    note_result(results,
                                inst['benchmark'],
                                inst['implementation'],
                                inst['dataset'],
                                "could not run")
                    continue

        if proc.returncode != 0:
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "non-zero exit code")
            continue

        try:
            runtime = read_runtime(instdir)
        except IOError:
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "no runtime measurement")
            continue
        except ValueError:
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "non-integer runtime measurement")
            continue

        try:
            result = read_json_file(os.path.join(instdir, "result.json"))
            expected = inst['expected_output']
            if compare_json(result, expected):
                note_result(results,
                            inst['benchmark'],
                            inst['implementation'],
                            inst['dataset'],
                            runtime)
            else:
                note_result(results,
                            inst['benchmark'],
                            inst['implementation'],
                            inst['dataset'],
                            "invalid result")
        except (IOError, ValueError):
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "cannot read result")
    return (results, failed_insts)

def read_runtime(instdir):
    with open(os.path.join(instdir, "runtime.txt"), "r") as file:
        return int(file.read())

# returns an array containing the content of the JSON file.
def read_json_file(filename):
    with open(filename, "r") as file:
        return json.loads(file.read())


epsilon = 0.001

# Generic comparison function using epsilon for comparing floats.
def compare_json(json1, json2):
    if type(json1) != type(json2):
        return False

    if type(json1) is float:
        return abs(json1-json2) < epsilon

    if type(json1) is list:
        if len(json1) != len(json2):
            return False
        for x,y in zip(json1, json2):
            if not compare_json(x,y):
                return False
        return True

    if type(json1) is dict:
        keys1 = json1.keys()
        keys2 = json2.keys()
        keys1.sort()
        keys2.sort()
        if keys1 == keys2:
            for key in keys1:
                if not compare_json(json1[key], json2[key]):
                    return False
            return True
        else:
            return False

    return json1 == json2

def what_to_run():
    if len(sys.argv) == 1:
        return ("benchmarks/CalibVolDiff", None, None)
    elif len(sys.argv) == 2:
        return (sys.argv[1], None, None)
    elif len(sys.argv) == 3:
        return (sys.argv[1], sys.argv[2].split(','), None)
    elif len(sys.argv) == 4:
        return (sys.argv[1], sys.argv[2].split(','), sys.argv[3].split(','))
    else:
        exit("Usage: %s [benchmark] [implementation1,implementation2,...] [dataset1,dataset2,...]" %
             sys.argv[0])

if __name__ == '__main__':
    """This main function first creates lists of benchmarks, implementations, and datasets
    which it will run. It then runs these benchmarks and prints the results from stdout 
    and from stderr.
    """
    (benchmark, implementations, datasets) = what_to_run()
    instantiations_dir = "instantiations"
    platform_file = "config/platform_example.json"

    print("Assuming I am being run from main hipermark directory.")
    print("Will run benchmark %s." % benchmark)

    if implementations is None:
        print("Will run all implementations.")
    else:
        print("Will run these implementations: %s" % implementations)

    if datasets is None:
        print("Will use all datasets.")
    else:
        print("Will use these datasets: %s" % datasets)

    print("Will instantiate in %s." % instantiations_dir)
    print("Will use platform configuration %s." % platform_file)
    hipermark_dir = os.getcwd()

    #Pointers to these functions are passed as arguments below.
    def run_implementation_fn(impl):
        if implementations is None:
            return True
        else:
            return impl.name in implementations

    def use_dataset_fn(dataset):
        if datasets is None:
            return True
        else:
            return dataset in datasets

    (results, failed_to_instantiate) = (
        run_some_benchmark_implementations(hipermark_dir,
                                           os.path.join(hipermark_dir, instantiations_dir),
                                           os.path.join(hipermark_dir, benchmark),
                                           os.path.join(hipermark_dir, platform_file),
                                           run_implementation_fn,
                                           use_dataset_fn)
    )

    for failed in failed_to_instantiate:
        print("Failed to instantiate implementation %s with dataset %s; exit code %d." %
              (failed['implementation'], failed['dataset'], failed['retcode']))
        print("Stdout\n")
        print(failed['stdout'])
        print("Stderr\n")
        print(failed['stderr'])
        print("")

    successful = {}
    failed = {}

    for result in results:
        if type(results[result]) is int:
            successful[result] = results[result]
        else:
            failed[result] = results[result]

    print "\nFailed:"
    for f in failed:
        (benchmark, implementation, dataset) = f
        print("%s: %s" %
              (desc_triple(benchmark, implementation, dataset),
               failed[f]))

    print "\nSucceeded:"
    for s in successful:
        (benchmark, implementation, dataset) = s
        print("%s: %s" %
              (desc_triple(benchmark, implementation, dataset),
               successful[s]))
