#!/usr/bin/env python

import os
import subprocess
import json
import sys
import errno # needed to prevent error in exc.errno == errno.EEXIST
import math
import time
import hashlib
import multiprocessing
import getopt
from scipy.stats import t # Student's T distribution

# R indicates that the file is read, W that it is written
STAT_CONFIG_TEMPLATE_FILENAME = "static_configuration_template.json" #R
RUNTIME_CONFIG_TEMPLATE_FILENAME = "dynamic_configuration_template.json" #R
STAT_CONFIG_FILENAME = "static_configuration.json" #W
RUNTIME_CONFIG_FILENAME = "dynamic_configuration.json" #W
STDOUT_FILENAME = "stdout.log" #W
STDERR_FILENAME = "stderr.log" #W
RUNDATA_FILENAME = "rundata.json" #W, contains retcode, validation, runtime, tss
RESULT_FILENAME = "result.json" #W (by implementation.)
RUNTIME_FILENAME = "runtime.txt" #W (by impl.)
MAKEFILE_FILENAME = "Makefile" #R
EXT_VALIDATION_STDOUT_FILENAME = "ext_val_stdout.log" #W (by ext. val.)
EXT_VALIDATION_STDERR_FILENAME = "ext_val_stderr.log" #W (by ext. val.)

# A dict containing the data for a given run is created. These are the keys.
TIMESTAMP_START = "timestamp_start"
TIMESTAMP_END = "timestamp_end"
RETURN_CODE = "return_code"
VALIDATION = "validation"
RUNTIME = "runtime"

# Numerical constants
CONFIDENCE_LEVEL = 0.95

def executable(fpath):
    """Returns true if the file exists and can be executed. """
    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

# From http://stackoverflow.com/a/600612/496605
# when migrating to Python 3, os.makedirs(.., exists_ok = True)
# should be used instead.
def mkdir_p(path):
    """Makes a directory and all non-existing directories needed to contain this directory 
    If the directory already exists, no error message is given and the function returns.
    
    keyword arguments:
    path -- full path of the directory to be made
    """
    try:
        os.makedirs(path)
    except OSError as exc: # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else: raise

def unknown_keyword(keyword):
    raise Exception("Unknown value keyword '%s' in template." %
                    keyword)

def dynamic_keywords(keyword):
    def _list_of_cpu_numbers():
        return map(lambda count: count + 1, range(multiprocessing.cpu_count()))
    keywords = {
        'ncpus': _list_of_cpu_numbers
    }
    return keywords.get(keyword,
                        lambda: unknown_keyword(keyword))()

def static_keywords(keyword):
    keywords = {
    }
    return keywords.get(keyword,
                        lambda: unknown_keyword(keyword))()

class Implementation:
    """An implementation is the source code of a solution to a general problem.
    The hierarchy is: benchmark, implementation, static configuration, ....
    """
    def __init__(self, hipermark_lib_dir, benchmark_lib_dir, name, directory):
        """The constructor of the class "Implementation".
        Sets the appropriate variables for an implementation.
        Also checks whether an instantiation is correctly configured with folders,
        executables etc.
        Does not compile the source code of the implementation.
        
        keyword arguments:
        hipermark_lib_dir -- /lib
        benchmark_lib_dir -- /benchmarks/<benchmark>/lib
        name -- Name of implementation's folder.
        directory -- /benchmarks/<benchmark>/implementations/<implementation>
        """
        if not os.path.isdir(directory): #returns true if path is an existing directory.
            if os.path.isfile(directory):
                raise Exception("File %s exists, but implementation should be \
                contained in its own directory." % directory)
            else:
                raise Exception("Implementation directory %s does not exist "
                                % directory)
        self.directory = directory
        self.name = name
        self.hipermark_lib_dir = hipermark_lib_dir
        self.benchmark_lib_dir = benchmark_lib_dir
        self.makefile_file = os.path.join(self.directory, MAKEFILE_FILENAME)
        self._gather_static_configs()
        self._gather_runtime_configs()

    def _all_possible_configurations(self, keyword_handler, template):
        """ Takes a dict (representing a JSON) where the values are lists or
        strings.
        Finds the cartesian product of the lists and returns all points in
        the space of the cartesian product as a list of dicts where the values
        are scalars (non lists). Strings are converted to lists by the
        keyword_handler function.

        keyword arguments:
        keyword_handler -- A function converting keywords to lists
        template -- A dict with lists as values
        """
        all_configs = [{}]
        for var in template:
            options = template[var]
            if type(options) is not list:
                options = keyword_handler(options)
            new_configs = []
            for config in all_configs:
                for option in options:
                    if type(option) in [dict, list]:
                        raise Exception("Invalid value for variable %s in configuration \
                        template: %s" % (var, options))
                    new_config = config.copy()
                    new_config[var] = str(option)
                    new_configs.append(new_config)
            all_configs = new_configs
        return all_configs

    def _get_hashed_names(self, dicts):
        """ Returns map (sha256) of a list of dicts. 
        """
        return map(lambda dict: hashlib.sha256(str(dict).encode('utf-8')).\
                   hexdigest()[0:12], dicts)

    def _gather_runtime_configs(self):
        """ Reads the runtime config JSON file and produces the cartesian product
        of all values therein that are lists."""
        runtime_config_file = os.path.join(self.directory,
                                           RUNTIME_CONFIG_TEMPLATE_FILENAME)
        if not os.path.isfile(runtime_config_file):
            self.runtime_config_exists = False
            self.runtime_config_list = [{'name': "default", 'dict': {}}]
            return
        self.runtime_config_exists = True
        input_dict = read_json_file(runtime_config_file)
        runtime_configs = self._all_possible_configurations(dynamic_keywords, input_dict)
        runtime_names = self._get_hashed_names(runtime_configs)
        self.runtime_config_list = map(lambda runtime_config, runtime_name:\
                                       {'name': runtime_name,\
                                        'dict': runtime_config},\
                                       runtime_configs, runtime_names)

    def _gather_static_configs(self):
        """ Reads the static config JSON file and produces the cartesian product
        of all values therein that are lists."""
        static_config_file = os.path.join(self.directory, STAT_CONFIG_TEMPLATE_FILENAME)
        if not os.path.isfile(static_config_file):
            self.static_config_exists = False 
            self.static_config_list = [{'name': "default", 'dict': {}}]
            return
        self.static_config_exists = True
        input_dict = read_json_file(static_config_file)
        static_configs = self._all_possible_configurations(static_keywords, input_dict)
        static_names = self._get_hashed_names(static_configs)
        self.static_config_list = map(lambda static_config, static_name:\
                                       {'name': static_name,\
                                        'dict': static_config},\
                                       static_configs, static_names)

    def instantiate(self, inst_dir, static_conf, platform_file):
        """Runs 'make' which creates a run file to run the
        implementation and also runs whatever compilers may be necessary.
        The Makefile copies all the necessary source files in its own
        folder in /instantiations/.
        The compilation of the implementation source code takes place here.

        keyword arguments:
        inst_dir -- /instantiations/"<benchmark>/<implementation>" (target directory)
        platform_file -- /config/platform_example.json
        """
        if not os.path.isdir(inst_dir):
            mkdir_p(inst_dir)

        # Sets some environment variables. These are used in the subprocess.Popen function call, and they are handled in the Makfile.
        # HIPERMARK_LIB_DIR: /lib/
        # HIPERMARK_BENCHMARK_LIB_DIR: /benchmarks/<benchmark>/lib
        # HIPERMARK_IMPLEMENTATION_DIR: /benchmarks/<benchmark>/implementations/<implementation>
        # HIPERMARK_PLATFORM: path of platform file. /config/platform_example.json
        # env. vars should be set in seperate function.
        instantiate_env = os.environ.copy()
        instantiate_env["HIPERMARK_LIB_DIR"] = self.hipermark_lib_dir
        instantiate_env["HIPERMARK_BENCHMARK_LIB_DIR"] = self.benchmark_lib_dir
        instantiate_env["HIPERMARK_IMPLEMENTATION_DIR"] = self.directory
        instantiate_env["HIPERMARK_PLATFORM"] = platform_file
        for sc in static_conf:
            # Unknown if str() is nec. below.
            instantiate_env["HIPERMARK_CONFIG_" + str(sc)] = static_conf[sc]

        # cwd = inst_dir sets the child process' folder to:
        # /instantiations/"<benchmark>/<implementation>/"
        proc = subprocess.Popen(["make", "-f", self.makefile_file],
                                env=instantiate_env,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                close_fds=True,
                                cwd=inst_dir)

        # Read data from stdout and stderr, until end-of-file is reached. Also waits for process to terminate.
        (stdoutdata, stderrdata) = proc.communicate(input=None)
        # should this one write the static_conf file to the inst_dir??
        with open(os.path.join(inst_dir, STAT_CONFIG_FILENAME), "w") as fp:
            fp.write(json.dumps(static_conf))
        return (proc.returncode, stdoutdata, stderrdata)

class Benchmark:
    def __init__(self, hipermarkdir, name, directory):
        """This initialization stores all the implementations and all the datasets of
        this benchmark. It thus collects info about this benchmark's implementations.
        These source code of which should be located in folders in:
        /benchmarks/<benchmark>/implementations/
        It also does some checks on the content of the implementations and the datasets 
        such as checking that an "implementations" and a "datasets" folder exists in
        the correct folder.
        
        keyword arguments:
        hipermark_dir -- The directory from which this program was invoked.
        name -- name of benchmark as defined in the folder for its source code.
        directory -- directory of the benchmark. E.g. /benchmarks/OptionPricing
        """
        self.hipermarkdir = hipermarkdir
        self.name = name
        
        if not os.path.isdir(directory):
            raise Exception("Benchmark directory %s does not exist" % directory)
        self.directory = directory
        dircontents = os.listdir(self.directory) # list of content in benchmark root directory
        self.benchmark_lib_dir = os.path.join(self.directory, "lib")
        self.hipermark_lib_dir = os.path.join(self.hipermarkdir, "lib")

        if not ("implementations" in dircontents):
            raise Exception("No \"implementation\" directory in %s" % directory)
        self._gather_implementations() #creates list of implementations to be run.

        if not ("datasets" in dircontents):
            raise Exception("No datasets directory in %s" % directory)        
        self._gather_datasets()
        self._get_validation_function()

    def _get_validation_function(self):
        """ Checks if benchmark uses internal or an external validation function
        and sets the object variables describing this.
        """
        dircontents = os.listdir(self.directory)
        if "validation_function" in dircontents:
            self.external_validation = True
            validation_function_path = os.path.join(self.directory,
                                                    "validation_function")
            self.external_validation_file = validation_function_path
            if not executable(validation_function_path):
                raise Exception("Validation function exists at %s but it is not executable." %
                                validation_function_path)
        else:
            self.external_validation = False

    def _gather_implementations(self):
        """ Creates an implementation object for each non-disabled folder in 
        /benchmarks/<benchmark>/implementations/. The objects are placed in
        a list self.implementations.
        """
        self.implementations = [] #list of implementation objects.
        # This for-loop runs over all folders (and files) found in the "implementations" folder.
        implrootpath = os.path.join(self.directory, "implementations")
        for impl in os.listdir(implrootpath):
            implpath = os.path.join(self.directory, "implementations", impl)
            if os.path.isdir(implpath):
                disable_file = os.path.join(implpath, "disable")
                if os.path.isfile(disable_file):
                    continue
                self.implementations.append(Implementation(self.hipermark_lib_dir,
                                                           self.benchmark_lib_dir,
                                                           impl,
                                                           os.path.join(self.directory, "implementations", impl)))
            else:
                raise Exception("%s may only contain folders, not files." %implrootpath )
            
    def _gather_datasets(self):
        """Checks that each dataset in /benchmarks/<benchmark>/datasets/
        has the correct structure. Sets the self.datasets variable to an
        associative array of tuples containing the file name of the input
        file and a Python object representing the expected output.
        """
        self.datasets = {}
        dataset_dir = os.path.join(self.directory, "datasets")
        for dataset in os.listdir(dataset_dir):
            datasetpath = os.path.join(dataset_dir, dataset)
            if not os.path.isdir(datasetpath):
                raise Exception("Only directories containing datasets may be present in %s. %s is not a directory." %
                                (dataset_dir, datasetpath))
            inputfile = os.path.join(self.directory, "datasets", dataset, "input.json")
            outputfile = os.path.join(self.directory, "datasets", dataset, "output.json")
            # Checks that input and output files are present.
            if not os.path.isfile(inputfile):
                raise Exception("File %s missing or not a file." % inputfile)
            if not os.path.isfile(outputfile):
                raise Exception("File %s missing or not a file." % outputfile)
            self.datasets[dataset] = { 'input_file' : inputfile,
                                       'expected_output_file' : outputfile
            }

def desc_octuple(benchmark, implementation, stat_config_name, stat_config_dict, runtime_name, runtime_dict, rundataset, run):
    if stat_config_dict == {}:
        stat_config_dict = "None"
    return ("Benchmark %s, implementation %s, static configuration name %s,\
    static configuration description: %s, runtime configuration name %s,\
    runtime configuration description: %s, dataset %s, run %d (of this execution)" %
            (benchmark, implementation, stat_config_name, str(stat_config_dict), runtime_name, str(runtime_dict), dataset, run))

def desc_septuple(benchmark, implementation, stat_config_name, stat_config_dict, runtime_name, runtime_dict, dataset):
    if stat_config_dict == {}:
        stat_config_dict = "None"
    return ("Benchmark %s, implementation %s, static configuration name %s,\
    static configuration description: %s, runtime configuration name %s,\
    runtime configuration description: %s, dataset %s" %
            (benchmark, implementation, stat_config_name, str(stat_config_dict), runtime_name, str(runtime_dict), dataset))

# This is a local variable that is only used in evaluation of this execution. For serious data analysis, the folder structure is used.
def note_result(results, benchmark, implementation, stat_name, stat_dict, runtime_name, runtime_dict, dataset, run, runtime):
    """Stores the runtime of the execution of an implementation in a dictionary.
    This dictionary has triple's (benchmark, implementation, dataset) as key.
    The values are stored in a list which indexes over the run counter.

    keyword arguments:
    results -- name of dictionary in which runtime is saved (string)
    benchmark -- name of benchmark (string)
    implementation -- name of implementation (string)
    stat_name -- hash of str(stat_dict)
    stat_dict -- dictionary of static environment variables
    dataset -- name of dataset. (string)
    run -- run counter (int)
    runtime -- runtime as reported by the implementation. Stored in a list in results.
    """
    if not((benchmark, implementation, stat_name, str(stat_dict), runtime_name, str(runtime_dict), dataset) in results.keys()):
        results[(benchmark, implementation, stat_name, str(stat_dict), runtime_name, str(runtime_dict), dataset)] = []
    results[(benchmark,implementation, stat_name, str(stat_dict), runtime_name, str(runtime_dict), dataset)].append(runtime)

def get_nth_moment (result_times, n):
    """ 
    Calculates the nth moment of a list.

    Keyword values:
    result_times -- list of result times (floats)
    n -- integer describing that statistical moment of result_times which is returned
    """
    powers = map(lambda x: x**n, result_times)
    return sum(powers)/float(len(result_times))

def get_mean_and_unc(result_times):
    """ Returns mean and uncertainty of mean for a list. The uncertainty is 
    calculated from Student's T distribution since the population variance is
    unknown.

    Keyword values:
    result_times -- list of result times (floats)

    return values:
    sample_mean -- the observed mean
    unc -- The uncertainty according to Student's T distribution
    """
    N = len(result_times)
    sample_mean = get_nth_moment(result_times, 1)
    sample_variance = sum([(x - sample_mean)**2 for x in result_times])/float(N - 1)
    # since the uncertainty in each measurement is 0.5 ms, the
    # sample_variance should never go below 0.5/sqrt(N).
    if sample_variance < 0.5/math.sqrt(N):
        sample_variance = 0.5/math.sqrt(N)
    estimated_std_err = math.sqrt(sample_variance) / math.sqrt(N)
    interval = t.interval(CONFIDENCE_LEVEL,
                          N - 1,
                          loc = sample_mean,
                          scale = estimated_std_err)
    unc = (interval[1] - interval[0])/2
    return (sample_mean, unc)

# def get_nth_moment (result_times, n):
#     """ 
#     Calculates the nth moment of a list.

#     Keyword values:
#     result_times -- list of result times (floats)
#     n -- integer describing that statistical moment of result_times which is returned
#     """
#     powers = map(lambda x: x**n, result_times)
#     return sum(powers)/float(len(powers))

# def get_average_and_stddev (result_times):
#     """ Returns average and stddev for a list.

#     Keyword values:
#     result_times -- list of result times (floats)
#     """
#     av = get_nth_moment(result_times, 1)
#     return (av, math.sqrt(get_nth_moment(result_times, 2) - av**2))

def find_run_counter(target_dir):
    """Finds the run number for the first run of this execution.
    This is done to avoid overwriting output results.

    Keyword values:
    target_dir -- The dir where the count is performed.
    """
    dirlist = os.listdir(target_dir)
    max_number = 0
    empty = True
    try:
        a = []
        for path in dirlist:
            # Allow for the runtime config file to be located here.
            if path == RUNTIME_CONFIG_FILENAME:
                continue
            empty = False
            b = int(path)
            if b < 0:
                raise Exception("Only non-negative run numbers (i.e., directory names)\
                are allowed in %s." % target_dir)
            a.append(b)
    except ValueError:
        raise Exception("The folder %s may only contain folders with integer name." %
                        target_dir)
    if not empty:
        max_number = max(a) + 1
    return max_number

def configure_environment_vars(env_dict, instantiate_env):
    """ Sets the environment variables according to the dict.
    """
    for env_name in env_dict:
        instantiate_env["HIPERMARK_CONFIG_" + str(env_name)] = env_dict[env_name]

def run_case(results, inst, dataset, runtimeconf):
    inst_dir = inst['instantiation_directory']
    path_for_outputs = os.path.join(inst_dir, "run_output/", dataset, runtimeconf['name'])
    benchmark = inst['benchmark']
    dataset_file = benchmark.datasets[dataset]['input_file']
    expected_output_file = benchmark.datasets[dataset]['expected_output_file']

    mkdir_p(path_for_outputs)
    # write runtime configuration into JSON file
    with open(os.path.join(path_for_outputs, RUNTIME_CONFIG_FILENAME), "w") as fp:
        fp.write(json.dumps(runtimeconf['dict']))
    first_run_counter = find_run_counter(path_for_outputs)
    for run in range(first_run_counter, number_of_runs + first_run_counter):
        run_output = os.path.join(inst_dir, "run_output/", dataset, runtimeconf['name'], str(run))
        mkdir_p(run_output)
        print("\nRunning %s" % run_output)
        stdoutpath = os.path.join(run_output, STDOUT_FILENAME)
        stderrpath = os.path.join(run_output, STDERR_FILENAME)
        resultpath = os.path.join(run_output, RESULT_FILENAME)
        runtimepath = os.path.join(run_output, RUNTIME_FILENAME)                
        #returncodepath = os.path.join(run_output, RETURNCODE_FILENAME)
        #validationpath = os.path.join(run_output, VALIDATION_FILENAME)
        #timestampstartpath = os.path.join(run_output, TIMESTAMPSTART_FILENAME)
        #timestampendpath = os.path.join(run_output, TIMESTAMPEND_FILENAME)
        with open(stdoutpath, "w") as stdout:
            with open(stderrpath, "w") as stderr:
                rundata_dict = {}
                print("Output can be found in %s and %s" % (stdoutpath, stderrpath))
                try:
                    instantiate_env = os.environ.copy()
                    #HIPERMARK_RESULT/RUNTIME: path for impl. to store result/runtime
                    instantiate_env["HIPERMARK_INPUT_FILE"] = dataset_file
                    instantiate_env["HIPERMARK_RESULT"] = resultpath
                    instantiate_env["HIPERMARK_RUNTIME"] = runtimepath
                    configure_environment_vars(runtimeconf['dict'],
                                               instantiate_env)
                    timestamp = time.time()
                    rundata_dict[TIMESTAMP_START] = str(timestamp)
                    proc = subprocess.Popen(["make", "-f", inst['implementation_makefile'], "run"],
                                            env=instantiate_env,
                                            stdout=stdout,
                                            stderr=stderr,
                                            stdin=subprocess.PIPE,
                                            close_fds=True,
                                            cwd=inst_dir) #run a case
                    proc.stdin.close() # Make sure stdin EOFs.
                    proc.wait()
                    timestamp = time.time()
                    rundata_dict[TIMESTAMP_END] = str(timestamp)
                    rundata_dict[RETURN_CODE] = str(proc.returncode)
                except:
                    note_result(results,
                                benchmark.name,
                                inst['implementation'].name,
                                inst['stat_config_name'],
                                inst['stat_config_dict'],
                                runtimeconf['name'],
                                runtimeconf['dict'],
                                dataset,
                                run,
                                "could not run")
                    print("Spoiler: failed")
                    continue
        if proc.returncode != 0:
            note_result(results,
                        benchmark.name,
                        inst['implementation'].name,
                        inst['stat_config_name'],
                        inst['stat_config_dict'],
                        runtimeconf['name'],
                        runtimeconf['dict'],
                        dataset,
                        run,
                        "non-zero exit code")
            print("Spoiler: failed")
            continue
        
        try:
            runtime = read_runtime(run_output)
            rundata_dict[RUNTIME] = str(runtime)
        except IOError:
            note_result(results,
                        benchmark.name,
                        inst['implementation'].name,
                        inst['stat_config_name'],
                        inst['stat_config_dict'],
                        runtimeconf['name'],
                        runtimeconf['dict'],
                        dataset,
                        run,
                        "no runtime measurement")
            rundata_dict[RUNTIME] = "no runtime measurement"
            continue
        except ValueError:
            note_result(results,
                    benchmark.name,
                        inst['implementation'].name,
                        inst['stat_config_name'],
                        inst['stat_config_dict'],
                        runtimeconf['name'],
                        runtimeconf['dict'],
                        dataset,
                        run,
                        "non-integer runtime measurement")
            rundata_dict[RUNTIME] = "non-integer runtime measurement"
            continue
        
        try:
            resultfile = os.path.join(run_output, RESULT_FILENAME)
            if benchmark.external_validation:
                evstdoutpath = os.path.join(run_output, EXT_VALIDATION_STDOUT_FILENAME)
                evstderrpath = os.path.join(run_output, EXT_VALIDATION_STDERR_FILENAME)
                with open(evstdoutpath, 'w') as evstdout:
                    with open(evstdoutpath, 'w') as evstderr:
                        print("Launching external validation")
                        proc = subprocess.Popen([benchmark.external_validation_file, resultfile, expected_output_file],
                                                stdout=evstdout,
                                                stderr=evstderr,
                                                stdin=subprocess.PIPE,
                                                close_fds=True,
                                                cwd=inst_dir)
                        #return code 0 of above means succesfull val.
                proc.stdin.close() # Make sure stdin EOFs.
                proc.wait()
                match = 1 ^ proc.returncode
                rundata_dict[VALIDATION] = str(int(match))
            else:
                result = read_json_file(resultfile)
                expected = read_json_file(expected_output_file)
                match = compare_json(result, expected)
                rundata_dict[VALIDATION] = str(int(match))
            if match:
                note_result(results,
                            benchmark.name,
                            inst['implementation'].name,
                            inst['stat_config_name'],
                            inst['stat_config_dict'],
                            runtimeconf['name'],
                            runtimeconf['dict'],
                            dataset,
                            run,
                            runtime)
            else:
                note_result(results,
                            benchmark.name,
                            inst['implementation'].name,
                            inst['stat_config_name'],
                            inst['stat_config_dict'],
                            runtimeconf['name'],
                            runtimeconf['dict'],
                            dataset,
                            run,
                            "result did not match expected result")
        except (IOError, ValueError):
            note_result(results,
                        benchmark.name,
                        inst['implementation'].name,
                        inst['stat_config_name'],
                        inst['stat_config_dict'],
                        runtimeconf['name'],
                        runtimeconf['dict'],
                        dataset,
                        run,
                        "cannot read result")
            rundata_dict[VALIDATION] = "cannot read result\n"
        f = open(os.path.join(run_output, RUNDATA_FILENAME), "w")
        f.write(json.dumps(rundata_dict))
        f.close()

def run_some_benchmark_implementations(hipermark_dir,
                                       workdir,
                                       benchmarkdir,
                                       number_of_runs,
                                       platform,
                                       run_implementation_fn,
                                       use_dataset_fn):
    """Declares all the objects associated with a specific benchmark:
    both benchmark and implementation objects declaration are called here
    (implementation decl. is called through benchmark decl.). 
    
    keyword arguments:
    hipermark_dir -- The directory from which this process was invoked.
    workdir -- Folder where instantiations (binaries and other) are stored.
    benchmarkdir -- benchmarks/<benchmark>. Path of benchmark to run.
    number_of_runs -- number of times that each case (dataset) will be run
    platform -- path of platform file.
    run_implementation_fn -- pointer to function run_implementation_fn
    use_dataset_fn --pointer to function use_dataset_fn
    """

    # Instantiates the benchmark objects which contain the implementation objects. 
    # Does not compile.
    b = Benchmark(hipermark_dir,
                  os.path.basename(os.path.normpath(benchmarkdir)),
                  benchmarkdir)
    insts = []  # all succesfull implementation compilations.
    failed_insts = [] # all failed implementation compilations
    results = {} # dict in Python. Like an associative array. key: (bench, impl, dataset)
    # value: list of runtimes. Manipulated through note_result function.
    # Things are overwritten here.

    # Running all Makefiles
    for impl in b.implementations:
        if not run_implementation_fn(impl):
            continue
        for sconf in impl.static_config_list:
            inst_dir = os.path.join(os.getcwd(), workdir, "%s/%s/%s" %
                                    (b.name, impl.name, sconf['name']))
            print("Instantiating source code in %s" % inst_dir)
            (retcode, stdout, stderr) = impl.instantiate(inst_dir, sconf['dict'], platform)#compile source code
            if retcode != 0:
                failed_insts.append({ 'benchmark' : b,
                                      'implementation' : impl,
                                      'stat_config_name': sconf['name'],
                                      'stat_config_dict': sconf['dict'],
                                      'instantiation_directory' : inst_dir,
                                      'retcode' : retcode,
                                      'stdout' : stdout,
                                      'stderr' : stderr})
                print("Ran instantiate. Spoiler: failed")
            else:
                insts.append({ 'benchmark' : b,
                               'implementation' : impl,
                               'stat_config_name': sconf['name'],
                               'stat_config_dict': sconf['dict'],
                               'instantiation_directory' : inst_dir,
                               'datasets' : b.datasets,
                               'implementation_makefile' : impl.makefile_file})

    # Here, all data instantiations are looped over and the makefile run
    # is executed for each dataset*statconfig.
    for inst in insts:
        for dataset in inst['datasets']:
            if not use_dataset_fn(dataset):
                continue
            for runtimeconf in inst['implementation'].runtime_config_list:
                run_case(results, inst, dataset, runtimeconf)
    return (results, failed_insts)
                        
                        
def read_runtime(output_dir):
    """Reads the runtime.txt holding the run time, and created by the executable.

    keyword arguments:
    output_dir -- absolute path of dir in which runtime results are stored
    """
    with open(os.path.join(output_dir, "runtime.txt"), "r") as file:
        return int(file.read())


def read_json_file(filename):
    """ returns an array containing the content of the JSON file.

    keyword arguments:
    filename -- name of json file to interpret
    """
    with open(filename, "r") as file:
        return json.loads(file.read())


# Should a relative value be used instead of epsilon?
epsilon = 0.001


def compare_json(json1, json2):
    """Generic comparison function using epsilon for comparing floats.
    The order of the arguments is arbitrary.
    
    keyword aguments:
    json1 -- calculated result
    json2 -- expected result (Function is symmetrical in json1, json2)
    """
    if type(json1) != type(json2):
        return False

    if type(json1) is float:
        return abs(json1-json2) < epsilon

    if type(json1) is list:
        if len(json1) != len(json2):
            return False
        for x,y in zip(json1, json2):
            if not compare_json(x,y):
                return False
        return True

    if type(json1) is dict:
        keys1 = json1.keys()
        keys2 = json2.keys()
        keys1.sort()
        keys2.sort()
        if keys1 == keys2:
            for key in keys1:
                if not compare_json(json1[key], json2[key]):
                    return False
            return True
        else:
            return False

    return json1 == json2

if __name__ == '__main__':
    """This main function first creates lists of benchmarks, implementations, and datasets
    which it will run. It then runs these benchmarks and prints the results from stdout 
    and from stderr.
    """
    def _parse_sysarg(sysargv):
        try:
            opts, args = getopt.getopt(sysargv,
                                       "o:n:b:m:d:",
                                       ["output_dir=",
                                        "number_of_runs=",
                                        "benchmark_folder=",
                                        "implementation_name=",
                                        "dataset_name="])
        except getopt.GetoptError as err:
            print str(err)
            sys.exit(2)
        implementations = []
        datasets = []
        for o, a in opts:
            if o in ("-o", "--output-dir="):
                instdir = a
            elif o in ("-n", "--number-of-runs="):
                number_of_runs = int(a)
            elif o in ("-b", "--benchmark-folder="):
                benchmark_folder = a
            elif o in ("-m", "--implementation-name="):
                implementations.append(a)
            elif o in ("-d", "--dataset-name="):
                datasets.append(a)
            else:
                print("unhandled option given as command line argument: %s", o)
                print("USAGE: hipermark [(-o|--output-dir=)|(-n|--number-of-runs=)|(-b|benchmark-folder=)|(-m|implementation-name=)|(-d|dataset-name=)]")
                assert False
        if not os.path.isdir(benchmark_folder): ## more checks to valid arguements could be given here.
            raise Exception("The benchmark dir must be a directory.")
        return instdir, number_of_runs, benchmark_folder, implementations, datasets

    
    (instantiations_dir, number_of_runs, benchmark, implementations, datasets) = _parse_sysarg(sys.argv[1:])
    platform_file = "config/platform_example.json"

    print("Assuming I am being run from main hipermark directory.")
    
    # somewhere it should be checked that these benchmarks and these implementations exist.
    # Generally, the handling of benchmark/implementation sanity checks should be looked at.
    print("Will run benchmark %s." % benchmark)

    if implementations is None:
        print("Will run all implementations.")
    else:
        print("Will run these implementations: %s" % implementations)

    if datasets is None:
        print("Will use all datasets.")
    else:
        print("Will use these datasets: %s" % datasets)

    print("Each case will be run %d times" % number_of_runs)

    print("Will instantiate in %s." % instantiations_dir)
    print("Will use platform configuration %s." % platform_file)
    hipermark_dir = os.getcwd() # current folder. Is used to set workdir

    #Pointers to these functions are passed as arguments below.
    def run_implementation_fn(impl):
        if implementations is None:
            return True
        else:
            return impl.name in implementations

    def use_dataset_fn(dataset):
        if datasets is None:
            return True
        else:
            return dataset in datasets

    (results, failed_to_instantiate) = (
        run_some_benchmark_implementations(hipermark_dir,
                                           instantiations_dir,
                                           os.path.join(hipermark_dir, benchmark),
                                           number_of_runs,
                                           os.path.join(hipermark_dir, platform_file),
                                           run_implementation_fn,
                                           use_dataset_fn)
    )

    for failed in failed_to_instantiate:
        print("Failed to instantiate benchmark %s, implementation %s, static config dict %s, static config name %s. Exit code was %d." %
              (failed['benchmark'], failed['implementation'], failed['stat_config_dict'], failed['stat_config_name'], failed['retcode']))
        print("Stdout\n")
        print(failed['stdout'])
        print("Stderr\n")
        print(failed['stderr'])
        print("")

    # These should due to consistency be treated as immutables!
    successful_octuple = {}
    successful_septuple = {}
    failed_octuple = {}

    for result_septuple in results:
        successful_counter = 0
        for run in range(len(results[result_septuple])):
            run_desc = results[result_septuple][run] # int for success, string for fail
            run_tuple = (run,)
            result_octuple = result_septuple + run_tuple
            if type(run_desc) is int:
                successful_octuple[result_octuple] = run_desc
                if successful_counter == 0:
                    successful_septuple[result_septuple] = []
                successful_counter += 1
                successful_septuple[result_septuple].append(run_desc)
            else:
                failed_octuple[result_octuple] = run_desc

    # DO NOT MODIFY successful_triple or successful_quadruple after this line!

    if failed_octuple == {}:
        print("\nNo runs failed.")
    else:
        print("\nThe following runs failed:")
        for f in failed_octuple:
            (benchmark, implementation, stat_config_name, stat_config_dict, runtime_name, runtime_dict, dataset, run) = f
            print("%s: %s" %
                  (desc_octuple(benchmark, implementation, stat_config_name, stat_config_dict, runtime_name, runtime_dict, dataset, run),
                   failed_octuple[f]))

## When uncommented, these lines prints data from all succesful runs.        
    # print("\nSucceeded:")
    # for s in successful_octuple:
    #     (benchmark, implementation, stat_config_name, stat_config_dict, runtime_name, runtime_dict, dataset, run) = s
    #     print("%s: %s" %
    #           (desc_octuple(benchmark, implementation, stat_config_name, stat_config_dict, runtime_name, runtime_dict, dataset, run),
    #            successful_octuple[s]))

    print("\nAverage Run Times:")
    for s in successful_septuple:
        (benchmark, implementation, stat_config_name, stat_config_dict, runtime_name, runtime_dict, dataset) = s
        av, unc = get_mean_and_unc(successful_septuple[s])
        rel_unc = unc/float(av)
        print("%s: mean runtime: %d, uncertainty of mean: %.1f, relative uncertainty: %.1E" %
              (desc_septuple(benchmark, implementation, stat_config_name, stat_config_dict, runtime_name, runtime_dict, dataset), av, unc, rel_unc))
