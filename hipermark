#!/usr/bin/env python

import os
import subprocess
import json
import sys

def executable(fpath):
    """Returns true if the file exists and can be executed. """
    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)


# From http://stackoverflow.com/a/600612/496605
# when migrating to Python 3, os.makedirs(.., exists_ok = True)
# should be used instead.
def mkdir_p(path):
    """Makes a directory and all non-existing directories needed to contain this directory."""
    try:
        os.makedirs(path)
    except OSError as exc: # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else: raise


class Implementation:
    """ 
    An implementation is the source code of a solution to a general problem. The hierarchy is:
    problem -> implementation -> benchmark.
    """
    def __init__(self, hipermark_lib_dir, benchmark_lib_dir, name, directory):
        """
        The constructor of the class "Implementation". The self represents the instance of the object itself.
        A function call "Implemantion()" will create an object and pass this object to __init__.
        
        keyword arguments:
        hipermark_lib_dir --
        benchmark_bli_dir --
        name --
        directory --
        """
        if os.path.isdir(directory): #returns true if path is an existing directory.
            self.directory = directory # directory of implementation
        else:
            # If the path is not a directory, one of these two errors are raised.
            if os.path.isfile(directory):
                raise Exception("File %s exists, but should be an implementation directory" % directory)
            else:
                raise Exception("Implementation directory %s does not exist " % directory)

        self.name = name
        self.hipermark_lib_dir = hipermark_lib_dir
        self.benchmark_lib_dir = benchmark_lib_dir

        # Joins the directory path with "instantiate" to get the full path of the instantiate file. Sets the object var instantiatefile if this is an executable.
        instantiatefile = os.path.join(self.directory, "instantiate")
        if not executable(instantiatefile):
            raise Exception("%s is not an executable file" % instantiatefile)
        self.instantiatefile = instantiatefile

    
    # 
    # This function returns the spawned process' retcode, its stdout, and its stderr.
    # This function ALSO handles the input data which is the 2nd argument of the function call. This function creates the run file which contains a reference to input data.
    # This function could receive ALL possible input data and run script could then receive an argument to determine which input data should be run.
    def instantiate(self, where, input_data, platform_file):
        """
        Runs the instantiatefile which creates a run file to run the implementation and also runs the local Makefile.
        The compilation of the implementation source code takes place here.
        """
        if not os.path.isdir(where):
            mkdir_p(where)

        # Sets some environment variables. These are used in the subprocess.Popen function call, and they are handled in the "instantiate" files.
        # We use the current environment as a template, and then
        # extend it with some new variables.
        instantiate_env = os.environ.copy()
        instantiate_env["HIPERMARK_LIB_DIR"] = self.hipermark_lib_dir
        instantiate_env["HIPERMARK_BENCHMARK_LIB_DIR"] = self.benchmark_lib_dir
        instantiate_env["HIPERMARK_IMPLEMENTATION"] = self.directory
        instantiate_env["HIPERMARK_INPUT"] = input_data
        instantiate_env["HIPERMARK_PLATFORM"] = platform_file

        # A new process which runs the instantiatefile is created. The stderr + stdout can be read from this process. 
        # close_fds=true means that the open file descriptors in this process are not inherited by the new process.
        # The env=instantiate_env gives the instantiate file some values for its variables.
        proc = subprocess.Popen(self.instantiatefile,
                                env=instantiate_env,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                close_fds=True,
                                cwd=where)
        # Reads data from stdout + stderr of spawned process until EOF is reached. Also waits for process to terminate.
        (stdoutdata, stderrdata) = proc.communicate(input=None)
        return (proc.returncode, stdoutdata, stderrdata)

# This class describes a benchmark which is an abstract problem but not its specific implementation. A benchmark can contain many implementations.
class Benchmark:
    # In the constructor, we do some basic sanity-checking.  The
    # primary purpose here is to provide better error messages than if
    # we fail to read files later down the line.
    # This initialization stores all the implementations and all the datasets of this benchmark. These should be located in folders under specific benchmark folder.
    # It also does some checks on the content of the implementations and the datasets such as checking that an "implementations" and a "datasets" folder exists in the correct folder.
    # directory 
    def __init__(self, hipermarkdir, name, directory):
        self.hipermarkdir = hipermarkdir
        self.name = name

        if os.path.isdir(directory):
            self.directory = directory
        else:
            raise Exception("Benchmark directory %s does not exist" % directory)
        dircontents = os.listdir(self.directory) # list of content in directory

        # The lib directories do not have to exist.
        # The library functions for a benchmark is located in benchmarks/<benchmark>/lib/include
        self.benchmark_lib_dir = os.path.join(self.directory, "lib")
        self.hipermark_lib_dir = os.path.join(self.hipermarkdir, "lib")

        # The implementations folder contains the different implementations of a benchmark.
        if "implementations" in dircontents:
            self.__gather_implementations() #creates list of implementations to be run.
        else:
            raise Exception("No implementation directory in %s" % directory)

        if "datasets" in dircontents:
            self.datasets = os.path.join(self.directory, "datasets")
            self.__gather_datasets()
        else:
            raise Exception("No datasets directory in %s" % directory)


# Creates a new Implementation object for each non-disabled folder inside /benchmarks/<benchmark> and appends these to a list of implementations which is part of the class' variables. The "Implementation" function call creates a new object and runs __init__ in the Implementations class.    
    def __gather_implementations(self):
        self.implementations = [] #list of implementation objects.
        # This for loop runs over all folders (and files) found in the "implementations" folder.
        for impl in os.listdir(os.path.join(self.directory, "implementations")):
            # implpath contains the absolute path of the folder containing the specific implementation.
            implpath = os.path.join(self.directory, "implementations", impl)
            if os.path.isdir(implpath):
                # Placing a file called "disable" inside an implementation folder will disable its execution and implementation. 
                disable_file = os.path.join(implpath, "disable")
                if os.path.isfile(disable_file):
                    continue
                self.implementations.append(Implementation(self.hipermark_lib_dir,
                                                           self.benchmark_lib_dir,
                                                           impl,
                                                           os.path.join(self.directory, "implementations", impl)))

# Sets the self.datasets variable to an associative array of tuples containing the file name of the input file 
    def __gather_datasets(self):
        self.datasets = {}
        # For each directory (or file) in "datasets", do
        for dataset in os.listdir(os.path.join(self.directory, "datasets")):
            datasetpath = os.path.join(self.directory, "datasets", dataset)
            if os.path.isdir(datasetpath):
                inputfile = os.path.join(self.directory, "datasets", dataset, "input.json")
                outputfile = os.path.join(self.directory, "datasets", dataset, "output.json")
                # Checks that input and output files are present.
                if not os.path.isfile(inputfile):
                    raise Exception("File %s missing or not a file." % inputfile)
                if not os.path.isfile(outputfile):
                    raise Exception("File %s missing or not a file." % outputfile)

                # The path of the input file is stored. The content of the JSON output file is stored as an array in datasets array in the benchmark class.
                self.datasets[dataset] = { 'input_file' : inputfile,
                                           'expected_output' : read_json_file(outputfile)
                                         }

def desc_triple(benchmark, implementation, dataset):
    return "Benchmark %s, implementation %s, dataset %s" % (benchmark, implementation, dataset)

def note_result(results, benchmark, implementation, dataset, runtime):
    results[(benchmark,implementation,dataset)] = runtime

def run_some_benchmark_implementations(hipermark_dir,
                                       workdir,
                                       benchmarkdir,
                                       platform,
                                       run_implementation_fn,
                                       use_dataset_fn):
    b = Benchmark(hipermark_dir, os.path.basename(os.path.normpath(benchmarkdir)), benchmarkdir)
    insts = []
    failed_insts = []
    results = {}

    for impl in b.implementations:
        if not run_implementation_fn(impl):
            continue
        for dataset in b.datasets:
            if not use_dataset_fn(dataset):
                continue
            instdir = os.path.join(workdir,
                                   "%s-%s-%s" % (b.name, impl.name, dataset))
            print("Instantiating in %s" % instdir)
            (retcode, stdout, stderr) = impl.instantiate(instdir, b.datasets[dataset]['input_file'], platform)
            if retcode != 0:
                failed_insts.append({ 'benchmark' : b.name,
                                      'implementation' : impl.name,
                                      'dataset' : dataset,
                                      'instantiation_directory' : instdir,
                                      'retcode' : retcode,
                                      'stdout' : stdout,
                                      'stderr' : stderr})
                print("Spoiler: failed")
            else:
                insts.append({ 'benchmark' : b.name,
                               'implementation' : impl.name,
                               'dataset' : dataset,
                               'instantiation_directory' : instdir,
                               'expected_output' : b.datasets[dataset]['expected_output']
                           })

    for inst in insts:
        instdir = inst['instantiation_directory']
        print("\nRunning %s" % instdir)
        stdoutpath = os.path.join(instdir, "stdout.log")
        stderrpath = os.path.join(instdir, "stderr.log")
        with open(stdoutpath, "w") as stdout:
            with open(stderrpath, "w") as stderr:
                print("Output can be found in %s and %s" % (stdoutpath, stderrpath))
                try:
                    proc = subprocess.Popen(os.path.join(instdir, "run"),
                                            stdout=stdout,
                                            stderr=stderr,
                                            stdin=subprocess.PIPE,
                                            close_fds=True,
                                            cwd=instdir)
                    proc.stdin.close() # Make sure stdin EOFs.
                    proc.wait()
                except:
                    note_result(results,
                                inst['benchmark'],
                                inst['implementation'],
                                inst['dataset'],
                                "could not run")
                    continue

        if proc.returncode != 0:
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "non-zero exit code")
            continue

        try:
            runtime = read_runtime(instdir)
        except IOError:
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "no runtime measurement")
            continue
        except ValueError:
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "non-integer runtime measurement")
            continue

        try:
            result = read_json_file(os.path.join(instdir, "result.json"))
            expected = inst['expected_output']
            if compare_json(result, expected):
                note_result(results,
                            inst['benchmark'],
                            inst['implementation'],
                            inst['dataset'],
                            runtime)
            else:
                note_result(results,
                            inst['benchmark'],
                            inst['implementation'],
                            inst['dataset'],
                            "invalid result")
        except (IOError, ValueError):
            note_result(results,
                        inst['benchmark'],
                        inst['implementation'],
                        inst['dataset'],
                        "cannot read result")
    return (results, failed_insts)

def read_runtime(instdir):
    with open(os.path.join(instdir, "runtime.txt"), "r") as file:
        return int(file.read())

# returns an array containing the content of the JSON file.
def read_json_file(filename):
    with open(filename, "r") as file:
        return json.loads(file.read())


epsilon = 0.001

# Generic comparison function using epsilon for comparing floats.
def compare_json(json1, json2):
    if type(json1) != type(json2):
        return False

    if type(json1) is float:
        return abs(json1-json2) < epsilon

    if type(json1) is list:
        if len(json1) != len(json2):
            return False
        for x,y in zip(json1, json2):
            if not compare_json(x,y):
                return False
        return True

    if type(json1) is dict:
        keys1 = json1.keys()
        keys2 = json2.keys()
        keys1.sort()
        keys2.sort()
        if keys1 == keys2:
            for key in keys1:
                if not compare_json(json1[key], json2[key]):
                    return False
            return True
        else:
            return False

    return json1 == json2

def what_to_run():
    if len(sys.argv) == 1:
        return ("benchmarks/CalibVolDiff", None, None)
    elif len(sys.argv) == 2:
        return (sys.argv[1], None, None)
    elif len(sys.argv) == 3:
        return (sys.argv[1], sys.argv[2].split(','), None)
    elif len(sys.argv) == 4:
        return (sys.argv[1], sys.argv[2].split(','), sys.argv[3].split(','))
    else:
        exit("Usage: %s [benchmark] [implementation1,implementation2,...] [dataset1,dataset2,...]" %
             sys.argv[0])

if __name__ == '__main__':
    (benchmark, implementations, datasets) = what_to_run()
    instantiations_dir = "instantiations"
    platform_file = "config/platform_example.json"

    print("Assuming I am being run from main hipermark directory.")
    print("Will run benchmark %s." % benchmark)

    if implementations is None:
        print("Will run all implementations.")
    else:
        print("Will run these implementations: %s" % implementations)

    if datasets is None:
        print("Will use all datasets.")
    else:
        print("Will use these datasets: %s" % datasets)

    print("Will instantiate in %s." % instantiations_dir)
    print("Will use platform configuration %s." % platform_file)
    hipermark_dir = os.getcwd()

    def run_implementation_fn(impl):
        if implementations is None:
            return True
        else:
            return impl.name in implementations

    def use_dataset_fn(dataset):
        if datasets is None:
            return True
        else:
            return dataset in datasets

    (results, failed_to_instantiate) = (
        run_some_benchmark_implementations(hipermark_dir,
                                           os.path.join(hipermark_dir, instantiations_dir),
                                           os.path.join(hipermark_dir, benchmark),
                                           os.path.join(hipermark_dir, platform_file),
                                           run_implementation_fn,
                                           use_dataset_fn)
    )

    for failed in failed_to_instantiate:
        print("Failed to instantiate implementation %s with dataset %s; exit code %d." %
              (failed['implementation'], failed['dataset'], failed['retcode']))
        print("Stdout\n")
        print(failed['stdout'])
        print("Stderr\n")
        print(failed['stderr'])
        print("")

    successful = {}
    failed = {}

    for result in results:
        if type(results[result]) is int:
            successful[result] = results[result]
        else:
            failed[result] = results[result]

    print "\nFailed:"
    for f in failed:
        (benchmark, implementation, dataset) = f
        print("%s: %s" %
              (desc_triple(benchmark, implementation, dataset),
               failed[f]))

    print "\nSucceeded:"
    for s in successful:
        (benchmark, implementation, dataset) = s
        print("%s: %s" %
              (desc_triple(benchmark, implementation, dataset),
               successful[s]))
