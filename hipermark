#!/usr/bin/env python

import os
import subprocess
import json
import sys
import errno # needed to prevent error in exc.errno == errno.EEXIST
import math
import time
# The 'six' library could be imported since it is compatible with both P2 and P3.

STAT_CONFIG_FILENAME = "statconfig.json"
STDOUT_FILENAME = "stdout.log"
STDERR_FILENAME = "stderr.log"
RETURNCODE_FILENAME = "returncode.txt"
VALIDATION_FILENAME = "validation.txt"
RESULT_FILENAME = "result.json"
RUNTIME_FILENAME = "runtime.txt"
TIMESTAMPSTART_FILENAME = "tsstartrun.txt"
TIMESTAMPEND_FILENAME = "tsendrun.txt"
INSTANTIATE_FILENAME = "instantiate"
INSTANTIATE_DATA_FILENAME = "instantiate_data"
EXT_VALIDATION_STDOUT_FILENAME = "ext_val_stdout.log"
EXT_VALIDATION_STDERR_FILENAME = "ext_val_stderr.log"

def executable(fpath):
    """Returns true if the file exists and can be executed. """
    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)


# From http://stackoverflow.com/a/600612/496605
# when migrating to Python 3, os.makedirs(.., exists_ok = True)
# should be used instead.
def mkdir_p(path):
    """Makes a directory and all non-existing directories needed to contain this directory 
    If the directory already exists, no error message is given and the function returns.
    
    keyword arguments:
    path -- full path of the directory to be made
    """
    try:
        os.makedirs(path)
    except OSError as exc: # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else: raise

#class Statconfig:



class Implementation:
    """An implementation is the source code of a solution to a general problem.
    The hierarchy is: problem -> implementation -> benchmark.
    """
    def __init__(self, hipermark_lib_dir, benchmark_lib_dir, name, directory):
        """The constructor of the class "Implementation".
        Sets the appropriate variables for an implementation.
        Also checks whether an instantiation is correctly configured with folders,
        executables etc.
        Does not compile the source code of the implementation.
        
        keyword arguments:
        hipermark_lib_dir -- /lib
        benchmark_lib_dir -- /benchmarks/<benchmark>/lib
        name -- Name of implementation's folder.
        directory -- /benchmarks/<benchmark>/implementations/<implementation>
        """
        if not os.path.isdir(directory): #returns true if path is an existing directory.
            if os.path.isfile(directory):
                raise Exception("File %s exists, but implementation should be contained in its own directory." % directory)
            else:
                raise Exception("Implementation directory %s does not exist " % directory)
        self.directory = directory
        self.name = name
        self.hipermark_lib_dir = hipermark_lib_dir
        self.benchmark_lib_dir = benchmark_lib_dir

        instantiate_file = os.path.join(self.directory, INSTANTIATE_FILENAME)
        instantiate_data_file = os.path.join(self.directory, INSTANTIATE_DATA_FILENAME)
        if not executable(instantiate_file):
            raise Exception("%s does not exist or it is not an executable file" % instantiate_file)
        if not executable(instantiate_data_file):
            raise Exception("%s does not exist or it is not an executable file" %
                            instantiate_data_file)
        self.instantiate_file = instantiate_file
        self.instantiate_data_file = instantiate_data_file
        _gather_statconfigs()

    def _fail_on_keywords(keyword):
        raise Exception("Unknown value keyword '%s' in template." %
                        keyword)

    def _all_possible_configurations(keyword_handler, template):
        """ Takes a dict (representing a JSON) where the values are lists.
        Finds the cartesian product of the lists and returns all points in
        the space of the cartesian product as a list of dicts where the values
        are scalars (non lists).

        keyword arguments:
        keyword_handler -- A function parsing keywords (vals that are strings, not lists)
        template -- A dict with lists as values
        """
        all_configs = [{}]
        for var in template:
            options = template[var]
            if type(options) is not list:
                options = keyword_handler(options)
            new_configs = []
            for config in all_configs:
                for option in options:
                    if type(option) in [dict, list]:
                        raise Exception("Invalid value for variable %s in configuration \
                        template: %s" % (var, options))
                    new_config = config.copy()
                    new_config[var] = str(option)
                    new_configs.append(new_config)
            all_configs = new_configs
        return all_configs

    def _gather_statconfigs():
        stat_config_file = os.path.join(self.directory, STAT_CONFIG_FILENAME)
        if not os.path.isfile(stat_config_file):
            self.statconfig_exists = False
            self.statconfiglist = ["default", {}]
            return
        self.statconfig_exists = True
        input_dict = read_json_file(stat_config_file)
        # what if this is not a JSON?
        statconfiglist = _all_possible_configurations(keyword_handler, input_dict)
        # generate list of hash names.
        # A map could be used here.
        # store this as a tuple.
        statconfig_names = []
        for x in self.statconfiglist:
            hash_val = hashlib.sha256(str(d)).hexdigest()[0:12]
            statconfig_names.append(hash_val)
        self.statconfiglist = zip(statconfig_names, statconfiglist)
        
    def instantiate_data(self, inst_dir, input_data, dataset_name):
        """Run the instantiate_data script which prepares the input data of the case.

        keyword arguments:
        inst_dir -- /instantiations/"<benchmark>/<implementation>" (target directory)
        input_data -- path of input file. /benchmarks/<benchmark>/datasets/<dataset>
        dataset_name -- name of dataset, specified by dir name of dataset in source files.
        """
        if not os.path.isdir(inst_dir):
            mkdir_p(inst_dir)
        
        # HIPERMARK_LIB_DIR: path of main library. /lib/
        # HIPERMARK_INPUT: path of input file. /benchmarks/<benchmark>/datasets/<dataset>/input.json
        # HIPERMARK_INPUT_NAME: Name of the folder in which the dataset was contained. This
        # prevents the overwriting of other data files, so instantiate_data must use this 
        # variable.
        instantiate_env = os.environ.copy()
        instantiate_env["HIPERMARK_LIB_DIR"] = self.hipermark_lib_dir
        instantiate_env["HIPERMARK_INPUT"] = input_data
        instantiate_env["HIPERMARK_INPUT_NAME"] = dataset_name

        proc = subprocess.Popen(self.instantiate_data_file,
                                env=instantiate_env,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                close_fds=True,
                                cwd=inst_dir)

        # Read data from stdout and stderr, until end-of-file is reached. Also waits for process to terminate.
        (stdoutdata, stderrdata) = proc.communicate(input=None)
        return (proc.returncode, stdoutdata, stderrdata)

 
    def instantiate(self, inst_dir, static_conf, platform_file):
        """Runs the instantiate_file which creates a run file to run the
        implementation and also runs the local Makefile.
        The instantiate file copies all the necessary source files in its own
        folder in /instantiations/.
        The compilation of the implementation source code takes place here.

        keyword arguments:
        inst_dir -- /instantiations/"<benchmark>/<implementation>" (target directory)
        platform_file -- /config/platform_example.json
        """
        if not os.path.isdir(inst_dir):
            mkdir_p(inst_dir)

        # Sets some environment variables. These are used in the subprocess.Popen function call, and they are handled in the "instantiate" files.
        # HIPERMARK_LIB_DIR: /lib/
        # HIPERMARK_BENCHMARK_LIB_DIR: /benchmarks/<benchmark>/lib
        # HIPERMARK_IMPLEMENTATION: /benchmarks/<benchmark>/implementations/<implementation>
        # HIPERMARK_PLATFORM: path of platform file. /config/platform_example.json
        # env. vars should be set in seperate function.
        instantiate_env = os.environ.copy()
        instantiate_env["HIPERMARK_LIB_DIR"] = self.hipermark_lib_dir
        instantiate_env["HIPERMARK_BENCHMARK_LIB_DIR"] = self.benchmark_lib_dir
        instantiate_env["HIPERMARK_IMPLEMENTATION"] = self.directory
        instantiate_env["HIPERMARK_PLATFORM"] = platform_file
        for sc in static_conf:
            # should this one include the "HIPERMARK_" prefix?? Unknown if str() is nec. below.
            instantiate_env["HIPERMARK_" + str(sc)] = static_conf[sc]

        # cwd = inst_dir sets the child process' folder to:
        # /instantiations/"<benchmark>/<implementation>/"
        proc = subprocess.Popen(self.instantiate_file,
                                env=instantiate_env,
                                stdout=subprocess.PIPE,
                                stderr=subprocess.PIPE,
                                stdin=subprocess.PIPE,
                                close_fds=True,
                                cwd=inst_dir)

        # Read data from stdout and stderr, until end-of-file is reached. Also waits for process to terminate.
        (stdoutdata, stderrdata) = proc.communicate(input=None)
        return (proc.returncode, stdoutdata, stderrdata)

class Benchmark:
    def __init__(self, hipermarkdir, name, directory):
        """This initialization stores all the implementations and all the datasets of
        this benchmark. It thus collects info about this benchmark's implementations.
        These source code of which should be located in folders in:
        /benchmarks/<benchmark>/implementations/
        It also does some checks on the content of the implementations and the datasets 
        such as checking that an "implementations" and a "datasets" folder exists in
        the correct folder.
        
        keyword arguments:
        hipermark_dir -- The directory from which this program was invoked.
        name -- name of benchmark as defined in the folder for its source code.
        directory -- directory of the benchmark. E.g. /benchmarks/OptionPricing
        """
        self.hipermarkdir = hipermarkdir
        self.name = name
        
        if not os.path.isdir(directory):
            raise Exception("Benchmark directory %s does not exist" % directory)
        self.directory = directory
        dircontents = os.listdir(self.directory) # list of content in benchmark root directory
        self.benchmark_lib_dir = os.path.join(self.directory, "lib")
        self.hipermark_lib_dir = os.path.join(self.hipermarkdir, "lib")

        if not ("implementations" in dircontents):
            raise Exception("No \"implementation\" directory in %s" % directory)
        self._gather_implementations() #creates list of implementations to be run.

        if not ("datasets" in dircontents):
            raise Exception("No datasets directory in %s" % directory)        
        self._gather_datasets()
        self._get_validation_function()

    def _get_validation_function(self):
        """ Checks if benchmark uses internal or an external validation function
        and sets the object variables describing this.
        """
        dircontents = os.listdir(self.directory)
        if "validation_function" in dircontents:
            self.external_validation = True
            validation_function_path = os.path.join(self.directory,
                                                    "validation_function")
            self.external_validation_file = validation_function_path
            if not executable(validation_function_path):
                raise Exception("Validation function exists at %s but it is not executable." %
                                validation_function_path)
        else:
            self.external_validation = False

    def _gather_implementations(self):
        """ Creates an implementation object for each non-disabled folder in 
        /benchmarks/<benchmark>/implementations/. The objects are placed in
        a list self.implementations.
        """
        self.implementations = [] #list of implementation objects.
        # This for-loop runs over all folders (and files) found in the "implementations" folder.
        implrootpath = os.path.join(self.directory, "implementations")
        for impl in os.listdir(implrootpath):
            implpath = os.path.join(self.directory, "implementations", impl)
            if os.path.isdir(implpath):
                disable_file = os.path.join(implpath, "disable")
                if os.path.isfile(disable_file):
                    continue
                self.implementations.append(Implementation(self.hipermark_lib_dir,
                                                           self.benchmark_lib_dir,
                                                           impl,
                                                           os.path.join(self.directory, "implementations", impl)))
            else:
                raise Exception("%s may only contain folders, not files." %implrootpath )
            
    def _gather_datasets(self):
        """Checks that each dataset in /benchmarks/<benchmark>/datasets/
        has the correct structure. Sets the self.datasets variable to an
        associative array of tuples containing the file name of the input
        file and a Python object representing the expected output.
        """
        self.datasets = {}
        dataset_dir = os.path.join(self.directory, "datasets")
        for dataset in os.listdir(dataset_dir):
            datasetpath = os.path.join(dataset_dir, dataset)
            if not os.path.isdir(datasetpath):
                raise Exception("Only directories containing datasets may be present in %s. %s is not a directory." %
                                (dataset_dir, datasetpath))
            inputfile = os.path.join(self.directory, "datasets", dataset, "input.json")
            outputfile = os.path.join(self.directory, "datasets", dataset, "output.json")
            # Checks that input and output files are present.
            if not os.path.isfile(inputfile):
                raise Exception("File %s missing or not a file." % inputfile)
            if not os.path.isfile(outputfile):
                raise Exception("File %s missing or not a file." % outputfile)
            self.datasets[dataset] = { 'input_file' : inputfile,
                                       'expected_output_file' : outputfile
            }

def desc_quadruple(benchmark, implementation, dataset, run):
    return "Benchmark %s, implementation %s, dataset %s, run %d (of this execution)" % (benchmark, implementation, dataset, run)

def desc_triple(benchmark, implementation, dataset):
    return "Benchmark %s, implementation %s, dataset %s" % (benchmark, implementation, dataset)

# This is a local variable that is only used in evaluation of this execution. For serious data analysis, the folder structure is used.
def note_result(results, benchmark, implementation, stat_name, stat_dict, dataset, run, runtime):
    """Stores the runtime of the execution of an implementation in a dictionary.
    This dictionary has triple's (benchmark, implementation, dataset) as key.
    The values are stored in a list which indexes over the run counter.

    keyword arguments:
    results -- name of dictionary in which runtime is saved (string)
    benchmark -- name of benchmark (string)
    implementation -- name of implementation (string)
    dataset -- name of dataset. (string)
    run -- run counter (int)
    runtime -- runtime as reported by the implementation. Stored in a list in results.
    """
    if not((benchmark, implementation, dataset) in results.keys()):
        results[(benchmark, implementation, dataset)] = []
    results[(benchmark,implementation,dataset)].append(runtime)

def get_nth_moment (result_times, n):
    """ 
    Calculates the nth moment of a list.

    Keyword values:
    result_times -- list of result times (floats)
    n -- integer describing that statistical moment of result_times which is returned
    """
    suml = 0
    N = len(result_times)
    for i in range(N):
        suml += result_times[i]**n
    return (suml/float(N))

def get_average_and_stddev (result_times):
    """ Returns average and stddev for a list.

    Keyword values:
    result_times -- list of result times (floats)
    """
    av = get_nth_moment(result_times, 1)
    return (av, math.sqrt(get_nth_moment(result_times, 2) - av**2))

def find_run_counter(target_dir):
    """Finds the run number for the first run of this execution.
    This is done to avoid overwriting output results.

    Keyword values:
    target_dir -- The dir where the count is performed.
    """
    dirlist = os.listdir(target_dir)
    max_number = 0
    empty = True
    try:
        a = []
        for path in dirlist:
            empty = False
            b = int(path)
            if b < 0:
                raise Exception("Only non-negative run numbers (i.e., directory names) are allowed in %s." %
                                target_dir)
            a.append(b)
    except ValueError:
        raise Exception("The folder %s may only contain folders with integer name." %
                        target_dir)
    if not empty:
        max_number = max(a) + 1
    return max_number        
    
def run_some_benchmark_implementations(hipermark_dir,
                                       workdir,
                                       benchmarkdir,
                                       number_of_runs,
                                       platform,
                                       run_implementation_fn,
                                       use_dataset_fn):
    """Declares all the objects associated with a specific benchmark:
    both benchmark and implementation objects declaration are called here
    (implementation decl. is called through benchmark decl.). 
    
    keyword arguments:
    hipermark_dir -- The directory from which this process was invoked.
    workdir -- Folder where instantiations (binaries and other) are stored.
    benchmarkdir -- benchmarks/<benchmark>. Path of benchmark to run.
    number_of_runs -- number of times that each case (dataset) will be run
    platform -- path of platform file.
    run_implementation_fn -- pointer to function run_implementation_fn
    use_dataset_fn --pointer to function use_dataset_fn
    """

    # Instantiates the benchmark objects which contain the implementation objects. 
    # Does not compile.
    b = Benchmark(hipermark_dir,
                  os.path.basename(os.path.normpath(benchmarkdir)),
                  benchmarkdir)
    data_insts = []  # all succesfull data compilations.
    failed_data_insts = [] # all failed data compilations
    insts = []  # all succesfull implementation compilations.
    failed_insts = [] # all failed implementation compilations
    results = {} # dict in Python. Like an associative array. key: (bench, impl, dataset)
    # value: list of runtimes. Manipulated through note_result function.
    # Things are overwritten here.
    # Running all "instantiate_data" files
    for impl in b.implementations:
        if not run_implementation_fn(impl):
            continue
        # the os.getcwd ensures that inst_dir is absolute path.
        # If user has specified absolute path, then the os.getcwd() is ignored.
        # for all statconfigs
         #what if this is empty??? Then this will not run even once!
         # a goto might be used?
         # sconf is a list of tuples(sconf_name, sconf_dict)
        for sconf in impl.statconfiglist:
            inst_dir = os.path.join(os.getcwd(), workdir, "%s/%s/%s" %
                                    (b.name, impl.name, sconf[0]))
            for dataset in b.datasets:
                if not use_dataset_fn(dataset):
                    continue
                print("Instantiating dataset \"%s\" in %s" % (dataset, inst_dir))
                # SHOULD impl.instantiate_data have access to the env. vars of impl.statconfiglist??
                (retcode, stdout, stderr) = impl.instantiate_data(
                    inst_dir, b.datasets[dataset]['input_file'], dataset)
            if retcode != 0:
                failed_data_insts.append({ 'benchmark' : b.name,
                                           'implementation' : impl.name,
                                           'stat_config_name': sconf[0],
                                           'stat_config_dict': sconf[1],
                                           'dataset' : dataset,
                                           'instantiation_directory' : inst_dir,
                                           'retcode' : retcode,
                                           'stdout' : stdout,
                                           'stderr' : stderr})
                print("Spoiler: failed")
            else:
                data_insts.append({ 'benchmark' : b.name,
                                    'implementation' : impl.name,
                                    'stat_config_name': sconf[0],
                                    'stat_config_dict': sconf[1],
                                    'dataset' : dataset,
                                    'instantiation_directory' : inst_dir,
                                    'expected_output_file' : b.datasets[dataset]['expected_output_file']})

    # def set_env_vars(dict):
    #     bla
    #     bla
    #     return 0

    # Running all "instantiate" files
    for impl in b.implementations:
        if not run_implementation_fn(impl):
            continue
        for sconf in impl.statconfiglist:
            inst_dir = os.path.join(os.getcwd(), workdir, "%s/%s/%s" %
                                    (b.name, impl.name, sconf[0]))
            print("Instantiating source code in %s" % inst_dir)
            # set env vars.
            (retcode, stdout, stderr) = impl.instantiate(inst_dir, sconf[1], platform)#compile source code
            if retcode != 0:
                failed_insts.append({ 'benchmark' : b.name,
                                      'implementation' : impl.name,
                                      'stat_config_name': sconf[0],
                                      'stat_config_dict': sconf[1],
                                      'instantiation_directory' : inst_dir,
                                      'retcode' : retcode,
                                      'stdout' : stdout,
                                      'stderr' : stderr})
                print("Ran instantiate. Spoiler: failed")
            else:
                insts.append({ 'benchmark' : b.name,
                               'implementation' : impl.name,
                               'stat_config_name': sconf[0],
                               'stat_config_dict': sconf[1],
                               'instantiation_directory' : inst_dir,
                               'expected_output_file' : b.datasets[dataset]['expected_output_file']})

    # Here, all data instantiations are looped over and the run file
    # is executed for each dataset*statconfig.
    for inst in data_insts:
        inst_dir = inst['instantiation_directory']
        path_for_outputs = os.path.join(inst_dir, "run_output/", inst['dataset'])
        mkdir_p(path_for_outputs)
        first_run_counter = find_run_counter(path_for_outputs)
        for run in range(first_run_counter, number_of_runs + first_run_counter):
            run_output = os.path.join(inst_dir, "run_output/", inst['dataset'], str(run))
            mkdir_p(run_output)
            print("\nRunning %s" % run_output)
            stdoutpath = os.path.join(run_output, STDOUT_FILENAME)
            stderrpath = os.path.join(run_output, STDERR_FILENAME)
            returncodepath = os.path.join(run_output, RETURNCODE_FILENAME)
            validationpath = os.path.join(run_output, VALIDATION_FILENAME)
            resultpath = os.path.join(run_output, RESULT_FILENAME)
            runtimepath = os.path.join(run_output, RUNTIME_FILENAME)
            timestampstartpath = os.path.join(run_output, TIMESTAMPSTART_FILENAME)
            timestampendpath = os.path.join(run_output, TIMESTAMPEND_FILENAME)
            with open(stdoutpath, "w") as stdout:
                with open(stderrpath, "w") as stderr:
                    with open(returncodepath, "w") as returncode_file:
                        with open(timestampstartpath, "w") as tsstart_file:
                            with open(timestampendpath, "w") as tsend_file:
                                print("Output can be found in %s and %s" % (stdoutpath, stderrpath))
                                try:
                                    instantiate_env = os.environ.copy()
                                    #HIPERMARK_RESULT/RUNTIME: path for implementation to store result/runtime
                                    instantiate_env["HIPERMARK_INPUT_NAME"] = inst['dataset']
                                    instantiate_env["HIPERMARK_RESULT"] = resultpath
                                    instantiate_env["HIPERMARK_RUNTIME"] = runtimepath
                                    timestamp = time.time()
                                    tsstart_file.write(str(timestamp) + "\n")
                                    proc = subprocess.Popen(os.path.join(inst_dir, "run"),
                                                            env=instantiate_env,
                                                            stdout=stdout,
                                                            stderr=stderr,
                                                            stdin=subprocess.PIPE,
                                                            close_fds=True,
                                                            cwd=inst_dir) #run a case
                                    proc.stdin.close() # Make sure stdin EOFs.
                                    proc.wait()
                                    timestamp = time.time()
                                    tsend_file.write(str(timestamp) + "\n")
                                    returncode_file.write(str(proc.returncode) + "\n")
                                except:
                                    note_result(results,
                                                inst['benchmark'],
                                                inst['implementation'],
                                                inst['stat_config_name'],
                                                inst['stat_config_dict'],
                                                inst['dataset'],
                                                run,
                                                "could not run")
                                    # a call to note_result function adds an entry in the result dict.
                                    print("Spoiler: failed")
                                    continue
                                if proc.returncode != 0:
                                    note_result(results,
                                                inst['benchmark'],
                                                inst['implementation'],
                                                inst['stat_config_name'],
                                                inst['stat_config_dict'],
                                                inst['dataset'],
                                                run,
                                                "non-zero exit code")
                                    print("Spoiler: failed")
                                    continue
                                
                                try:
                                    runtime = read_runtime(run_output)
                                except IOError:
                                    note_result(results,
                                            inst['benchmark'],
                                                inst['implementation'],
                                                inst['stat_config_name'],
                                                inst['stat_config_dict'],
                                                inst['dataset'],
                                                run,
                                                "no runtime measurement")
                                    continue
                                except ValueError:
                                    note_result(results,
                                                inst['benchmark'],
                                                inst['implementation'],
                                                inst['stat_config_name'],
                                                inst['stat_config_dict'],
                                                inst['dataset'],
                                                run,
                                                "non-integer runtime measurement")
                                    continue
                                
                                try:
                                    resultfile = os.path.join(run_output, RESULT_FILENAME)
                                    expectedfile = inst['expected_output_file']
                                    if b.external_validation:
                                        evstdoutpath = os.path.join(run_output, EXT_VALIDATION_STDOUT_FILENAME)
                                        evstderrpath = os.path.join(run_output, EXT_VALIDATION_STDERR_FILENAME)
                                        with open(evstdoutpath, 'w') as evstdout:
                                            with open(evstdoutpath, 'w') as evstderr:
                                                print("Launching external validation")
                                                proc = subprocess.Popen([b.external_validation_file, resultfile, expectedfile],
                                                                        stdout=evstdout,
                                                                        stderr=evstderr,
                                                                        stdin=subprocess.PIPE,
                                                                        close_fds=True,
                                                                        cwd=inst_dir)
                                                #return code 0 of above means succesfull val.
                                                proc.stdin.close() # Make sure stdin EOFs.
                                                proc.wait()
                                                match = 1 ^ proc.returncode
                                    else:
                                        result = read_json_file(resultfile)
                                        expected = read_json_file(expectedfile)
                                        match = compare_json(result, expected)
                                    f = open(validationpath, "w")
                                    f.write(str(int(match)) + "\n")
                                    f.close()
                                    if match:
                                        note_result(results,
                                                    inst['benchmark'],
                                                    inst['implementation'],
                                                    inst['stat_config_name'],
                                                    inst['stat_config_dict'],
                                                    inst['dataset'],
                                                    run,
                                                    runtime)
                                    else:
                                        note_result(results,
                                                    inst['benchmark'],
                                                    inst['implementation'],
                                                    inst['stat_config_name'],
                                                    inst['stat_config_dict'],
                                                    inst['dataset'],
                                                    run,
                                                    "result did not match expected result")
                                except (IOError, ValueError):
                                    note_result(results,
                                                inst['benchmark'],
                                                inst['implementation'],
                                                inst['stat_config_name'],
                                                inst['stat_config_dict'],
                                                inst['dataset'],
                                                run,
                                                "cannot read result")
                                    f = open(validationpath, "w")
                                    f.write("cannot read result\n")
                                    f.close()
    return (results, failed_insts, failed_data_insts) 
                        
                        
def read_runtime(output_dir):
    """Reads the runtime.txt holding the run time, and created by the executable.

    keyword arguments:
    output_dir -- absolute path of dir in which runtime results are stored
    """
    with open(os.path.join(output_dir, "runtime.txt"), "r") as file:
        return int(file.read())


def read_json_file(filename):
    """ returns an array containing the content of the JSON file.

    keyword arguments:
    filename -- name of json file to interpret
    """
    with open(filename, "r") as file:
        return json.loads(file.read())


# Should a relative value be used instead of epsilon?
epsilon = 0.001


def compare_json(json1, json2):
    """Generic comparison function using epsilon for comparing floats.
    The order of the arguments is arbitrary.
    
    keyword aguments:
    json1 -- calculated result (arbitrary order)
    json2 -- expected result (arbitrary order)
    """
    if type(json1) != type(json2):
        return False

    if type(json1) is float:
        return abs(json1-json2) < epsilon

    if type(json1) is list:
        if len(json1) != len(json2):
            return False
        for x,y in zip(json1, json2):
            if not compare_json(x,y):
                return False
        return True

    if type(json1) is dict:
        keys1 = json1.keys()
        keys2 = json2.keys()
        keys1.sort()
        keys2.sort()
        if keys1 == keys2:
            for key in keys1:
                if not compare_json(json1[key], json2[key]):
                    return False
            return True
        else:
            return False

    return json1 == json2

def find_target_dir_and_what_to_run():
    """ Returns the implementations and dataset to run. It should be checked that the implementations, datasets, etc. exists and an error should be returned if they do not.

    Arguments can be seperated by commas: "arg1,arg2" etc.

    Return values:
    ret1 -- folder where instantiations should be placed
    ret2 -- number of runs per case
    ret3 -- folder of specific benchmark. E.g., "benchmarks/CalibVolDiff".
    ret4 -- name of implementation. E.g., "cpp_sequential".
    ret5 -- name of dataset. E.g., "small".
    """
    if len(sys.argv) == 1:
        return ("instantiations", 1, "benchmarks/CalibVolDiff", None, None)
    elif len(sys.argv) == 2:
        return (sys.argv[1], 1, "benchmarks/CalibVolDiff", None, None)
    elif len(sys.argv) == 3:
        return (sys.argv[1], int(sys.argv[2]), "benchmarks/CalibVolDiff", None, None)
    elif len(sys.argv) == 4:
        return (sys.argv[1], int(sys.argv[2]), sys.argv[3], None, None)
    elif len(sys.argv) == 5:
        return (sys.argv[1], int(sys.argv[2]), sys.argv[3], sys.argv[4].split(','), None)
    elif len(sys.argv) == 6:
        return (sys.argv[1], int(sys.argv[2]), sys.argv[3], sys.argv[4].split(','), sys.argv[5].split(','))
    else:
        exit("Usage: %s [target_directory] [number_of_runs_per_case] [benchmark] [implementation1,implementation2,...] [dataset1,dataset2,...]" %
             sys.argv[0])

if __name__ == '__main__':
    """This main function first creates lists of benchmarks, implementations, and datasets
    which it will run. It then runs these benchmarks and prints the results from stdout 
    and from stderr.
    """

    (instantiations_dir, number_of_runs, benchmark, implementations, datasets) = find_target_dir_and_what_to_run()
    platform_file = "config/platform_example.json"

    print("Assuming I am being run from main hipermark directory.")
    
    # somewhere it should be checked that these benchmarks and these implementations exist.
    # Generally, the handling of benchmark/implementation sanity checks should be looked at.
    print("Will run benchmark %s." % benchmark)

    if implementations is None:
        print("Will run all implementations.")
    else:
        print("Will run these implementations: %s" % implementations)

    if datasets is None:
        print("Will use all datasets.")
    else:
        print("Will use these datasets: %s" % datasets)

    print("Each case will be run %d times" % number_of_runs)

    print("Will instantiate in %s." % instantiations_dir)
    print("Will use platform configuration %s." % platform_file)
    hipermark_dir = os.getcwd() # current folder. Is used to set workdir

    #Pointers to these functions are passed as arguments below.
    def run_implementation_fn(impl):
        if implementations is None:
            return True
        else:
            return impl.name in implementations

    def use_dataset_fn(dataset):
        if datasets is None:
            return True
        else:
            return dataset in datasets

    (results, failed_to_instantiate, failed_to_instantiate_data) = (
        run_some_benchmark_implementations(hipermark_dir,
                                           instantiations_dir,
                                           os.path.join(hipermark_dir, benchmark),
                                           number_of_runs,
                                           os.path.join(hipermark_dir, platform_file),
                                           run_implementation_fn,
                                           use_dataset_fn)
    )

    for failed in failed_to_instantiate_data:
        print("Failed to instantiate this dataset: benchmark %s, implementation %s, static config dict %s, static config name %s, dataset %s. Exit code was %d." %
              (failed['benchmark'], failed['implementation'], failed['stat_config_dict'], failed['stat_config_name'], failed['dataset'], failed['retcode']))
        print("Stdout\n")
        print(failed['stdout'])
        print("Stderr\n")
        print(failed['stderr'])
        print("")

    for failed in failed_to_instantiate:
        print("Failed to instantiate benchmark %s, implementation %s, static config dict %s, static config name %s. Exit code was %d." %
              (failed['benchmark'], failed['implementation'], failed['stat_config_dict'], failed['stat_config_name'], failed['retcode']))
        print("Stdout\n")
        print(failed['stdout'])
        print("Stderr\n")
        print(failed['stderr'])
        print("")

    # CODE BELOW THIS MIGHT NEED TO BE MODIFIED TO HANDLE STATIC CONF!
    # These should due to consistency be treated as immutables!
    successful_quadruple = {}
    successful_triple = {}
    failed_quadruple = {}

    for result_triple in results:
        successful_counter = 0
        for run in range(len(results[result_triple])):
            run_desc = results[result_triple][run] # int for success, text string for fail
            run_tuple = (run,)
            result_quadruple = result_triple + run_tuple
            if type(run_desc) is int:
                successful_quadruple[result_quadruple] = run_desc
                if successful_counter == 0:
                    successful_triple[result_triple] = []
                successful_counter += 1
                successful_triple[result_triple].append(run_desc)
            else:
                failed_quadruple[result_quadruple] = run_desc

    # DO NOT MODIFY successful_triple or successful_quadruple after this line!

    print("\nFailed:")
    for f in failed_quadruple:
        (benchmark, implementation, dataset, run) = f
        print("%s: %s" %
              (desc_quadruple(benchmark, implementation, dataset, run),
               failed_quadruple[f]))

    print("\nSucceeded:")
    for s in successful_quadruple:
        (benchmark, implementation, dataset, run) = s
        print("%s: %s" %
              (desc_quadruple(benchmark, implementation, dataset, run),
               successful_quadruple[s]))

    print("\nAverage Run Times:")
    for s in successful_triple:
        (benchmark, implementation, dataset) = s
        av, stddev = get_average_and_stddev(successful_triple[s])
        rel_stddev = stddev/float(av)
        print("%s: Average time: %d, standard deviation: %.1f, relative standard deviation: %.1E" % (desc_triple(benchmark, implementation, dataset), av, stddev, rel_stddev))
