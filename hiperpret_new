#!/usr/bin/env python

import os
import sys
import math
import json
import matplotlib.pyplot as plt
import numpy as np


# These files are read, not written.
RESULT_FILENAME = "result.json"
RUNDATA_FILENAME = "rundata.json"
RUNTIME_FILENAME = "runtime.txt"
STDERR_FILENAME = "stderr.log"
STDOUT_FILENAME = "stdout.log"
STATIC_CONFIGURATION_FILENAME = "static_configuration.json"
DYNAMIC_CONFIGURATION_FILENAME = "dynamic_configuration.json"
RUN_OUTPUT_PATH = "run_output"

# dict entries in rundata.json
TIMESTAMP_START = "timestamp_start"
TIMESTAMP_END = "timestamp_end"
RETURN_CODE = "return_code"
VALIDATION = "validation"
RUNTIME = "runtime"

def read_json_file(filename):
    """ returns an array containing the content of the JSON file.

    keyword arguments:
    filename -- name of json file to interpret
    """
    with open(filename, "r") as file:
        return json.loads(str(file.read()))


def get_nth_moment (result_times, n):
    """ 
    Calculates the nth moment of a list.

    Keyword values:
    result_times -- list of result times (floats)
    n -- integer describing that statistical moment of result_times which is returned
    """
    powers = map(lambda x: x**n, result_times)
    return sum(powers)/float(len(result_times))

def get_average_and_stddev (result_times):
    """ Returns average and stddev for a list.

    Keyword values:
    result_times -- list of result times (floats)
    """
    av = get_nth_moment(result_times, 1)
    return (av, math.sqrt(get_nth_moment(result_times, 2) - av**2))


class Run:
    def __init__(self, benchmark,
                 implementation,
                 static_configuration,
                 case,
                 dynamic_configuration,
                 runnum,
                 directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.static_configuration = static_configuration
        self.case = case
        self.dynamic_configuration = dynamic_configuration # an pointer to an object
        self.name = runnum
        self.directory = directory
        rundir_content = [RESULT_FILENAME, RUNTIME_FILENAME,\
                          STDERR_FILENAME, STDOUT_FILENAME, RUNDATA_FILENAME]
        dircontents = os.listdir(self.directory)
        N = len(rundir_content)
        for i in range(N):
            if not rundir_content[i] in dircontents:
                raise Exception("The folder %s does not contain the file %s." %
                                (self.directory, rundir_content[i]))
        with open(os.path.join(self.directory,
                               RUNTIME_FILENAME), "r") as runtime_fp:
            with open(os.path.join(self.directory,
                                   STDERR_FILENAME), "r") as stderr_fp:
                with open(os.path.join(self.directory,
                                       STDOUT_FILENAME), "r") as stdout_fp:
                    try:
                        # Also catch the KeyError here!!
                        rd_dict_path = os.path.join(self.directory,
                                                    RUNDATA_FILENAME)
                        rd_dict = read_json_file(rd_dict_path)
                        self.retcode = int(rd_dict[RETURN_CODE])
                        self.ts_start = float(rd_dict[TIMESTAMP_START])
                        self.ts_end = float(rd_dict[TIMESTAMP_END])
                        self.validation = int(rd_dict[VALIDATION])
                        self.runtime = int(runtime_fp.read())
                        self.stderr = str(stderr_fp.read())
                        self.stdout = str(stdout_fp.read())
                    except ValueError as e:
                        # How are the arguments of the exception presented?
                        raise Exception("The content of one of\
                        the result files in %s was malformed:\
                        %s." % (self.directory, e.args()))

    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}, \
        dataset: {3}, \
        dynamic configuration: {4}\
        run number: {5}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.static_configuration.configuration),
                self.case.name,
                str(self.dynamic_configuration.configuration),
                str(self.name))

class Dynamic_configuration:
    def __init__(self, benchmark, implementation,
                 static_configuration, case, name, directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.static_configuration = static_configuration
        self.case = case
        self.name = name
        self.directory = directory
        self.runs = {}
        dconf_filename = os.path.join(self.directory,
                                      DYNAMIC_CONFIGURATION_FILENAME)
        if not os.path.isfile(dconf_filename):
            raise Exception("%s does not contain a file describing its \
            runtime variables. If no runtime vars, a file containing \
            \"{}\" should still be here." % self.directory)
        try:
            self.configuration = read_json_file(dconf_filename)
        except ValueError as e:
            print("Error reading JSON")
            print(e.args)
            raise Exception("Error reading JSON file %s", dconf_filename)
        dircontents = os.listdir(self.directory)
        for run in dircontents:
            if run == DYNAMIC_CONFIGURATION_FILENAME:
                continue
            runpath = os.path.join(self.directory, run)
            if not os.path.isdir(runpath):
                raise Exception("%s may only contain %s and directories \
                containing runs. %s is neither." %
                                (self.directory, DYNAMIC_CONFIGURATION_FILENAME,
                                 runpath))
            try:
                runnum = int(run)
                if runnum < 0:
                    raise Exception("Only non-negative run numbers (i.e., directory names)\
                are allowed in %s." % target_dir)
                self.runs[runnum] = Run(self.benchmark, self.implementation,
                                        self.static_configuration, self.case,
                                        self, runnum, runpath)
            except ValueError:
                raise Exception("The folder %s may only contain folders with\
                integer name." % self.directory)
        self.number_of_runs = len(self.runs.keys())


    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}, \
        dataset: {3}, \
        dynamic configuration: {4}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.static_configuration.configuration),
                self.case.name,
                str(self.configuration))

class Case:
    def __init__(self, benchmark, implementation,
                 static_configuration, name, directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.static_configuration = static_configuration
        self.name = name # name of a dataset.
        self.directory = directory
        self.dynamic_configurations = {}
        dircontents = os.listdir(self.directory)
        for dconf in dircontents:
            dconf_path = os.path.join(self.directory, dconf)
            if not os.path.isdir(dconf_path):
                raise Exception("%s may only contain folders containing dynamic\
                variables configurations. %s is not a folder" %
                                (self.directory,
                                 dconf_path))
            self.dynamic_configurations[dconf] \
                = Dynamic_configuration(self.benchmark,
                                        self.implementation,
                                        self.static_configuration,
                                        self,
                                        dconf,
                                        dconf_path)
        if not self.dynamic_configurations:
            raise Exception("The case:\n %s does not contain any folders for \
            dynamic configurations." % str(self))

    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}, \
        dataset: {3}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.static_configuration.configuration),
                self.name)
    
class Static_configuration:
    def __init__(self, benchmark, implementation, name, directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.name = name
        self.directory = directory
        self.cases = {}
        sconf_filename = os.path.join(self.directory,
                                      STATIC_CONFIGURATION_FILENAME)
        run_output_path = os.path.join(self.directory,
                                       RUN_OUTPUT_PATH)
        self.run_output_path = run_output_path
        if not os.path.isfile(sconf_filename):
            raise Exception("%s does not contain a file describing its \
            compile-time variables. If no compile-time vars, a file containing \
            \"{}\" should still be here." % self.directory)
        if not os.path.isdir(run_output_path):
            raise Exception("%s does not contain a folder called %s as it \
            should." % (self.directory, RUN_OUTPUT_PATH))
        # what if this is a malformed JSON??
        try:
            self.configuration = read_json_file(sconf_filename)
        except ValueError as e:
            print("\n" + str(e.args) + "\n")
            raise Exception("Error reading JSON file: %s" % sconf_filename)
        datasets = os.listdir(run_output_path)
        empty = True
        for dataset in datasets:
            empty = False
            dataset_path = os.path.join(run_output_path, dataset)
            if not os.path.isdir(dataset_path):
                raise Exception("The folder %s may only contain folders with \
                names of datasets." % run_output_path)
            self.cases[dataset] = Case(self.benchmark,
                                       self.implementation,
                                       self,
                                       dataset,
                                       dataset_path)
        if empty:
            raise Exception("%s does not contain folder(s) for datasets as it\
            should" % run_output_path)

    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.configuration))
    
class Implementation:
    def __init__(self, benchmark, name, directory):
        self.benchmark = benchmark
        self.name = name
        self.directory = directory
        self.static_configs = {}
        empty = True
        # contains only folders with statconf names.
        dircontents = os.listdir(self.directory)
        for sconf in dircontents:
            empty = False
            sconf_dir = os.path.join(self.directory, sconf)
            if not os.path.isdir(sconf_dir):
                raise Exception("The content of %s must be folders with names \
                for static configurations (hash values).\
                But %s is not a folder." % (run_outputdir, sconf_dir))
            # Here, the keys of the cases will be hash values.
            self.static_configs[sconf] = \
            Static_configuration(self.benchmark, self,
                                 sconf, sconf_dir)
        if empty:
                raise Exception("No folders for static configurations was found\
                in %s." % self.directory)
            
    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}"\
        .format(self.benchmark.name,
                self.name)
    
class Benchmark:
    def __init__(self, name, directory):
        self.name = name
        self.directory = directory
        self.implementations = {}
        dircontents = os.listdir(self.directory)
        for impl in dircontents:
            impldir = os.path.join(self.directory, impl)
            if not os.path.isdir(impldir):
                raise Exception("The folder containing the %s benchmark must\
                only contain directories. %s is not a directory." %
                                (self.name, impldir))
            implname = os.path.basename(impldir)
            self.implementations[implname] = Implementation(self,
                                                            implname,
                                                            impldir)
            
    def __str__(self):
        """ This function allows for printing of benchmark objects.
        """
        a = "benchmark: {0}".format(self.name)
        return a

def get_run_values(run_dicts):
    """For a given list of run_dicts (probably calculated from\
    get_run_dicts), this function returns a dictionary where the keys are \
    var_names and the values are a list that the respective run_dicts take.
    """
    first_dict = run_dicts[0]
    for var in first_dict: # all variables
        val = []
        for d in run_dicts: # all dicts
            if d[var] not in val:
                val.append(d[var])
        var_dict[var] = val.sort()
    return var_dict

#########################################################
# Below this comment are the visualization functions    #
# which parse the data collected in the objects whose   #
# classes are defined above                             #
#########################################################

## VISUALIZATION HELPER FUNCTIONS
def get_benchmark_object(benchmark_name, instdir):
    if not os.path.isdir(instdir):
        raise Exception("Directory %s could not be found." % sys.argv[1])
    dircontents = os.listdir(instdir)
    b = {}
    rootdir = os.getcwd()
    non_dirs = filter(lambda path:
                      not os.path.isdir(os.path.join(rootdir, instdir, path)),
                      dircontents)
    if non_dirs:
        raise Exception("Only directories may be be placed in %s." % sys.argv[1])
    bmdirs = map(lambda bm: os.path.join(rootdir, instdir, bm), dircontents)
    b_vals = map(lambda name, bmdir: Benchmark(name, bmdir), dircontents, bmdirs)
    b = dict(zip(dircontents, b_vals))
    try:
        benchmark = b[benchmark_name]
    except KeyError:
        raise Exception("%s was not found among the benchmark objects." %
                        benchmark_name)
    return benchmark

def get_implementation_object(benchmark, implementation_name):
    try:
        implementation = benchmark.implementations[implementation_name]
    except KeyError:
        raise Exception("%s was not found among the implementations of benchmark\
        %s" %(implementation_name))
    return implementation

def get_statistics(dconfs):
    """ Returns a list of tuples where tuple[0] = av, tuple[1] = std_dev,
        tuple[2] = uncertainty on average."""
    def _get_statistic(dconf):
        """Given one dynamic configuration, this function returns the average,
            standard deviation, and the uncertainty of the av."""
        N = dconf.number_of_runs
        runtimes = map(lambda run: run.runtime, dconf.runs.itervalues())
        av, std_dev = get_average_and_stddev(runtimes)
        uncertainty = std_dev/math.sqrt(N)
        return av, std_dev, uncertainty
    
    return map(_get_statistic, dconfs)



def get_all_static_configs(implementation):
    sconfs = implementation.static_configs.values()
    return sconfs

def flatten_list(listlist):
    flatlist = reduce(lambda l1, l2: l1 + l2, listlist) #list list -> list
    return flatlist

def get_all_case_objects(sconfs):
    cases = map(lambda sconf: sconf.cases.values(), sconfs)
    cases = flatten_list(cases)
    return cases

def get_all_dynamic_configs(cases):
    dconfs = map(lambda case: case.dynamic_configurations.values(), cases)
    dconfs = flatten_list(dconfs)
    return dconfs

def get_all_runs(dconfs):
    runs = map(lambda dconf: dconf.runs.values(), dconfs)
    runs = flatten_list(runs)
    return runs


### DICTIONARY CREATING FUNCTIONS
def get_run_dicts(benchmark_name, implementation_name, instdir):
    def _generate_json(run):
        run_data = {}
        run_data['benchmark_name'] = run.benchmark.name
        run_data['implementation_name'] = run.implementation.name
        run_data.update(run.static_configuration.configuration)
        run_data['dataset_name'] = run.case.name
        run_data.update(run.dynamic_configuration.configuration)        
        run_data['runnum'] = run.name
        run_data['retcode'] = run.retcode
        run_data['validation'] = run.validation
        run_data['runtime'] = run.runtime
        run_data['ts_start'] = run.ts_start
        run_data['ts_end'] = run.ts_end
        return run_data

    benchmark = get_benchmark_object(benchmark_name, instdir)
    implementation = get_implementation_object(benchmark, implementation_name)
    sconfs = get_all_static_configs(implementation)
    cases = get_all_case_objects(sconfs)
    dconfs = get_all_dynamic_configs(cases)
    runs = get_all_runs(dconfs)
    dicts = map(_generate_json, runs)
    return dicts

def get_dconf_dicts(benchmark_name, implementation_name, instdir):
    """Given an implementation, this function exports all data generated by
    hipermark to a JSON file specified by the user. The exported data is a list
    of all hipermark generated data from each configuration and thus also
    contains average, standard deviation and uncertainty if more than one run
    for a given configuration has been made
    """
    def belongs_to_dconf(configuration, run_dict):
        for key in configuration:
            if not run_dict[key] == configuration[key]:
                return False
        return True
    
    def get_runs_of_dconf(configuration, run_dicts):
        dcruns = []
        for run_dict in run_dicts:
            if belongs_to_dconf(configuration, run_dict):
                dcruns.append(run_dict)
        return dcruns

    def _generate_json_dconf(dconf):
        dconf_data = {}
        dconf_data['benchmark_name'] = dconf.benchmark.name
        dconf_data['implementation_name'] = dconf.implementation.name
        dconf_data.update(dconf.static_configuration.configuration)
        dconf_data['dataset_name'] = dconf.case.name
        dconf_data.update(dconf.configuration)
        return dconf_data

    def set_statistics_for_dconf(configuration, run_dicts):
        run_dicts_of_dconf = get_runs_of_dconf(configuration, run_dicts)
        run_times = []
        for run_dict in run_dicts_of_dconf:
            if (run_dict['validation'] != 1) or (run_dict['retcode'] != 0):
                raise Exception("Non-validated run encountered in the attempt \
                to export a JSON each configuration. Stopped. Error found in:",\
                                str(run_dict))
            run_times.append(run_dict['runtime'])
        N = len(run_times)
        av, stddev = get_average_and_stddev(run_times)
        configuration['average_runtime'] = av
        configuration['std_dev'] = stddev
        configuration['number_of_runs'] = N
        configuration['uncertainty_of_average'] = stddev/math.sqrt(N)
        return

    benchmark = get_benchmark_object(benchmark_name, instdir)
    implementation = get_implementation_object(benchmark, implementation_name)
    sconfs = get_all_static_configs(implementation)
    cases = get_all_case_objects(sconfs)
    dconfs = get_all_dynamic_configs(cases)
    run_dicts = get_run_dicts(benchmark_name, implementation_name, instdir)
    configurations = map(_generate_json_dconf, dconfs)
    for config in configurations:
        set_statistics_for_dconf(config, run_dicts)
    return configurations


### EXPORT FUNCTIONS
def export_runs(instdir, benchmark_name, implementation_name, filename):
    """Given an implementation, this function exports all data generated by
    hipermark to a JSON file specified by the user. The exported data is a list
    of all hipermark generated data from each run.
    """   
    run_dicts = get_run_dicts(benchmark_name, implementation_name, instdir)
    with open(filename, "w") as fp:
        ret = fp.write(json.dumps(run_dicts))
    return ret

def export_dconfs(instdir, benchmark_name, implementation_name, filename):
    dconf_dicts = get_dconf_dicts(benchmark_name, implementation_name, instdir)
    with open(filename, "w") as fp:
        ret = fp.write(json.dumps(dconf_dicts))
    return ret


### FUNCTION THAT GENERATES POINTS IN 2-D
def get_2d_points_dconf_dicts(dconf_dicts,
                              free_var,
                              locked_vars,
                              dep_var = 'average_runtime',
                              dep_unc = 'uncertainty_of_average'):
    """Given dicts that describe configurations, this function returns three
    lists. One representing x point, one y points, and one uncertainties on the
    y-axis.

    Keyword arguments:
    dconf_dicts -- the list of dictionaries, one element for each configuration.
    free_var -- The name of the free variable 
    locked_vars -- a dictionary representing all the locked variables.
    """
    num_of_vars = len(dconf_dicts[0].keys())
    num_of_locked_vars = len(locked_vars.keys())
    if num_of_vars - num_of_locked_vars != 5:
        raise Exception("The wrong nmber of locked variables has been provided.\
        The configuration dicts contain %d variables." % num_of_vars)
    # filter the dconfs that agree with the locked variables.
    try:
        ret_dconfs = filter(lambda dconf_dict:
                            all(map(lambda key: dconf_dict[key] == locked_vars[key],
                                     locked_vars.keys())),
                            dconf_dicts)
    except KeyError as e:
        raise Exception("Variable defined in locked_vars not found in configuration dicts: %s" %
                        (e.args))
    # Ensure that keys and vals are all strings
    for ret_dconf in ret_dconfs:
        for key in ret_dconf.keys():
            ret_dconf[str(key)] = ret_dconf.pop(key)
            ret_dconf[str(key)] = str(ret_dconf[str(key)])

    try:
        x_values = map(lambda ret_dconf: ret_dconf[free_var], ret_dconfs)
        
    except KeyError:
        raise Exception("free_var argument given to %s function not found in dconf_dicts" % sys._getframe().f_code.co_name)
    try:
        y_values = map(lambda ret_dconf: ret_dconf[dep_var], ret_dconfs)
    except KeyError:
        raise Exception("dep_var argument given to %s function not found in dconf_dicts" % sys._getframe().f_code.co_name)
    try:
        y_unc = map(lambda ret_dconf: ret_dconf[dep_unc], ret_dconfs)
    except KeyError:
        raise Exception("dep_unc argument given to %s function not found in dconf_dicts" % sys._getframe().f_code.co_name)
    return x_values, y_values, y_unc
    

if __name__ == "__main__":
    if len(sys.argv) != 5:
        print("Usage: %s <instantiation dir path> <benchmark name>\
        <implementation name> <filename>" % sys.argv[0])
        exit(1)
    instdir = sys.argv[1]
    benchmark_name = sys.argv[2]
    implementation_name = sys.argv[3]
    filename_runs = sys.argv[4] + "_runs.json"
    filename_dconfs = sys.argv[4] + "_configurations.json"
    export_runs(instdir, benchmark_name, implementation_name, filename_runs)
    export_dconfs(instdir, benchmark_name, implementation_name, filename_dconfs)
    dconf_dicts = get_dconf_dicts(benchmark_name, implementation_name, instdir)
    free_var = "NUM_THREADS"
    locked_vars = {"dataset_name": "medium",
                   "REAL_TYPE": "double",
                   "benchmark_name": "OptionPricing",
                   "implementation_name": "cpp_openmp"}
    retx, rety, rety_unc = get_2d_points_dconf_dicts(dconf_dicts, free_var, locked_vars)
    print(retx)
    print(rety)
    print(rety_unc)
