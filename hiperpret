#!/usr/bin/env python

import os
import sys
import math
import json
import matplotlib.pyplot as plt
import numpy as np
import pylab #
import operator #used for sorting
import errno


# These files are read, not written.
RESULT_FILENAME = "result.json"
RUNDATA_FILENAME = "rundata.json"
RUNTIME_FILENAME = "runtime.txt"
STDERR_FILENAME = "stderr.log"
STDOUT_FILENAME = "stdout.log"
STATIC_CONFIGURATION_FILENAME = "static_configuration.json"
DYNAMIC_CONFIGURATION_FILENAME = "dynamic_configuration.json"
RUN_OUTPUT_PATH = "run_output"
FIGURE_CONFIGURATION_FILENAME = "hiperpret_figure_configuration.json"

#written files
EXPORT_DCONFS_FILENAME_PREPEND = "export_dconfs_"
EXPORT_RUNS_FILENAME_PREPEND = "export_runs_"
EXPORT_FOLDER_NAME = "exported_jsons"
FIGURE_FOLDER_NAME = "produced_figures"
HIPERPRET_FOLDER = "hiperpret"

# dict entries in rundata.json
TIMESTAMP_START = "timestamp_start"
TIMESTAMP_END = "timestamp_end"
RETURN_CODE = "return_code"
VALIDATION = "validation"
RUNTIME = "runtime"

def mkdir_p(path):
    """Makes a directory and all non-existing directories needed to contain this directory 
    If the directory already exists, no error message is given and the function returns.
    
    keyword arguments:
    path -- full path of the directory to be made
    """
    try:
        os.makedirs(path)
    except OSError as exc: # Python >2.5
        if exc.errno == errno.EEXIST and os.path.isdir(path):
            pass
        else: raise

def read_json_file(filename):
    """ returns an array containing the content of the JSON file.

    keyword arguments:
    filename -- name of json file to interpret
    """
    with open(filename, "r") as file:
        return json.loads(str(file.read()))


def get_nth_moment (result_times, n):
    """ 
    Calculates the nth moment of a list.

    Keyword values:
    result_times -- list of result times (floats)
    n -- integer describing that statistical moment of result_times which is returned
    """
    powers = map(lambda x: x**n, result_times)
    return sum(powers)/float(len(result_times))

def get_average_and_stddev (result_times):
    """ Returns average and stddev for a list.

    Keyword values:
    result_times -- list of result times (floats)
    """
    av = get_nth_moment(result_times, 1)
    return (av, math.sqrt(get_nth_moment(result_times, 2) - av**2))


class Run:
    def __init__(self, benchmark,
                 implementation,
                 static_configuration,
                 case,
                 dynamic_configuration,
                 runnum,
                 directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.static_configuration = static_configuration
        self.case = case
        self.dynamic_configuration = dynamic_configuration # an pointer to an object
        self.name = runnum
        self.directory = directory
        rundir_content = [RESULT_FILENAME, RUNTIME_FILENAME,\
                          STDERR_FILENAME, STDOUT_FILENAME, RUNDATA_FILENAME]
        dircontents = os.listdir(self.directory)
        N = len(rundir_content)
        for i in range(N):
            if not rundir_content[i] in dircontents:
                raise Exception("The folder %s does not contain the file %s." %
                                (self.directory, rundir_content[i]))
        with open(os.path.join(self.directory,
                               RUNTIME_FILENAME), "r") as runtime_fp:
            with open(os.path.join(self.directory,
                                   STDERR_FILENAME), "r") as stderr_fp:
                with open(os.path.join(self.directory,
                                       STDOUT_FILENAME), "r") as stdout_fp:
                    try:
                        rd_dict_path = os.path.join(self.directory,
                                                    RUNDATA_FILENAME)
                        rd_dict = read_json_file(rd_dict_path)
                        self.retcode = int(rd_dict[RETURN_CODE])
                        self.ts_start = float(rd_dict[TIMESTAMP_START])
                        self.ts_end = float(rd_dict[TIMESTAMP_END])
                        self.validation = int(rd_dict[VALIDATION])
                        self.runtime = int(runtime_fp.read())
                        self.stderr = str(stderr_fp.read())
                        self.stdout = str(stdout_fp.read())
                    except ValueError as e:
                        raise Exception("The content of one of\
                        the results in %s was malformed:\
                        %s." % (self.directory, e.args()))
                    except KeyError as e:
                        print("Missing key in JSON file.")
                        print(e.args())
                        raise Exception("The content of %s did not contain the\
                        expected keys." % RUNDATA_FILENAME)

    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}, \
        dataset: {3}, \
        dynamic configuration: {4}\
        run number: {5}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.static_configuration.configuration),
                self.case.name,
                str(self.dynamic_configuration.configuration),
                str(self.name))

class Dynamic_configuration:
    def __init__(self, benchmark, implementation,
                 static_configuration, case, name, directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.static_configuration = static_configuration
        self.case = case
        self.name = name
        self.directory = directory
        self.runs = {}
        dconf_filename = os.path.join(self.directory,
                                      DYNAMIC_CONFIGURATION_FILENAME)
        if not os.path.isfile(dconf_filename):
            raise Exception("%s does not contain a file describing its \
            runtime variables. If no runtime vars, a file containing \
            \"{}\" should still be here." % self.directory)
        try:
            self.configuration = read_json_file(dconf_filename)
        except ValueError as e:
            print("Error reading JSON")
            print(e.args)
            raise Exception("Error reading JSON file %s", dconf_filename)
        dircontents = os.listdir(self.directory)
        for run in dircontents:
            if run == DYNAMIC_CONFIGURATION_FILENAME:
                continue
            runpath = os.path.join(self.directory, run)
            if not os.path.isdir(runpath):
                raise Exception("%s may only contain %s and directories \
                containing runs. %s is neither." %
                                (self.directory, DYNAMIC_CONFIGURATION_FILENAME,
                                 runpath))
            try:
                runnum = int(run)
                if runnum < 0:
                    raise Exception("Only non-negative run numbers (i.e., directory names)\
                are allowed in %s." % target_dir)
                self.runs[runnum] = Run(self.benchmark, self.implementation,
                                        self.static_configuration, self.case,
                                        self, runnum, runpath)
            except ValueError:
                raise Exception("The folder %s may only contain folders with\
                integer name." % self.directory)
        self.number_of_runs = len(self.runs.keys())


    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}, \
        dataset: {3}, \
        dynamic configuration: {4}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.static_configuration.configuration),
                self.case.name,
                str(self.configuration))

class Case:
    def __init__(self, benchmark, implementation,
                 static_configuration, name, directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.static_configuration = static_configuration
        self.name = name # name of a dataset.
        self.directory = directory
        self.dynamic_configurations = {}
        dircontents = os.listdir(self.directory)
        for dconf in dircontents:
            dconf_path = os.path.join(self.directory, dconf)
            if not os.path.isdir(dconf_path):
                raise Exception("%s may only contain folders containing dynamic\
                variables configurations. %s is not a folder" %
                                (self.directory,
                                 dconf_path))
            self.dynamic_configurations[dconf] \
                = Dynamic_configuration(self.benchmark,
                                        self.implementation,
                                        self.static_configuration,
                                        self,
                                        dconf,
                                        dconf_path)
        if not self.dynamic_configurations:
            raise Exception("The case:\n %s does not contain any folders for \
            dynamic configurations." % str(self))

    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}, \
        dataset: {3}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.static_configuration.configuration),
                self.name)
    
class Static_configuration:
    def __init__(self, benchmark, implementation, name, directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.name = name
        self.directory = directory
        self.cases = {}
        sconf_filename = os.path.join(self.directory,
                                      STATIC_CONFIGURATION_FILENAME)
        run_output_path = os.path.join(self.directory,
                                       RUN_OUTPUT_PATH)
        self.run_output_path = run_output_path
        if not os.path.isfile(sconf_filename):
            raise Exception("%s does not contain a file describing its \
            compile-time variables. If no compile-time vars, a file containing \
            \"{}\" should still be here." % self.directory)
        if not os.path.isdir(run_output_path):
            raise Exception("%s does not contain a folder called %s as it \
            should." % (self.directory, RUN_OUTPUT_PATH))
        # what if this is a malformed JSON??
        try:
            self.configuration = read_json_file(sconf_filename)
        except ValueError as e:
            print("\n" + str(e.args) + "\n")
            raise Exception("Error reading JSON file: %s" % sconf_filename)
        datasets = os.listdir(run_output_path)
        empty = True
        for dataset in datasets:
            empty = False
            dataset_path = os.path.join(run_output_path, dataset)
            if not os.path.isdir(dataset_path):
                raise Exception("The folder %s may only contain folders with \
                names of datasets." % run_output_path)
            self.cases[dataset] = Case(self.benchmark,
                                       self.implementation,
                                       self,
                                       dataset,
                                       dataset_path)
        if empty:
            raise Exception("%s does not contain folder(s) for datasets as it\
            should" % run_output_path)

    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.configuration))
    
class Implementation:
    def __init__(self, benchmark, name, directory):
        self.benchmark = benchmark
        self.name = name
        self.directory = directory
        self.static_configs = {}
        empty = True
        # contains only folders with statconf names.
        dircontents = os.listdir(self.directory)
        for sconf in dircontents:
            empty = False
            sconf_dir = os.path.join(self.directory, sconf)
            if not os.path.isdir(sconf_dir):
                raise Exception("The content of %s must be folders with names \
                for static configurations (hash values).\
                But %s is not a folder." % (run_outputdir, sconf_dir))
            # Here, the keys of the cases will be hash values.
            self.static_configs[sconf] = \
            Static_configuration(self.benchmark, self,
                                 sconf, sconf_dir)
        if empty:
                raise Exception("No folders for static configurations was found\
                in %s." % self.directory)
            
    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}"\
        .format(self.benchmark.name,
                self.name)
    
class Benchmark:
    def __init__(self, name, directory):
        self.name = name
        self.directory = directory
        self.implementations = {}
        dircontents = os.listdir(self.directory)
        for impl in dircontents:
            impldir = os.path.join(self.directory, impl)
            if not os.path.isdir(impldir):
                raise Exception("The folder containing the %s benchmark must\
                only contain directories. %s is not a directory." %
                                (self.name, impldir))
            implname = os.path.basename(impldir)
            self.implementations[implname] = Implementation(self,
                                                            implname,
                                                            impldir)
            
    def __str__(self):
        """ This function allows for printing of benchmark objects.
        """
        a = "benchmark: {0}".format(self.name)
        return a

def get_run_values(run_dicts):
    """For a given list of run_dicts (probably calculated from\
    get_run_dicts), this function returns a dictionary where the keys are \
    var_names and the values are a list that the respective run_dicts take.
    """
    first_dict = run_dicts[0]
    for var in first_dict: # all variables
        val = []
        for d in run_dicts: # all dicts
            if d[var] not in val:
                val.append(d[var])
        var_dict[var] = val.sort()
    return var_dict

#########################################################
# Below this comment are the visualization functions    #
# which parse the data collected in the objects whose   #
# classes are defined above                             #
#########################################################

## VISUALIZATION HELPER FUNCTIONS
def get_benchmark_object(benchmark_name, instdir):
    if not os.path.isdir(instdir):
        raise Exception("Directory %s could not be found." % sys.argv[1])
    dircontents = os.listdir(instdir)
    b = {}
    rootdir = os.getcwd()
    non_dirs = filter(lambda path:
                      not os.path.isdir(os.path.join(rootdir, instdir, path)),
                      dircontents)
    if non_dirs:
        raise Exception("Only directories may be be placed in %s." % sys.argv[1])
    bmdirs = map(lambda bm: os.path.join(rootdir, instdir, bm), dircontents)
    b_vals = map(lambda name, bmdir: Benchmark(name, bmdir), dircontents, bmdirs)
    b = dict(zip(dircontents, b_vals))
    try:
        benchmark = b[benchmark_name]
    except KeyError:
        raise Exception("%s was not found among the benchmark objects." %
                        benchmark_name)
    return benchmark

def get_implementation_object(benchmark, implementation_name):
    try:
        implementation = benchmark.implementations[implementation_name]
    except KeyError:
        raise Exception("%s was not found among the implementations of benchmark\
        %s" %(implementation_name))
    return implementation

def get_statistics(dconfs):
    """ Returns a list of tuples where tuple[0] = av, tuple[1] = std_dev,
        tuple[2] = uncertainty on average."""
    def _get_statistic(dconf):
        """Given one dynamic configuration, this function returns the average,
            standard deviation, and the uncertainty of the av."""
        N = dconf.number_of_runs
        runtimes = map(lambda run: run.runtime, dconf.runs.itervalues())
        av, std_dev = get_average_and_stddev(runtimes)
        uncertainty = std_dev/math.sqrt(N)
        return av, std_dev, uncertainty
    
    return map(_get_statistic, dconfs)



def get_all_static_configs(implementation):
    sconfs = implementation.static_configs.values()
    return sconfs

def flatten_list(listlist):
    flatlist = reduce(lambda l1, l2: l1 + l2, listlist) #list list -> list
    return flatlist

def get_all_case_objects(sconfs):
    cases = map(lambda sconf: sconf.cases.values(), sconfs)
    cases = flatten_list(cases)
    return cases

def get_all_dynamic_configs(cases):
    dconfs = map(lambda case: case.dynamic_configurations.values(), cases)
    dconfs = flatten_list(dconfs)
    return dconfs

def get_all_runs(dconfs):
    runs = map(lambda dconf: dconf.runs.values(), dconfs)
    runs = flatten_list(runs)
    return runs


### DICTIONARY CREATING FUNCTIONS
def get_run_dicts(benchmark_name, implementation_name, instdir):
    def _generate_json(run):
        run_data = {}
        run_data['benchmark_name'] = run.benchmark.name
        run_data['implementation_name'] = run.implementation.name
        run_data.update(run.static_configuration.configuration)
        run_data['dataset_name'] = run.case.name
        run_data.update(run.dynamic_configuration.configuration)        
        run_data['runnum'] = run.name
        run_data['retcode'] = run.retcode
        run_data['validation'] = run.validation
        run_data['runtime'] = run.runtime
        run_data['ts_start'] = run.ts_start
        run_data['ts_end'] = run.ts_end
        return run_data

    benchmark = get_benchmark_object(benchmark_name, instdir)
    implementation = get_implementation_object(benchmark, implementation_name)
    sconfs = get_all_static_configs(implementation)
    cases = get_all_case_objects(sconfs)
    dconfs = get_all_dynamic_configs(cases)
    runs = get_all_runs(dconfs)
    dicts = map(_generate_json, runs)
    return dicts

def get_dconf_dicts(benchmark_name, implementation_name, instdir):
    """Given an implementation, this function exports all data generated by
    hipermark to a JSON file specified by the user. The exported data is a list
    of all hipermark generated data from each configuration and thus also
    contains average, standard deviation and uncertainty if more than one run
    for a given configuration has been made
    """
    def belongs_to_dconf(configuration, run_dict):
        for key in configuration:
            if not run_dict[key] == configuration[key]:
                return False
        return True
    
    def get_runs_of_dconf(configuration, run_dicts):
        dcruns = []
        for run_dict in run_dicts:
            if belongs_to_dconf(configuration, run_dict):
                dcruns.append(run_dict)
        return dcruns

    def _generate_json_dconf(dconf):
        dconf_data = {}
        dconf_data['benchmark_name'] = dconf.benchmark.name
        dconf_data['implementation_name'] = dconf.implementation.name
        dconf_data.update(dconf.static_configuration.configuration)
        dconf_data['dataset_name'] = dconf.case.name
        dconf_data.update(dconf.configuration)
        return dconf_data

    def set_statistics_for_dconf(configuration, run_dicts):
        run_dicts_of_dconf = get_runs_of_dconf(configuration, run_dicts)
        run_times = []
        for run_dict in run_dicts_of_dconf:
            if (run_dict['validation'] != 1) or (run_dict['retcode'] != 0):
                raise Exception("Non-validated run encountered in the attempt \
                to export a JSON each configuration. Stopped. Error found in:",\
                                str(run_dict))
            run_times.append(run_dict['runtime'])
        N = len(run_times)
        av, stddev = get_average_and_stddev(run_times)
        configuration['average runtime'] = av # This key should perhaps be args from func. call
        configuration['std_dev'] = stddev
        configuration['number_of_runs'] = N
        configuration['uncertainty_of_runtime'] = stddev/math.sqrt(N) # args from func call?
        return

    benchmark = get_benchmark_object(benchmark_name, instdir)
    implementation = get_implementation_object(benchmark, implementation_name)
    sconfs = get_all_static_configs(implementation)
    cases = get_all_case_objects(sconfs)
    dconfs = get_all_dynamic_configs(cases)
    run_dicts = get_run_dicts(benchmark_name, implementation_name, instdir)
    configurations = map(_generate_json_dconf, dconfs)
    for config in configurations:
        set_statistics_for_dconf(config, run_dicts)
    return configurations


### EXPORT FUNCTIONS
def export_runs(instdir, benchmark_name, implementation_name, filename):
    """Given an implementation, this function exports all data generated by
    hipermark to a JSON file specified by the user. The exported data is a list
    of all hipermark generated data from each run.
    """   
    run_dicts = get_run_dicts(benchmark_name, implementation_name, instdir)
    with open(filename, "w") as fp:
        ret = fp.write(json.dumps(run_dicts))
    return ret

def export_dconfs(instdir, benchmark_name, implementation_name, filename):
    dconf_dicts = get_dconf_dicts(benchmark_name, implementation_name, instdir)
    with open(filename, "w") as fp:
        ret = fp.write(json.dumps(dconf_dicts))
    return ret


### FUNCTION THAT GENERATES POINTS IN 2-D
def get_2d_points_dconf_dicts(dconf_dicts,
                              free_var,
                              locked_vars,
                              normalize_performance,
                              dep_var = "average runtime",
                              dep_unc = 'uncertainty_of_runtime'):
    """Given dicts that describe configurations, this function returns three
    lists. One representing x point, one y points, and one uncertainties on the
    y-axis.

    Keyword arguments:
    dconf_dicts -- the list of dictionaries, one element for each configuration.
    free_var -- The name of the free variable 
    locked_vars -- a dictionary representing all the locked variables.
    normalize_performance -- If True, this function returns the norm. recip. val. of dep_var.
    dep_var -- name of dependent variable (variable on y-axis)
    dep_unc -- name of uncertainty of dep_var
    """
    def _get_normalized_performance(runtimes, uncs):
        norm_const =runtimes[0] #all lists taken as args are assumed sorted by x_values.
        performance = map(lambda rt: norm_const / rt, runtimes)
        # unc_f(x) = unc_x*f'(x)
        unc_perf = map(lambda unc, runtime: norm_const*unc / runtime**2, uncs, runtimes)
        return performance, unc_perf        
        
    num_of_vars = len(dconf_dicts[0].keys())
    num_of_locked_vars = len(locked_vars.keys())
#    if num_of_vars - num_of_locked_vars != 5:
#       raise Exception("The wrong number of locked variables has been provided.\
#       The configuration dicts contain %d variables." % num_of_vars)
    #filter the dconfs that agree with the locked variables.
    try:
        ret_dconfs = filter(lambda dconf_dict:
                            all(map(lambda key: dconf_dict[key] == locked_vars[key],
                                    locked_vars.keys())),
                            dconf_dicts)
        if not ret_dconfs:
            raise ValueError("The values specified in locked_vars of the JSON\
            describing a function where not matched by any runs.")    
    except KeyError as e:
        raise Exception("Variable defined in locked_vars not found in configuration dicts: %s" %
                        (e.args))
    # Ensure that keys and vals are all strings
    for ret_dconf in ret_dconfs:
        for key in ret_dconf.keys():
            ret_dconf[str(key)] = ret_dconf.pop(key)
            ret_dconf[str(key)] = str(ret_dconf[str(key)])

    # Sort the list of dictionaries according to free variable.
    ret_dconfs.sort(key=operator.itemgetter(free_var))
    try:
        x_values = map(lambda ret_dconf: ret_dconf[free_var], ret_dconfs)
        
    except KeyError:
        raise Exception("free_var argument given to %s function not found in dconf_dicts" %
                        sys._getframe().f_code.co_name)
    try:
        y_values = map(lambda ret_dconf: ret_dconf[dep_var], ret_dconfs)
    except KeyError:
        raise Exception("dep_var argument given to %s function not found in dconf_dicts" %
                        sys._getframe().f_code.co_name)
    try:
        y_uncs = map(lambda ret_dconf: ret_dconf[dep_unc], ret_dconfs)
    except KeyError:
        raise Exception("dep_unc argument given to %s function not found in dconf_dicts" %
                        sys._getframe().f_code.co_name)

    y_values_floats = map(lambda y: float(y), y_values)
    y_unc_floats = map(lambda unc: float(unc), y_uncs)
    
    if normalize_performance:
        y_values_floats, y_unc_floats = _get_normalized_performance(y_values_floats,
                                                                    y_unc_floats)
    
    try:
        x_values_floats = map(lambda x: float(x), x_values)
    except ValueError:
        print("x-values not recognized as numbers in this configuration:")
        try:
            print(str(ret_dconfs[0]['benchmark_name']), str(ret_dconfs[0]['implementation_name']))
        except:
            pass
        return x_values, y_values_floats, y_unc_floats
    return x_values_floats, y_values_floats, y_unc_floats


def draw_figure(x_vals, y_vals, y_uncs, fig_name, dep_var, free_var_name, fig_type):
    """Draws a bar diagram with (x,y) points and uncertainties on the y-axis.
    If dependent variable name (dep_var) is \"normalized\" performance, then 
    the the values over the bars are written as floats with two decimals. 
    Otherwise, they are written as ints.
    """
    def autolabel(rects):
        for rect in rects:
            height = float(rect.get_height())
            if dep_var == "normalized performance":
                ax.text(rect.get_x()+rect.get_width()/2., 1.05*height, '%.2f'%float(height),
                        ha='center', va='bottom')
            else:    
                ax.text(rect.get_x()+rect.get_width()/2., 1.05*height, '%d'%int(height),
                        ha='center', va='bottom')
    N = len(x_vals)
    x_vals = tuple(x_vals)
    y_vals = tuple(y_vals)
    y_uncs = tuple(y_uncs)
    width = 0.35 #should probably be reciprocal of N
    ind = np.arange(N) #What does this do?
    fig, ax = plt.subplots() #What does this do?
    if fig_type == "bar":
        rects1 = ax.bar(ind, y_vals, width, color='r', yerr=y_uncs)
    elif fig_type == "graph":
        ax.plot(ind, y_vals, 'bo', ind, y_vals, 'k')
        ax.errorbar(ind, y_vals, yerr=y_uncs)
    else:
        raise Exception("Unknown graph type requested")
    # change color to a dynamic value. Could also be defined in JSON.
    ax.set_ylabel(dep_var)
    ax.set_xlabel(free_var_name)
    title = fig_name
    ax.set_title(title)
    #ax.legend(rects1, fig_name) # Other options?
    if fig_type == "bar":
        autolabel(rects1)
        ax.set_xticks(ind + width/2)
        ax.set_xticklabels(x_vals) #Maybe the number of these should be reduced?
    else:
        ax.set_xticks(ind)
        ax.set_xticklabels(x_vals)
    fig_filename = os.path.join(HIPERPRET_FOLDER, FIGURE_FOLDER_NAME, fig_name)
    pylab.savefig(fig_filename, bbox_inches='tight')
    

    # Some modifications are needed if graphs with implementation names as indep. var
    # is to be creatable.
if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: %s <instantiation dir path>" % sys.argv[0])
        exit(1)
    instdir = sys.argv[1] #check that this is valid
    if not os.path.isdir(instdir):
        raise Exception("The instatiations dir must be a directory.")
    export_folder = os.path.join(HIPERPRET_FOLDER, EXPORT_FOLDER_NAME)
    figure_folder = os.path.join(HIPERPRET_FOLDER, FIGURE_FOLDER_NAME)
    mkdir_p(export_folder)
    mkdir_p(figure_folder)
    benchmarks = os.listdir(instdir)
    bms = []
    for i in range(len(benchmarks)):
        bms.append(get_benchmark_object(benchmarks[i], instdir))
    for bm in bms:
        for imp in bm.implementations.values():
            filename_runs = EXPORT_DCONFS_FILENAME_PREPEND + bm.name + " " +  imp.name + ".json"
            filename_runs = os.path.join(export_folder, filename_runs)
            filename_dconfs = EXPORT_RUNS_FILENAME_PREPEND + bm.name + " " +  imp.name + ".json"
            filename_dconfs = os.path.join(export_folder, filename_dconfs)
            figure_def_filename = FIGURE_CONFIGURATION_FILENAME
            export_runs(instdir, bm.name, imp.name, filename_runs)
            export_dconfs(instdir, bm.name, imp.name, filename_dconfs)
            dconf_dicts = get_dconf_dicts(bm.name, imp.name, instdir)
            figure_definitions = read_json_file(figure_def_filename)
            for figure_name, figure_vals in figure_definitions.items():
                try:
                    if not bm.name == figure_vals['benchmark_name']:
                        continue
                    if not imp.name == figure_vals['implementation_name']:
                        continue
                    fig_type = figure_vals['type']
                    free_var_name = figure_vals['free_var']
                    locked_vars = figure_vals['locked_vars']
                    sticky = figure_vals['sticky']
                    dep_var = figure_vals["dependent_var"]
                except KeyError as e:
                    print(e.args)
                    raise Exception("KeyError in JSON file. Missing key from JSON: %s" %
                                    figure_def_filename)
                try:
                    normalize_performance = bool(figure_vals["get normalized performance"])
                except ValueError:
                    raise Exception("The \"get normalized performance\" must be set to \"True\" of \"False\".")
                try:
                    retx, rety, rety_unc = get_2d_points_dconf_dicts(dconf_dicts,
                                                                     free_var_name,
                                                                     locked_vars,
                                                                     normalize_performance,
                                                                     dep_var)
                except ValueError as e:
                    print(e.args)
                    raise Exception("Problem with definition of figure with name: %s." % figure_name)
                if normalize_performance:
                    dep_var = "normalized performance"
                draw_figure(retx, rety, rety_unc, figure_name, dep_var, free_var_name, fig_type)
