#!/usr/bin/env python

import os
import sys
import math
import json
import matplotlib.pyplot as plt
import numpy as np

# These files are read, not written.
RESULT_FILENAME = "result.json"
RUNDATA_FILENAME = "rundata.json"
RUNTIME_FILENAME = "runtime.txt"
STDERR_FILENAME = "stderr.log"
STDOUT_FILENAME = "stdout.log"
STATIC_CONFIGURATION_FILENAME = "static_configuration.json"
DYNAMIC_CONFIGURATION_FILENAME = "dynamic_configuration.json"
RUN_OUTPUT_PATH = "run_output"

# dict entries in rundata.json
TIMESTAMP_START = "timestamp_start"
TIMESTAMP_END = "timestamp_end"
RETURN_CODE = "return_code"
VALIDATION = "validation"
RUNTIME = "runtime"

def read_json_file(filename):
    """ returns an array containing the content of the JSON file.

    keyword arguments:
    filename -- name of json file to interpret
    """
    with open(filename, "r") as file:
        return json.loads(str(file.read()))


def get_nth_moment (result_times, n):
    """ 
    Calculates the nth moment of a list.

    Keyword values:
    result_times -- list of result times (floats)
    n -- integer describing that statistical moment of result_times which is returned
    """
    powers = map(lambda x: x**n, result_times)
    return sum(powers)/float(len(result_times))

def get_average_and_stddev (result_times):
    """ Returns average and stddev for a list.

    Keyword values:
    result_times -- list of result times (floats)
    """
    av = get_nth_moment(result_times, 1)
    return (av, math.sqrt(get_nth_moment(result_times, 2) - av**2))


class Run:
    def __init__(self, benchmark,
                 implementation,
                 static_configuration,
                 case,
                 dynamic_configuration,
                 runnum,
                 directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.static_configuration = static_configuration
        self.case = case
        self.dynamic_configuration = dynamic_configuration # an pointer to an object
        self.name = runnum
        self.directory = directory
        rundir_content = [RESULT_FILENAME, RUNTIME_FILENAME,\
                          STDERR_FILENAME, STDOUT_FILENAME, RUNDATA_FILENAME]
        dircontents = os.listdir(self.directory)
        N = len(rundir_content)
        for i in range(N):
            if not rundir_content[i] in dircontents:
                raise Exception("The folder %s does not contain the file %s." %
                                (self.directory, rundir_content[i]))
        with open(os.path.join(self.directory,
                               RUNTIME_FILENAME), "r") as runtime_fp:
            with open(os.path.join(self.directory,
                                   STDERR_FILENAME), "r") as stderr_fp:
                with open(os.path.join(self.directory,
                                       STDOUT_FILENAME), "r") as stdout_fp:
                    try:
                        # Also catch the KeyError here!!
                        rd_dict_path = os.path.join(self.directory,
                                                    RUNDATA_FILENAME)
                        rd_dict = read_json_file(rd_dict_path)
                        self.retcode = int(rd_dict[RETURN_CODE])
                        self.ts_start = float(rd_dict[TIMESTAMP_START])
                        self.ts_end = float(rd_dict[TIMESTAMP_END])
                        self.validation = int(rd_dict[VALIDATION])
                        self.runtime = int(runtime_fp.read())
                        self.stderr = str(stderr_fp.read())
                        self.stdout = str(stdout_fp.read())
                    except ValueError as e:
                        # How are the arguments of the exception presented?
                        raise Exception("The content of one of\
                        the result files in %s was malformed:\
                        %s." % (self.directory, e.args()))

    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}, \
        dataset: {3}, \
        dynamic configuration: {4}\
        run number: {5}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.static_configuration.configuration),
                self.case.name,
                str(self.dynamic_configuration.configuration),
                str(self.name))

class Dynamic_configuration:
    def __init__(self, benchmark, implementation,
                 static_configuration, case, name, directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.static_configuration = static_configuration
        self.case = case
        self.name = name
        self.directory = directory
        self.runs = {}
        dconf_filename = os.path.join(self.directory,
                                      DYNAMIC_CONFIGURATION_FILENAME)
        if not os.path.isfile(dconf_filename):
            raise Exception("%s does not contain a file describing its \
            runtime variables. If no runtime vars, a file containing \
            \"{}\" should still be here." % self.directory)
        try:
            self.configuration = read_json_file(dconf_filename)
        except ValueError as e:
            print("Error reading JSON")
            print(e.args)
            raise Exception("Error reading JSON file %s", dconf_filename)
        dircontents = os.listdir(self.directory)
        for run in dircontents:
            if run == DYNAMIC_CONFIGURATION_FILENAME:
                continue
            runpath = os.path.join(self.directory, run)
            if not os.path.isdir(runpath):
                raise Exception("%s may only contain %s and directories \
                containing runs. %s is neither." %
                                (self.directory, DYNAMIC_CONFIGURATION_FILENAME,
                                 runpath))
            try:
                runnum = int(run)
                if runnum < 0:
                    raise Exception("Only non-negative run numbers (i.e., directory names)\
                are allowed in %s." % target_dir)
                self.runs[runnum] = Run(self.benchmark, self.implementation,
                                        self.static_configuration, self.case,
                                        self, runnum, runpath)
            except ValueError:
                raise Exception("The folder %s may only contain folders with\
                integer name." % self.directory)
        self.number_of_runs = len(self.runs.keys())


    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}, \
        dataset: {3}, \
        dynamic configuration: {4}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.static_configuration.configuration),
                self.case.name,
                str(self.configuration))

class Case:
    def __init__(self, benchmark, implementation,
                 static_configuration, name, directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.static_configuration = static_configuration
        self.name = name # name of a dataset.
        self.directory = directory
        self.dynamic_configurations = {}
        dircontents = os.listdir(self.directory)
        for dconf in dircontents:
            dconf_path = os.path.join(self.directory, dconf)
            if not os.path.isdir(dconf_path):
                raise Exception("%s may only contain folders containing dynamic\
                variables configurations. %s is not a folder" %
                                (self.directory,
                                 dconf_path))
            self.dynamic_configurations[dconf] \
                = Dynamic_configuration(self.benchmark,
                                        self.implementation,
                                        self.static_configuration,
                                        self,
                                        dconf,
                                        dconf_path)
        if not self.dynamic_configurations:
            raise Exception("The case:\n %s does not contain any folders for \
            dynamic configurations." % str(self))

    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}, \
        dataset: {3}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.static_configuration.configuration),
                self.name)
    
class Static_configuration:
    def __init__(self, benchmark, implementation, name, directory):
        self.benchmark = benchmark
        self.implementation = implementation
        self.name = name
        self.directory = directory
        self.cases = {}
        sconf_filename = os.path.join(self.directory,
                                      STATIC_CONFIGURATION_FILENAME)
        run_output_path = os.path.join(self.directory,
                                       RUN_OUTPUT_PATH)
        self.run_output_path = run_output_path
        if not os.path.isfile(sconf_filename):
            raise Exception("%s does not contain a file describing its \
            compile-time variables. If no compile-time vars, a file containing \
            \"{}\" should still be here." % self.directory)
        if not os.path.isdir(run_output_path):
            raise Exception("%s does not contain a folder called %s as it \
            should." % (self.directory, RUN_OUTPUT_PATH))
        # what if this is a malformed JSON??
        try:
            self.configuration = read_json_file(sconf_filename)
        except ValueError as e:
            print("\n" + str(e.args) + "\n")
            raise Exception("Error reading JSON file: %s" % sconf_filename)
        datasets = os.listdir(run_output_path)
        empty = True
        for dataset in datasets:
            empty = False
            dataset_path = os.path.join(run_output_path, dataset)
            if not os.path.isdir(dataset_path):
                raise Exception("The folder %s may only contain folders with \
                names of datasets." % run_output_path)
            self.cases[dataset] = Case(self.benchmark,
                                       self.implementation,
                                       self,
                                       dataset,
                                       dataset_path)
        if empty:
            raise Exception("%s does not contain folder(s) for datasets as it\
            should" % run_output_path)

    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}, \
        static configuration: {2}"\
        .format(self.benchmark.name,
                self.implementation.name,
                str(self.configuration))
    
class Implementation:
    def __init__(self, benchmark, name, directory):
        self.benchmark = benchmark
        self.name = name
        self.directory = directory
        self.static_configs = {}
        empty = True
        # contains only folders with statconf names.
        dircontents = os.listdir(self.directory)
        for sconf in dircontents:
            empty = False
            sconf_dir = os.path.join(self.directory, sconf)
            if not os.path.isdir(sconf_dir):
                raise Exception("The content of %s must be folders with names \
                for static configurations (hash values).\
                But %s is not a folder." % (run_outputdir, sconf_dir))
            # Here, the keys of the cases will be hash values.
            self.static_configs[sconf] = \
            Static_configuration(self.benchmark, self,
                                 sconf, sconf_dir)
        if empty:
                raise Exception("No folders for static configurations was found\
                in %s." % self.directory)
            
    def __str__(self):
        return "benchmark: {0}, \
        implementation: {1}"\
        .format(self.benchmark.name,
                self.name)
    
class Benchmark:
    def __init__(self, name, directory):
        self.name = name
        self.directory = directory
        self.implementations = {}
        dircontents = os.listdir(self.directory)
        for impl in dircontents:
            impldir = os.path.join(self.directory, impl)
            if not os.path.isdir(impldir):
                raise Exception("The folder containing the %s benchmark must\
                only contain directories. %s is not a directory." %
                                (self.name, impldir))
            implname = os.path.basename(impldir)
            self.implementations[implname] = Implementation(self,
                                                            implname,
                                                            impldir)
            
    def __str__(self):
        """ This function allows for printing of benchmark objects.
        """
        a = "benchmark: {0}".format(self.name)
        return a




def get_y_vs_x(y_name, x_name):
    """ Given a certain benchmark, this should allow the user to select one free 
    variable and one dependent variable. """

def get_static_var_names(implementation):
    """Returns a list of all static (compile time) configuration variable names.
    Returns the empty list if there are no
    """
    if implementation.static_configs == {}:
        return []
    first = True
    for sconf_name in implementation.static_configs:
        static_conf = implementation.static_configs[sconf_name]
        if first:
            static_var_names= list(static_conf.configuration.keys())
            first = False
        else:
            # we demand
            # that the dictionary contain the same keys as the first.
            sconf_keys = list(static_conf.configuration.keys())
            if not set(static_var_names) == set(sconf_keys):
                raise Exception("Static configuration keys differ for \
                implementation %s" % implementation.name)
    return map(lambda name: str(name), static_var_names)

def get_runtime_var_names(implementation):
    """Returns a list of names of all the runtime variables that have been used
    for a given implementation. These must be the same for all runs.
    """
    first = True
    for sconf in implementation.static_configs.itervalues():
        for cases in sconf.cases.itervalues():
            for dconf in cases.dynamic_configurations.itervalues():
                if first:
                    dconf_names = list(dconf.configuration.keys())
                    first = False
                else:
                    dconf_names_new = list(dconf.configuration.keys())
                    if not set(dconf_names) == set(dconf_names_new):
                        raise Exception("The runs of %s do not agree on the \
                        names of the dynamic configuration variables. Both \
                        %s and %s were observed." %
                                        (dconf_names, dconf_names_new))
    return map(lambda name: str(name), dconf_names)         

def get_var_names(implementation):
    """Returns the variable names that a specific implementation contains.
    The return type is a dict with list of strings as values.
    """
    variables = {}
    variables["static_configurations"] = get_static_var_names(implementation)
    variables["runtime_configurations"] = get_runtime_var_names(implementation)
    return variables

def get_run_values(run_dicts):
    """For a given list of run_dicts (probably calculated from\
    get_run_dicts), this function returns a dictionary where the keys are \
    var_names and the values are a list that the respective run_dicts take.
    """
    first_dict = run_dicts[0]
    for var in first_dict: # all variables
        val = []
        for d in run_dicts: # all dicts
            if d[var] not in val:
                val.append(d[var])
        var_dict[var] = val.sort()
    return var_dict

# Why is rootdir needed? It allows for relative paths upon execution!
# This program could also demand a benchmark as sys.argv[1] thus only
# allowing it to analyze one benchmark at a time.
if len(sys.argv) != 2:
    raise Exception("Usage %s <directory_containing_hipermark_results>" %
                    sys.argv[0])
instdir = sys.argv[1]
if not os.path.isdir(instdir):
    raise Exception("Directory %s could not be found." % sys.argv[1])
dircontents = os.listdir(instdir)
b = {}
rootdir = os.getcwd()
for bm in dircontents:
    bmdir = os.path.join(rootdir, instdir, bm)
    if not os.path.isdir(bmdir):
        raise Exception("Only directories may be placed in %s. %s is not a \
        directory." % (sys.argv[1], bm))
    name = os.path.basename(os.path.normpath(bm))
    b[name] = Benchmark(name, bmdir)

#########################################################
# Below this comment are the visualization functions    #
# which parse the data collected in the objects whose   #
# classes are defined above                             #
#########################################################

def get_benchmark_object(benchmark_name, instdir):
    if not os.path.isdir(instdir):
        raise Exception("Directory %s could not be found." % sys.argv[1])
    dircontents = os.listdir(instdir)
    b = {}
    rootdir = os.getcwd()
    non_dirs = filter(lambda path:
                      not os.path.isdir(os.path.join(rootdir, instdir, path)),
                      dircontents)
    if non_dirs:
        raise Exception("Only directories may be be placed in %s." % sys.argv[1])
    bmdirs = map(lambda bm: os.path.join(rootdir, instdir, bm), dircontents)
    b_vals = map(lambda name, bmdir: Benchmark(name, bmdir), dircontents, bmdirs)
    b = dict(zip(dircontents, b_vals))
    try:
        benchmark = b[benchmark_name]
    except KeyError:
        raise Exception("%s was not found among the benchmark objects." %
                        benchmark_name)
    return benchmark

def get_implementation_object(benchmark, implementation_name):
    try:
        implementation = benchmark.implementations[implementation_name]
    except KeyError:
        raise Exception("%s was not found among the implementations of benchmark\
        %s" %(implementation_name))
    return implementation

def get_statistics(dconfs):
    """ Returns a list of tuples where tuple[0] = av, tuple[1] = std_dev,
        tuple[2] = uncertainty on average."""
    def _get_statistic(dconf):
        """Given one dynamic configuration, this function returns the average,
            standard deviation, and the uncertainty of the av."""
        N = dconf.number_of_runs
        runtimes = map(lambda run: run.runtime, dconf.runs.itervalues())
        av, std_dev = get_average_and_stddev(runtimes)
        uncertainty = std_dev/math.sqrt(N)
        return av, std_dev, uncertainty
    
    return map(_get_statistic, dconfs)

    # Can this be generalized?
    # Real types is a static variable name so this should be generalizable to
    # all static variable names.
def runtime_vs_real_types(instdir, benchmark_name, implementation_name,
                          dataset_name, num_cores):
    # Is this function necessary?
    # This function can be generalized to get_static_variable_values.
    def get_real_types(implementation):
        """Finds all the real types that have been used in this implementation
        """
        try:
            real_types = map(lambda sconf: sconf.configuration['REAL_TYPE'],\
                             implementation.static_configs.itervalues())
        except KeyError:
            raise Exception("REAL_TYPE is not a static configuration \
            variable of %s" % implementation.name)
        return map(lambda name: str(name), real_types)

    def get_case_objects(implementation, dataset_name):
        """Returns all cases that conform to parent function arguments.
        This function is the one that allows real type to be a variable
        since it does not filter on any static variable values.
        """
        sconfs = implementation.static_configs.itervalues()
        cases = map(lambda sconf: sconf.cases[dataset_name], sconfs)
        return cases

    # This function is probably generalizable to an arbitrary number of dynamic
    # variables where the filter function below is put into a loop or is mapped
    # over to include several variables which could be provided as a list.
    def get_dconf_objects(cases, num_cores):
        """Returns the dynamic configuration objects that conform to parent
        functions arguments.
        """
        def extract_dconfs(case):
            dconfs = filter(lambda dconf: num_cores == 
                           int(dconf.configuration['NUM_THREADS']),
                           case.dynamic_configurations.itervalues())
            if len(dconfs) != 1:
                raise Exception("Non-unique dynamic configuration extracted",
                                map(lambda dconf: dconf.name, dconfs))
            return dconfs[0]

        return map(extract_dconfs, cases)
            
    benchmark = get_benchmark_object(benchmark_name, instdir)
    implementation = get_implementation_object(benchmark, implementation_name)
    real_types = get_real_types(implementation) #NOT USED ATM
    cases = get_case_objects(implementation, dataset_name)
    dynamic_configs = get_dconf_objects(cases, num_cores)
    statistics = get_statistics(dynamic_configs)
    return statistics

# Generalize real type to a list of locked static variable values.
# This could be generalized to speed_up_vs_any_dynamic_variable
def speed_up_vs_no_threads(instdir, benchmark_name, implementation_name,
                           real_type, dataset_name):
    """This function returns a normalized measure of the reciprocal runtime.
    The normalization is wrt one thread.

    For openmp implementations only.
    """
    def get_static_configs(implementation, real_type_value):
        try:
            static_configs = filter(lambda sconf:
                                    real_type_value == sconf.configuration['REAL_TYPE'],
                                    implementation.static_configs.itervalues())
        except KeyError:
            raise Exception("The static configuration of implementation %s does\
            not contain a key with the name REAL_TYPE." % implementation.name)
        if not len(static_configs) == 1:
            raise Exception("Non-unique static configuration object was \
            extracted from the implementation %s." % implementation.name)
        return static_configs[0]

    def get_case_objects(static_config, dataset_name):
        try:
            case = static_config.cases[dataset_name]
        except KeyError:
            raise Exception("The dataset name %s was not among the case objects\
            in the static configuration object %s." %
            (dataset_name, static_config.name))
        return case

    def get_dconf_objects(case):
        dconfs =  case.dynamic_configurations.values()
        return dconfs

    def get_statistics(dconfs):
        """ Returns a list of tuples where tuple[0] = av, tuple[1] = std_dev,
        tuple[2] = uncertainty on average. Also returns the corresponding 
        x values"""
        def _get_statistic(dconf):
            """Given one dynamic configuration, this function returns the average,
            standard deviation, and the uncertainty of the av."""
            N = dconf.number_of_runs
            runtimes = map(lambda run: run.runtime,
                           dconf.runs.itervalues())
            no_of_threads = dconf.configuration['NUM_THREADS']
            no_of_threads = str(no_of_threads)
            no_of_threads = int(no_of_threads)
            av, std_dev = get_average_and_stddev(runtimes)
            uncertainty = std_dev/math.sqrt(N)
            return no_of_threads, av, std_dev, uncertainty

        def _normalize_tuple(st, nc):
            no_of_threads = st[0]
            av = st[1]
            std_dev = st[2]
            unc = st[3]
            perf_normalized = nc / av
            std_dev_perf = nc * std_dev / av**2
            unc_perf = nc * unc / av**2
            return (no_of_threads, perf_normalized, std_dev_perf, unc_perf)

        statistics = map(_get_statistic, dconfs)
        normalization_constant = filter(lambda stat: 1 == stat[0], statistics)[0][1]
        statistics = [_normalize_tuple(x, normalization_constant) for x in statistics]
        statistics.sort(key=lambda tup: tup[1])
        return statistics   
            
    benchmark = get_benchmark_object(benchmark_name, instdir)
    implementation = get_implementation_object(benchmark, implementation_name)
    static_config = get_static_configs(implementation, real_type)
    case = get_case_objects(static_config, dataset_name)
    dconfs = get_dconf_objects(case)
    statistics = get_statistics(dconfs)
    return statistics

def bar_visualize(statistics):
    N = len(statistics)
    avs = tuple(map(lambda stat: stat[0], statistics))
    uncs = tuple(map(lambda stat: stat[2], statistics))
    width = 0.35
    ind = np.arange(N)
    
    fig, ax = plt.subplots()
    print("AVS: ", avs)
    print("UNCS: ", uncs)
    rects1 = ax.bar(ind, avs, width, color='r', yerr=uncs)
    ax.set_ylabel('Runtime [ms]')
    ax.set_title('Runtime of OptionPricing, cpp_openmp, small dataset, 2 threads')
    ax.set_xticks(ind + width/2)
    ax.set_xticklabels( ('double', 'float') )
    
    def autolabel(rects):
        # attach some text labels
        for rect in rects:
            height = rect.get_height()
            ax.text(rect.get_x()+rect.get_width()/2., 1.05*height, '%d'%int(height),
                    ha='center', va='bottom')

    autolabel(rects1)
    plt.show()

def bar_visualize_rt_vs_nt(statistics1, statistics2):
    N = len(statistics1)
    avs1 = tuple(map(lambda stat: stat[1], statistics1))
    uncs1 = tuple(map(lambda stat: stat[3], statistics1))
    avs2 = tuple(map(lambda stat: stat[1], statistics2))
    uncs2 = tuple(map(lambda stat: stat[3], statistics2))
    width = 0.35
    ind = np.arange(N)
    
    fig, ax = plt.subplots()
    print("AVS1: ", avs1) #DEV!
    print("UNCS1: ", uncs1) #DEV!
    print("AVS2: ", avs2) #DEV!
    print("UNCS2: ", uncs2) #DEV!
    rects1 = ax.bar(ind, avs1, width, color='r', yerr=uncs1)
    rects2 = ax.bar(ind + width, avs2, width, color='g', yerr=uncs2)
    ax.set_ylabel('Performance increase')
    ax.set_title('Performance vs no of threads of OptionPricing, cpp_openmp, small dataset, doubles')
    ax.set_xticks(ind + width)
    labels = tuple(map(lambda stat: stat[0], statistics1))
    ax.set_xticklabels( labels )
    ax.legend( (rects1[0], rects2[0]), ('Double', 'Float') )

    def autolabel(rects):
        # attach some text labels
        for rect in rects:
            height = float(rect.get_height())
            ax.text(rect.get_x()+rect.get_width()/2., 1.05*height, '%.2f'%float(height),
                    ha='center', va='bottom')

    autolabel(rects1)
    autolabel(rects2)
    plt.show()

def get_all_static_configs(implementation):
    sconfs = implementation.static_configs.values()
    return sconfs

def flatten_list(listlist):
    flatlist = reduce(lambda l1, l2: l1 + l2, listlist) #list list -> list
    return flatlist

def get_all_case_objects(sconfs):
    cases = map(lambda sconf: sconf.cases.values(), sconfs)
    cases = flatten_list(cases)
    return cases

def get_all_dynamic_configs(cases):
    dconfs = map(lambda case: case.dynamic_configurations.values(), cases)
    dconfs = flatten_list(dconfs)
    return dconfs

def get_all_runs(dconfs):
    runs = map(lambda dconf: dconf.runs.values(), dconfs)
    runs = flatten_list(runs)
    return runs

    
def get_run_dicts(benchmark_name, implementation_name, instdir):
    def _generate_json(run):
        run_data = {}
        run_data['implementation_name'] = run.benchmark.name
        run_data.update(run.static_configuration.configuration)
        run_data['dataset_name'] = run.case.name
        run_data.update(run.dynamic_configuration.configuration)        
        run_data['runnum'] = run.name
        run_data['retcode'] = run.retcode
        run_data['validation'] = run.validation
        run_data['runtime'] = run.runtime
        run_data['ts_start'] = run.ts_start
        run_data['ts_end'] = run.ts_end
        return run_data

    benchmark = get_benchmark_object(benchmark_name, instdir)
    implementation = get_implementation_object(benchmark, implementation_name)
    sconfs = get_all_static_configs(implementation)
    cases = get_all_case_objects(sconfs)
    dconfs = get_all_dynamic_configs(cases)
    runs = get_all_runs(dconfs)
    dicts = map(_generate_json, runs)
    return dicts

def export_runs(instdir, benchmark_name, implementation_name, filename):
    """Given an implementation, this function exports all data generated by
    hipermark to a JSON file specified by the user. The exported data is a list
    of all hipermark generated data from each run.
    """   
    run_dicts = get_run_dicts(benchmark_name, implementation_name, instdir)
    with open(filename, "w") as fp:
        ret = fp.write(json.dumps(run_dicts))
    return ret

# This function should probably take run_dicts as argument
def get_dconf_dicts(benchmark_name, implementation_name, instdir):
    """Given an implementation, this function exports all data generated by
    hipermark to a JSON file specified by the user. The exported data is a list
    of all hipermark generated data from each configuration and thus also
    contains average, standard deviation and uncertainty if more than one run
    for a given configuration has been made
    """
    def belongs_to_dconf(configuration, run_dict):
        for key in configuration:
            if not run_dict[key] == configuration[key]:
                return False
        return True
    
    def get_runs_of_dconf(configuration, run_dicts):
        dcruns = []
        for run_dict in run_dicts:
            if belongs_to_dconf(configuration, run_dict):
                dcruns.append(run_dict)
        return dcruns

    def _generate_json_dconf(dconf):
        dconf_data = {}
        dconf_data['implementation_name'] = dconf.benchmark.name
        dconf_data.update(dconf.static_configuration.configuration)
        dconf_data['dataset_name'] = dconf.case.name
        dconf_data.update(dconf.configuration)
        return dconf_data

    def set_statistics_for_dconf(configuration, run_dicts):
        run_dicts_of_dconf = get_runs_of_dconf(configuration, run_dicts)
        run_times = []
        for run_dict in run_dicts_of_dconf:
            if (run_dict['validation'] != 1) or (run_dict['retcode'] != 0):
                raise Exception("Non-validated run encountered in the attempt \
                to export a JSON each configuration. Stopped. Error found in:",\
                                str(run_dict))
            run_times.append(run_dict['runtime'])
        N = len(run_times)
        av, stddev = get_average_and_stddev(run_times)
        configuration['average_runtime'] = av
        configuration['std_dev'] = stddev
        configuration['number_of_runs'] = N
        configuration['uncertainty_of_average'] = stddev/math.sqrt(N)
        return

    benchmark = get_benchmark_object(benchmark_name, instdir)
    implementation = get_implementation_object(benchmark, implementation_name)
    sconfs = get_all_static_configs(implementation)
    cases = get_all_case_objects(sconfs)
    dconfs = get_all_dynamic_configs(cases)
    run_dicts = get_run_dicts(benchmark_name, implementation_name, instdir)
    configurations = map(_generate_json_dconf, dconfs)
    for config in configurations:
        set_statistics_for_dconf(config, run_dicts)
    return configurations

def export_dconfs(instdir, benchmark_name, implementation_name, filename):
    dconf_dicts = get_dconf_dicts(benchmark_name, implementation_name, instdir)
    with open(filename, "w") as fp:
        ret = fp.write(json.dumps(dconf_dicts))
    return ret

# Should probably not plot graph but only extract the points in a 2D space.
# Function is not done and all function need to be cleaned up before it can be.
def bar_visualize(implementation, free_variable, locked_variable_dict):
    run_dicts = get_run_dicts(implementation.benchmark.name, implementation.name)
    variables = get_run_values(run_dicts)
    if free_variable not in variables.keys():
        raise Exception("The requested free variable %s was not found in" +
                        str(implementation))
    req_no_locked_vars = len(locked_variable_dict.keys())
    if req_no_locked_vars != (len(variables.keys()) - 1):
        raise Exception("An incorrect number of locked variables where given. \
        %d locked variables are required." % req_no_locked_vars)
    dconfs = get_dconf_dicts(implementation.benchmark.name, implementation.name, instdir)
    
    
# Testing occurs below this line. Specific dataset is needed for this to work.
print("These benchmarks were found:")
for benchmark in b.itervalues():
    print(benchmark.name)
lvc = b['OptionPricing']
print(lvc)
compn = lvc.implementations['cpp_openmp']
var_names = get_var_names(compn)
print("The static var names are:")
print(var_names["static_configurations"])
print("The dataset names are:")
print(var_names["datasets"])
print("The runtime variable's names are:")
print(var_names["runtime_configurations"])
print("The different values of NUM_THREADS are:")
statistics = runtime_vs_real_types(sys.argv[1], 'OptionPricing', 'cpp_openmp', 'small', 2)
print("FUNCTION RETURNS:")
print(statistics)
statistics1 = speed_up_vs_no_threads(sys.argv[1], 'OptionPricing', 'cpp_openmp',
                                     'double', 'small')
statistics2 = speed_up_vs_no_threads(sys.argv[1], 'OptionPricing', 'cpp_openmp',
                                     'float', 'small')
#print("2nd FUNCTION RETURNS:")
#print(statistics)
#bar_visualize_rt_vs_nt(statistics1, statistics2)
export_runs(sys.argv[1], 'OptionPricing', 'cpp_openmp', 'export.json')
export_dconfs(sys.argv[1], 'OptionPricing', 'cpp_openmp', 'export_dconfs.json')
